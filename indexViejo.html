<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 61</div>

    <div class="presentation-container">
        
        <!-- DIAPOSITIVA 1: TÍTULO -->
        <div class="slide" data-slide="1">
            <h1>Análisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Acústicos para la Clasificación del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque técnico para la modelación de características prosódicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro Pérez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Gutiérrez</strong></p>
            </div>
        </div>

        <!-- DIAPOSITIVA 2: JUSTIFICACIÓN -->
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificación Técnica</h2>
            <div class="highlight-box">
                <h3>¿Por qué Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma acústica compleja, rica en información latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Características Prosódicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribución de energía en las frecuencias (formantes) varía sistemáticamente con la emoción.</p></div>
                <div class="stat-item"><h3>Señal No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con técnicas de IA.</p></div>
            </div>
            <p><strong>¿Por qué Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracción de Patrones:</strong> Capacidad para identificar automáticamente características complejas en espectrogramas, indetectables para el análisis tradicional.</li>
                <li><strong>Análisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificación consistente y reproducible de las características vocales.</li>
                <li><strong>Modelado de Alta Dimensión:</strong> Habilidad para procesar miles de características extraídas de una sola señal de audio.</li>
            </ul>
            <footer class="footnote">
                <strong>Prosódicas:</strong> Características relacionadas con el ritmo, entonación y acentuación del habla. <strong>Espectrales:</strong> Propiedades relacionadas con la distribución de frecuencias en la señal de audio.
            </footer>
        </div>

        <!-- DIAPOSITIVA 3: PROBLEMA -->
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripción del Problema</h2>
            <div class="highlight-box">
                <h3>Problema Técnico Principal</h3>
                <p>El desafío de clasificar estados emocionales a partir de la señal del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¿Qué reto técnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones acústicos para 7 emociones distintas: <strong>alegría, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabación.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la señal hasta la clasificación.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente. <strong>Alta dimensionalidad:</strong> Datos con muchas características o variables, lo que aumenta la complejidad computacional.
            </footer>
        </div>

        <!-- DIAPOSITIVA 4: OBJETIVO GENERAL -->
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificación de emociones humanas a partir del análisis de características acústicas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la señal hasta la predicción del modelo.</p>
            </div>
        </div>

        <!-- DIAPOSITIVA 5: OBJETIVOS ESPECÍFICOS -->
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Específicos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar múltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar características acústicas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Diseñar, entrenar y optimizar modelos especializados de aprendizaje automático</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando métricas estándar de clasificación</div>
            <div class="methodology-step"><strong>5.</strong> Implementar técnicas de reducción de dimensionalidad y visualización</div>
            <footer class="footnote">
                <strong>Reducción de dimensionalidad:</strong> Técnicas para disminuir el número de variables en un dataset manteniendo la información más importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 6: METODOLOGÍA -->
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodología Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo cíclico, que nos permite refinar y mejorar continuamente nuestro modelo basándonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentación y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisición de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. Análisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracción de Características</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluación y Resultados</h4>
                </div>
            </div>
            <footer class="footnote">
                <strong>Metodología iterativa:</strong> Proceso de desarrollo que se repite en ciclos, permitiendo mejoras continuas basadas en resultados previos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 7: DATASETS -->
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisición de Conjuntos de Datos</h2>
            <div class="dataset-card">
                <h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 864 grabaciones de audio</li>
                    <li><strong>Características:</strong> Español mexicano, 6 emociones + neutralidad</li>
                    <li><strong>Utilidad:</strong> Adaptación específica al habla mexicana</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 2,496 grabaciones vocales</li>
                    <li><strong>Características:</strong> Calidad profesional, 24 actores</li>
                    <li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 4,800 muestras de audio</li>
                    <li><strong>Características:</strong> Alta calidad, actrices entrenadas</li>
                    <li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Dataset:</strong> Conjunto de datos organizados para entrenamiento de modelos. <strong>Benchmarks:</strong> Estándares de referencia para medir el rendimiento de un modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 8: DISTRIBUCIÓN FINAL -->
        <div class="slide hidden" data-slide="8">
            <h2>7. Distribución Final de Datos</h2>
            <div class="highlight-box">
                <h3>📈 Total de archivos procesados: 7,296</h3>
                <p>Después de la integración y limpieza de todos los datasets</p>
            </div>
            <div class="single-chart-container">
                <h3>Distribución de Emociones en el Dataset Completo</h3>
                <img src="Results/Plots/emotion_distribution.png" alt="Distribución de Emociones" style="max-width: 100%; border-radius: 15px;">
            </div>
            <h3>Características de la distribución:</h3>
            <ul>
                <li><strong>Entrenamiento:</strong> 4,668 muestras (64.0%)</li>
                <li><strong>Validación:</strong> 1,168 muestras (16.0%)</li>
                <li><strong>Prueba:</strong> 1,460 muestras (20.0%)</li>
                <li><strong>Diversidad:</strong> Múltiples hablantes, idiomas y contextos</li>
                <li><strong>Calidad:</strong> Grabaciones profesionales y controladas</li>
            </ul>
            <footer class="footnote">
                <strong>Distribución balanceada:</strong> Cuando todas las clases o categorías tienen aproximadamente la misma cantidad de ejemplos en el dataset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 9: EDA -->
        <div class="slide hidden" data-slide="9">
            <h2>8. Análisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigación:</h3>
            <ul>
                <li>¿Existen diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¿Cómo varían las características prosódicas entre emociones?</li>
                <li>¿Qué nivel de variabilidad existe dentro de cada categoría?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribución de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gráficos de Violín de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gráficos de Dispersión 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>EDA:</strong> Análisis Exploratorio de Datos, proceso de examinar datasets para descubrir patrones y relaciones. <strong>Pitch:</strong> Frecuencia fundamental de la voz, relacionada con qué tan aguda o grave suena.
            </footer>
        </div>

        <!-- DIAPOSITIVA 10: PITCH -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualización: Distribución del Pitch</h2>
            <div class="single-chart-container">
                <h3>Análisis de Pitch: Distribución de Frecuencia Fundamental (F0) por Emoción</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoción">
                <p>
                    Esta gráfica nos muestra cómo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoción. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores más altos significan un tono más agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qué tan comunes son ciertos tonos para una emoción.
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energía como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos más agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos más graves y monótonos.
                </div>
            </div>
            <footer class="footnote">
                <strong>Frecuencia fundamental (F0):</strong> La frecuencia más baja de una señal periódica, determina el pitch percibido. <strong>Hertz (Hz):</strong> Unidad de medida de frecuencia, equivale a ciclos por segundo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 11: MFCCS -->
        <div class="slide hidden" data-slide="11">
            <h2>10. Visualización y Análisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>Análisis Espectral: Distribución de los Primeros 13 MFCCs por Emoción</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gráficos de Violín de MFCCs por Emoción" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualización nos permite comparar la "forma" del sonido para cada emoción a través de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "violín" muestra el rango y la concentración de valores para un coeficiente (eje X) y una emoción (color).
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribución para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posición de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoción de otra.
                </div>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretación General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energía total o sonoridad de la señal.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles más finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>MFCCs:</strong> Coeficientes que capturan las características del espectro de frecuencias de manera similar a como el oído humano percibe el sonido. <strong>Cepstral:</strong> Análisis en el dominio del cepstrum, útil para separar la fuente del filtro en señales de voz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 12: FLUJO DE PROCESAMIENTO -->
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoción, debemos traducir la onda de sonido a un lenguaje que entienda: los números. Este proceso se llama <strong>Extracción de Características</strong>. A continuación, veremos el paso a paso de cómo convertimos un archivo de audio en un único vector de 180 características.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">🔊</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">#️⃣</div>
                    <div class="flow-text">Digitalización (Muestreo)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">🖼️</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">📊</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                     <div class="flow-icon">🧬</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Vector:</strong> Lista ordenada de números que representa las características extraídas del audio. <strong>Frames:</strong> Pequeños segmentos de audio analizados individualmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 13: DIGITALIZACIÓN -->
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 1: Digitalización y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalización</h4>
                    <p>Una onda de sonido es una señal analógica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de números, donde cada número es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es estática. Para analizarla, la dividimos en pequeños segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tamaño de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colección de muchos arreglos pequeños (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio_wave_sampling_and_framing.png" alt="Diagrama de Digitalización y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en múltiples ventanas (frames) para su análisis.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Muestreo:</strong> Proceso de convertir una señal continua en una secuencia discreta de valores. <strong>Amplitud:</strong> Intensidad o fuerza de la onda sonora en un momento dado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 14: FFT -->
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¿Qué es y para qué sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qué frecuencias la componen. La <strong>Transformada Rápida de Fourier (FFT)</strong> es la herramienta matemática que lo hace posible.</p>
                    <p>La FFT descompone la señal del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energía vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qué tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matemático: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un número complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El número total de muestras en la ventana.</li>
                            <li>$k$: El índice de la frecuencia que se está calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time_domain_to_frequency_domain_FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>FFT:</strong> Algoritmo eficiente para calcular la Transformada Discreta de Fourier. <strong>Dominio del tiempo vs. frecuencia:</strong> Dos formas de representar la misma señal, enfocándose en cuándo ocurren los eventos vs. qué frecuencias contiene.
            </footer>
        </div>

        <!-- DIAPOSITIVA 15: MFCCs -->
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Más allá del Espectro</h4>
                    <p>El espectro de la FFT es útil, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho más inteligente de resumir la información del espectro, imitando cómo funciona el oído humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logarítmica, similar a nuestra percepción auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energías, de nuevo, para imitar cómo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operación que comprime toda esa información espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripción numérica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC_block_diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Escala Mel:</strong> Escala perceptual de frecuencias que imita cómo el oído humano percibe diferencias en el tono. <strong>DCT:</strong> Transformada que convierte señales al dominio de frecuencia usando solo funciones coseno. <strong>Timbre:</strong> Cualidad que diferencia sonidos con el mismo pitch y volumen.
            </footer>
        </div>

        <!-- DIAPOSITIVA 16: OPERACIÓN PROMEDIO -->
        <div class="slide hidden" data-slide="16">
            <h2 class="technical-title">Paso 4: Operación de Promedio para Vector Final</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del Análisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de características, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 características.</p>
                    <p>Para obtener un único vector $V$ que represente todo el audio, calculamos la media de cada característica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">Cálculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                            <li>$V_j$: Es el valor final de la característica $j$ en nuestro vector.</li>
                            <li>$T$: Es el número total de ventanas (frames) en el audio.</li>
                            <li>$M_{t,j}$: Es el valor de la característica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                    <h4>¿Por qué usar el promedio?</h4>
                   <ul>
                        <li><strong>Reducción temporal:</strong> Condensa información variable en el tiempo a un valor estable</li>
                        <li><strong>Robustez:</strong> El promedio es menos sensible a valores atípicos en ventanas individuales</li>
                        <li><strong>Representatividad:</strong> Captura las características dominantes del audio completo</li>
                        <li><strong>Compatibilidad:</strong> Produce un vector de tamaño fijo para cualquier duración de audio</li>
                    </ul>
                   <p>Este proceso condensa la información temporal en una sola "ficha técnica" que describe las propiedades acústicas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature_matrix_averaging_to_vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
            <footer class="footnote">
                <strong>Promedio aritmético:</strong> Suma de todos los valores dividida entre el número total de valores. <strong>Vector de tamaño fijo:</strong> Representación numérica con dimensiones constantes, independiente de la duración del audio original.
            </footer>
        </div>

        <!-- DIAPOSITIVA 17: NORMALIZACIÓN DE AUDIO -->
        <div class="slide hidden" data-slide="17">
            <h2 class="technical-title">Normalización de Audio: ¿Por qué es Crucial?</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¿Qué es la Normalización de Audio?</h4>
                    <p>La normalización ajusta la amplitud de una señal de audio para que su valor máximo sea 1.0 y el mínimo sea -1.0, estandarizando el volumen entre diferentes grabaciones.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Fórmula de Normalización</span>
                        <p>$x_{norm}[n] = \frac{x[n]}{\max(|x[n]|)}$</p>
                        <ul>
                            <li>$x[n]$: Señal de audio original</li>
                            <li>$x_{norm}[n]$: Señal normalizada</li>
                            <li>$\max(|x[n]|)$: Valor absoluto máximo de la señal</li>
                        </ul>
                    </div>

                    <h4>Ventajas Críticas de la Normalización:</h4>
                    <div class="advantages-disadvantages">
                        <div class="advantages">
                            <h4>✅ Beneficios Técnicos</h4>
                            <ul>
                                <li><strong>Consistencia de Volumen:</strong> Elimina diferencias de grabación entre micrófonos y entornos</li>
                                <li><strong>Estabilidad Numérica:</strong> Previene overflow y underflow en cálculos posteriores</li>
                                <li><strong>Mejor Convergencia:</strong> Los algoritmos de ML convergen más rápido con datos normalizados</li>
                                <li><strong>Robustez:</strong> Reduce sensibilidad a variaciones de hardware de grabación</li>
                            </ul>
                        </div>
                        <div class="disadvantages">
                            <h4>⚠️ Sin Normalización</h4>
                            <ul>
                                <li><strong>Sesgo por Volumen:</strong> Grabaciones más fuertes dominarían el entrenamiento</li>
                                <li><strong>Características Distorsionadas:</strong> MFCCs y otras características serían inconsistentes</li>
                                <li><strong>Gradientes Inestables:</strong> Entrenamiento del modelo sería errático</li>
                                <li><strong>Clasificación Sesgada:</strong> El modelo podría aprender a clasificar por volumen, no por emoción</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Overflow/Underflow:</strong> Errores numéricos cuando los valores son demasiado grandes o pequeños para ser representados. <strong>Convergencia:</strong> Proceso por el cual un algoritmo de aprendizaje alcanza una solución estable.
            </footer>
        </div>

        <!-- DIAPOSITIVA 18: PREPROCESAMIENTO -->
        <div class="slide hidden" data-slide="18">
            <h2>11. Preprocesamiento para Reducción Dimensional</h2>
            <h3>¿Qué fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarización de Características (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    Técnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las características con rangos de valores más grandes (como el pitch) dominarían a las de rangos más pequeños (como los MFCCs), sesgando el análisis.
                </p>
            </div>
            <p>La estandarización asegura que todas las características contribuyan de manera equitativa al análisis, resultando en un modelo más justo y preciso.</p>
            <footer class="footnote">
                <strong>PCA:</strong> Análisis de Componentes Principales, técnica para reducir dimensiones preservando varianza. <strong>LDA:</strong> Análisis Discriminante Lineal, reduce dimensiones maximizando separación entre clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 19: STANDARD SCALER -->
        <div class="slide hidden" data-slide="19">
            <h2>12. ¿Cómo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. Cálculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las características (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">μ = (Σx) / n</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>2. Cálculo de la Desviación Estándar</h4>
                    <p>Luego, calcula la desviación estándar, que mide cuánta variación o dispersión existe respecto a la media.</p>
                     <p class="formula">σ = √[Σ(x-μ)² / n]</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>3. Transformación (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviación estándar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - μ) / σ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> Técnica de preprocesamiento que estandariza las características al remover la media y escalar a una varianza unitaria. <strong>Z-score:</strong> Medida estadística que indica cuántas desviaciones estándar está un valor de la media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 20: PCA VS LDA -->
        <div class="slide hidden" data-slide="20">
            <h2>13. Reducción de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>Análisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersión de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>Análisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separación entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyección para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegría"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por sí mismo. <strong>Varianza:</strong> Medida de dispersión que indica cuánto se alejan los datos de su media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 21: PCA 3D -->
        <div class="slide hidden" data-slide="21">
            <h2>14. Visualización 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gráfica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoción. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersión natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Componentes Principales:</strong> Nuevos ejes calculados que capturan la máxima variabilidad de los datos originales. <strong>Proyección:</strong> Transformación de datos de alta dimensión a un espacio de menor dimensión.
            </footer>
        </div>

        <!-- DIAPOSITIVA 22: LDA 3D ACTUALIZADA -->
        <div class="slide hidden" data-slide="22">
            <h2>15. Visualización 3D: LDA con Resultados Actuales</h2>
            <div class="iframe-plot-container">
                <iframe src="Results/LDA/lda_3d_visualization.html"></iframe>
            </div>
            <p class="plot-description">
                Aquí vemos nuestros resultados reales: el LDA ha reducido las 180 características originales a solo <strong>6 componentes discriminantes</strong>, capturando el 100% de la varianza entre clases. La separación clara entre las emociones demuestra la efectividad del LDA para maximizar la distinción entre categorías emocionales.
            </p>
            <div class="stats-grid">
                <div class="stat-item"><h3>180 → 6</h3><p>Reducción dimensional (96.7%)</p></div>
                <div class="stat-item"><h3>100%</h3><p>Varianza explicada total</p></div>
                <div class="stat-item"><h3>7 clases</h3><p>Emociones separadas</p></div>
            </div>
            <footer class="footnote">
                <strong>Discriminantes Lineales (LD):</strong> Combinaciones lineales de las características originales que mejor separan las clases. <strong>Cúmulos:</strong> Agrupaciones de puntos similares en el espacio de características.
            </footer>
        </div>

        <!-- DIAPOSITIVA 23: COMPARACIÓN PCA LDA ACTUALIZADA -->
        <div class="slide hidden" data-slide="23">
            <h2>16. Resultados LDA: Análisis Detallado</h2>
            <div class="single-chart-container">
                <h3>Análisis LDA: Componentes y Separabilidad</h3>
                <img src="Results/LDA/lda_analysis_main.png" alt="Análisis LDA Principal" style="max-width: 100%; border-radius: 15px;">
            </div>
            <h3>Tabla Comparativa: Características del Análisis</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>Análisis de Componentes Principales (PCA)</th>
                        <th>Análisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Dimensiones finales</td>
                        <td>Configurables (típicamente 3-50)</td>
                        <td><strong>6 componentes</strong></td>
                    </tr>
                    <tr>
                        <td>Varianza explicada</td>
                        <td>~85-95% (depende de componentes)</td>
                        <td><strong>100%</strong></td>
                    </tr>
                    <tr>
                        <td>Separabilidad de clases</td>
                        <td>No optimizada</td>
                        <td><strong>Maximizada</strong></td>
                    </tr>
                    <tr>
                        <td>Uso de etiquetas</td>
                        <td>No utiliza las etiquetas emocionales</td>
                        <td><strong>Optimizado con etiquetas</strong></td>
                    </tr>
                </tbody>
            </table>
            <h3>Interpretación de Resultados</h3>
            <p>Nuestro LDA logró una <strong>reducción excepcional del 96.7%</strong> (de 180 a 6 características) manteniendo toda la información discriminativa. Esto demuestra que las emociones en voz pueden representarse eficientemente en un espacio de muy baja dimensión cuando se optimiza para separabilidad.</p>
            <footer class="footnote">
                <strong>Separabilidad:</strong> Medida de qué tan bien se pueden distinguir diferentes clases en un espacio de características.
            </footer>
        </div>

        <!-- DIAPOSITIVA 24: NUEVA - CORRELACIÓN LDA -->
        <div class="slide hidden" data-slide="24">
            <h2>17. Matriz de Correlación: Componentes LDA</h2>
            <div class="single-chart-container">
                <h3>Análisis de Independencia entre Componentes Discriminantes</h3>
                <img src="Results/LDA/lda_correlation_matrix.png" alt="Matriz de Correlación LDA" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Esta matriz muestra las correlaciones entre los 6 componentes LDA. Los valores cercanos a 0 (colores fríos) indican independencia entre componentes, mientras que valores cercanos a ±1 (colores cálidos) indican correlación fuerte.</p>
            <div class="chart-observation">
                <strong>Observaciones Clave:</strong> Los componentes LDA muestran correlaciones bajas entre sí, lo que confirma que cada uno captura información única y complementaria para distinguir emociones. Esta independencia es crucial para el rendimiento del modelo CNN.
            </div>
            <h3>Implicaciones para el Modelo:</h3>
            <ul>
                <li><strong>Reducción de Redundancia:</strong> Componentes independientes evitan información duplicada</li>
                <li><strong>Eficiencia Computacional:</strong> Menos parámetros en la CNN sin pérdida de información</li>
                <li><strong>Mejor Generalización:</strong> Características no redundantes mejoran el aprendizaje</li>
                <li><strong>Interpretabilidad:</strong> Cada componente representa aspectos únicos de las emociones</li>
            </ul>
            <footer class="footnote">
                <strong>Correlación:</strong> Medida estadística que indica la relación lineal entre dos variables, desde -1 (correlación negativa perfecta) hasta +1 (correlación positiva perfecta). <strong>Independencia estadística:</strong> Cuando el conocimiento de una variable no proporciona información sobre otra.
            </footer>
        </div>

        <!-- DIAPOSITIVA 25: NUEVA - DISTANCIAS CENTROIDES -->
        <div class="slide hidden" data-slide="25">
            <h2>18. Distancias entre Centroides Emocionales</h2>
            <div class="single-chart-container">
                <h3>Mapa de Separabilidad: Distancias en el Espacio LDA</h3>
                <img src="Results/LDA/emotion_centroids_distances.png" alt="Distancias entre Centroides" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Este mapa de calor muestra las distancias euclidianas entre los centroides (puntos promedio) de cada emoción en el espacio LDA de 6 dimensiones. Colores más intensos indican mayor separación entre emociones.</p>
            <div class="chart-observation">
                <strong>Interpretación Psicoacústica:</strong> Las emociones más distantes (colores brillantes) son más fáciles de distinguir para el modelo, mientras que las cercanas (colores oscuros) representan mayor desafío de clasificación. Por ejemplo, 'angry' y 'sad' podrían estar más separadas que 'happy' y 'surprised'.
            </div>
            <h3>Análisis de Confusión Predictiva:</h3>
            <ul>
                <li><strong>Máxima Separación:</strong> Emociones con distancias >3.0 rara vez se confunden</li>
                <li><strong>Separación Moderada:</strong> Distancias 1.5-3.0 ocasionalmente generan errores</li>
                <li><strong>Separación Mínima:</strong> Distancias <1.5 son propensas a confusión</li>
                <li><strong>Validación Teórica:</strong> Coincide con estudios psicológicos sobre similaridad emocional</li>
            </ul>
            <footer class="footnote">
                <strong>Centroide:</strong> Punto promedio de todas las muestras de una clase en el espacio de características. <strong>Distancia euclidiana:</strong> Medida de distancia "en línea recta" entre dos puntos en un espacio multidimensional. <strong>Psicoacústica:</strong> Estudio de la percepción psicológica del sonido.
            </footer>
        </div>

        <!-- DIAPOSITIVA 26: NUEVA - COMPARACIÓN PCA VS LDA -->
        <div class="slide hidden" data-slide="26">
            <h2>19. Comparación Visual: PCA vs LDA</h2>
            <div class="single-chart-container">
                <h3>Efectividad Comparativa para Separación de Emociones</h3>
                <img src="Results/LDA/pca_vs_lda_comparison.png" alt="Comparación PCA vs LDA" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Esta comparación lado a lado demuestra visualmente por qué elegimos LDA sobre PCA para nuestro problema de clasificación emocional.</p>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Ventajas del LDA en Nuestro Caso</h4>
                    <ul>
                        <li><strong>Separación Clara:</strong> Cúmulos emocionales bien definidos</li>
                        <li><strong>Reducción Extrema:</strong> Solo 6 dimensiones vs potenciales 50+ en PCA</li>
                        <li><strong>100% Varianza:</strong> No hay pérdida de información discriminativa</li>
                        <li><strong>Optimización Dirigida:</strong> Específicamente diseñado para clasificación</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>⚠️ Limitaciones del PCA</h4>
                    <ul>
                        <li><strong>Solapamiento:</strong> Las emociones se mezclan en el espacio PCA</li>
                        <li><strong>Varianza vs Separabilidad:</strong> Optimiza varianza, no clasificación</li>
                        <li><strong>Más Dimensiones:</strong> Requiere más componentes para capturar información</li>
                        <li><strong>Información Irrelevante:</strong> Puede preservar ruido no discriminativo</li>
                    </ul>
                </div>
            </div>
            <h3>Conclusión Metodológica:</h3>
            <p>La elección de LDA resultó crucial para el éxito del proyecto, permitiendo un modelo más eficiente y preciso al trabajar con características optimizadas para distinguir emociones.</p>
            <footer class="footnote">
                <strong>Cúmulos:</strong> Agrupaciones naturales de datos similares en el espacio de características. <strong>Información discriminativa:</strong> Características que ayudan a distinguir entre diferentes clases o categorías.
            </footer>
        </div>

        <!-- DIAPOSITIVA 27: INTRODUCCIÓN CNN 1D -->
        <div class="slide hidden" data-slide="27">
            <h2>20. Introducción a las Redes Neuronales Convolucionales 1D</h2>
            <div class="highlight-box">
                <h3>¿Qué es una CNN 1D?</h3>
                <p>Una Red Neuronal Convolucional 1D es un tipo especializado de red neuronal diseñada para procesar secuencias de datos, como series temporales o características extraídas de audio.</p>
            </div>
            <h3>¿Por qué CNN 1D para Audio + LDA?</h3>
            <ul>
                <li><strong>Entrada Optimizada:</strong> Procesa directamente las 6 características LDA discriminantes</li>
                <li><strong>Detección de Patrones:</strong> Identifica relaciones entre componentes discriminantes</li>
                <li><strong>Eficiencia Extrema:</strong> Solo 7,911 parámetros vs modelos tradicionales de 50K+</li>
                <li><strong>Precisión Mantenida:</strong> 80.07% con entrada ultra-comprimida</li>
            </ul>
            <h3>Nuestra Arquitectura Innovadora</h3>
            <p>Diseñamos una CNN 1D ultra-eficiente que toma como entrada un vector de solo <strong>6 características LDA</strong> y produce probabilidades para 7 emociones diferentes, logrando una precisión excepcional con mínimos recursos computacionales.</p>
            <footer class="footnote">
                <strong>CNN:</strong> Convolutional Neural Network, tipo de red neuronal que usa operaciones de convolución. <strong>1D:</strong> Unidimensional, opera sobre secuencias lineales de datos. <strong>Características discriminantes:</strong> Representaciones optimizadas para distinguir entre clases diferentes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 28: CAPA DE ENTRADA ACTUALIZADA -->
        <div class="slide hidden" data-slide="28">
            <h2>21. Capa de Entrada (Input Layer) - Optimizada con LDA</h2>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Entrada:</strong> <span class="output-shape">(None, 6, 1)</span></p>
                        <p><strong>Neuronas:</strong> 6 (una por componente LDA)</p>
                        <p><strong>Características:</strong> 6 componentes LDA</p>
                        <p><strong>Función:</strong> Recibir vector discriminante optimizado</p>
                        <p><strong>Parámetros:</strong> 0 (no aprende, solo recibe datos)</p>
                    </div>
                </div>
            </div>
            <h3>Detalles Técnicos de la Optimización</h3>
            <ul>
                <li><strong>6 neuronas de entrada:</strong> Una neurona por cada componente discriminante LDA</li>
                <li><strong>Reducción del 96.7%:</strong> Desde 180 características originales</li>
                <li><strong>Varianza preservada:</strong> 100% de la información discriminativa mantenida</li>
                <li><strong>Formato compacto:</strong> Entrada ultra-eficiente para la CNN</li>
                <li><strong>None (batch):</strong> Permite procesar múltiples audios simultáneamente</li>
            </ul>
            <h3>Pipeline de Preprocesamiento</h3>
            <p>Antes de entrar a la red, los datos fueron:</p>
            <ul>
                <li>✅ Normalizados (StandardScaler) en 180 características originales</li>
                <li>✅ Reducidos con LDA a 6 componentes discriminantes</li>
                <li>✅ Reformateados a shape (6, 1) para CNN 1D</li>
                <li>✅ Divididos estratificadamente en train/validation/test</li>
            </ul>
            <footer class="footnote">
                <strong>Neurona de entrada:</strong> Unidad que recibe un valor de entrada específico sin procesarlo. <strong>Batch:</strong> Conjunto de muestras procesadas simultáneamente para eficiencia computacional. <strong>Shape:</strong> Dimensiones de los datos (filas, columnas, canales). <strong>Estratificado:</strong> División que mantiene la proporción de clases en cada subset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 29: PRIMERA CONVOLUCIÓN ACTUALIZADA -->
        <div class="slide hidden" data-slide="29">
            <h2>22. Primera Capa Convolucional - Adaptada para LDA</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_1 (64 filtros, kernel=3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 1)</p>
                        <p><strong>Salida:</strong> (None, 6, 64)</p>
                        <p><strong>Neuronas activadas:</strong> 384 (6 posiciones × 64 filtros)</p>
                        <p><strong>Parámetros:</strong> 256 (192 pesos + 64 bias)</p>
                        <p><strong>Activación:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¿Qué hace esta capa con las características LDA?</h3>
            <ul>
                <li><strong>64 filtros especializados:</strong> Cada uno aprende a detectar patrones específicos entre componentes LDA</li>
                <li><strong>384 neuronas:</strong> 6 posiciones × 64 filtros = 384 activaciones por muestra</li>
                <li><strong>Kernel size 3:</strong> Analiza relaciones entre 3 componentes discriminantes consecutivos</li>
                <li><strong>Padding 'same':</strong> Mantiene las 6 dimensiones LDA</li>
                <li><strong>ReLU:</strong> Activa solo patrones discriminantes positivos</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Convolución sobre LDA</span>
                <p>Para cada posición $i$ y filtro $f$:</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{2} w_j^f \cdot LDA_{i+j} + b^f\right)$</p>
                <ul>
                    <li>$w_j^f$: Peso $j$ del filtro $f$ (3 pesos por filtro)</li>
                    <li>$LDA_{i+j}$: Componente LDA en posición $i+j$</li>
                    <li>$b^f$: Bias del filtro $f$ (64 bias total)</li>
                    <li>Total pesos: 3 × 64 = 192 pesos</li>
                </ul>
            </div>
            <h3>Cálculo Detallado de Parámetros:</h3>
            <ul>
                <li><strong>Pesos de convolución:</strong> 3 (kernel) × 1 (canal entrada) × 64 (filtros) = 192</li>
                <li><strong>Parámetros bias:</strong> 64 (uno por filtro)</li>
                <li><strong>Total parámetros:</strong> 192 + 64 = 256</li>
                <li><strong>Eficiencia lograda:</strong> 66% reducción vs versión original (768 parámetros)</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas activadas:</strong> Cantidad de valores de salida que produce la capa (posiciones × filtros). <strong>Filtro/Kernel:</strong> Conjunto de pesos que se desliza sobre los datos para detectar patrones. <strong>Componentes LDA:</strong> Características discriminantes optimizadas para separar clases emocionales. <strong>Bias:</strong> Parámetro que permite ajustar el umbral de activación.
            </footer>
        </div>

        <!-- DIAPOSITIVA 30: BATCH NORMALIZATION 1 -->
        <div class="slide hidden" data-slide="30">
            <h2>23. Primera Capa de Normalización por Lotes</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm">
                    <h4>BatchNormalization_1</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 64)</p>
                        <p><strong>Salida:</strong> (None, 6, 64)</p>
                        <p><strong>Neuronas procesadas:</strong> 384 (6 × 64)</p>
                        <p><strong>Parámetros:</strong> 256 (128 γ + 128 β + 128 μ + 128 σ²)</p>
                        <p><strong>Función:</strong> Normalizar activaciones y estabilizar entrenamiento</p>
                    </div>
                </div>
            </div>
            <h3>¿Qué hace BatchNormalization después de Conv1D?</h3>
            <ul>
                <li><strong>Normalización por canal:</strong> Cada uno de los 64 filtros se normaliza independientemente</li>
                <li><strong>384 neuronas normalizadas:</strong> Procesa todas las activaciones (6 posiciones × 64 canales)</li>
                <li><strong>Estabilización del gradiente:</strong> Reduce el desplazamiento covariante interno</li>
                <li><strong>Aceleración del entrenamiento:</strong> Permite usar learning rates más altos</li>
                <li><strong>Regularización implícita:</strong> Añade ruido controlado que actúa como regularizador</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Batch Normalization</span>
                <p>Para cada canal $c$ de los 64 filtros:</p>
                <p>$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}$</p>
                <p>$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$</p>
                <ul>
                    <li>$\mu_c$: Media del canal $c$ en el batch</li>
                    <li>$\sigma_c^2$: Varianza del canal $c$ en el batch</li>
                    <li>$\gamma_c$: Parámetro de escala aprendible (64 total)</li>
                    <li>$\beta_c$: Parámetro de desplazamiento aprendible (64 total)</li>
                    <li>$\epsilon = 1e-3$: Constante para estabilidad numérica</li>
                </ul>
            </div>
            <h3>Configuración Específica para LDA:</h3>
            <ul>
                <li><strong>Momentum:</strong> 0.99 (memoria de estadísticas durante entrenamiento)</li>
                <li><strong>Axis:</strong> -1 (normalización por canal de filtro)</li>
                <li><strong>Center:</strong> True (usar parámetro β)</li>
                <li><strong>Scale:</strong> True (usar parámetro γ)</li>
                <li><strong>64 canales normalizados:</strong> Uno por cada filtro convolucional</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas procesadas:</strong> Cantidad total de activaciones que la capa normaliza. <strong>Desplazamiento covariante interno:</strong> Cambio en la distribución de activaciones durante entrenamiento que ralentiza convergencia. <strong>Parámetros aprendibles:</strong> γ y β se optimizan durante backpropagation. <strong>Estadísticas del batch:</strong> Media y varianza calculadas sobre el mini-batch actual.
            </footer>
        </div>

        <!-- DIAPOSITIVA 31: MAX POOLING 1 -->
        <div class="slide hidden" data-slide="31">
            <h2>24. Primera Capa de Max Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>MaxPooling1D_1 (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 64)</p>
                        <p><strong>Neuronas resultantes:</strong> 192 (3 × 64)</p>
                        <p><strong>Parámetros:</strong> 0 (operación determinística)</p>
                        <p><strong>Función:</strong> Reducción dimensional y extracción de características dominantes</p>
                    </div>
                </div>
            </div>
            <h3>¿Cómo funciona Max Pooling en características LDA?</h3>
            <ul>
                <li><strong>Reducción 6→3:</strong> Toma ventanas de tamaño 2 y selecciona el valor máximo</li>
                <li><strong>192 neuronas finales:</strong> 3 posiciones × 64 canales = 192 activaciones</li>
                <li><strong>Invarianza traslacional:</strong> Hace el modelo robusto a pequeños desplazamientos</li>
                <li><strong>Compresión de información:</strong> Retiene solo las activaciones más fuertes</li>
                <li><strong>Reducción computacional:</strong> 50% menos activaciones para capas posteriores</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación Max Pooling 1D</span>
                <p>Para cada canal $c$ y posición de salida $j$:</p>
                <p>$y_{j,c} = \max(x_{2j,c}, x_{2j+1,c})$</p>
                <p>Donde:</p>
                <ul>
                    <li>$x_{i,c}$: Activación en posición $i$ del canal $c$</li>
                    <li>$y_{j,c}$: Salida en posición $j$ del canal $c$</li>
                    <li>Pool size = 2: Ventana de pooling</li>
                    <li>Stride = 2: Sin solapamiento entre ventanas</li>
                </ul>
            </div>
            
            <h3>Mapeo Específico para Componentes LDA:</h3>
            <ul>
                <li><strong>LDA_1, LDA_2 → Posición 0:</strong> max(activaciones de componentes 1-2)</li>
                <li><strong>LDA_3, LDA_4 → Posición 1:</strong> max(activaciones de componentes 3-4)</li>
                <li><strong>LDA_5, LDA_6 → Posición 2:</strong> max(activaciones de componentes 5-6)</li>
                <li><strong>Preservación de 64 canales:</strong> Cada filtro convolucional mantiene su representación</li>
                <li><strong>384 → 192 neuronas:</strong> Reducción exacta del 50%</li>
            </ul>

            <h3>Ventajas en el Contexto LDA:</h3>
            <ul>
                <li><strong>Robustez a variaciones:</strong> Compensa pequeñas fluctuaciones en componentes discriminantes</li>
                <li><strong>Eficiencia computacional:</strong> 50% de reducción en dimensiones temporales</li>
                <li><strong>Foco en características dominantes:</strong> Enfatiza patrones más fuertes</li>
                <li><strong>Preparación para siguiente capa:</strong> Entrada optimizada para Conv1D_2</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas resultantes:</strong> Cantidad de activaciones que produce la capa tras el pooling. <strong>Invarianza traslacional:</strong> Propiedad que hace que el modelo sea insensible a pequeños desplazamientos en la entrada. <strong>Stride:</strong> Paso con el que se mueve la ventana de pooling. <strong>Determinística:</strong> Operación que siempre produce el mismo resultado para la misma entrada.
            </footer>
        </div>

        <!-- DIAPOSITIVA 32: DROPOUT 1 -->
        <div class="slide hidden" data-slide="32">
            <h2>25. Primera Capa de Dropout (Regularización)</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_1 (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 64)</p>
                        <p><strong>Neuronas regularizadas:</strong> 192 (3 × 64)</p>
                        <p><strong>Parámetros:</strong> 0 (máscara aleatoria)</p>
                        <p><strong>Función:</strong> Prevenir overfitting mediante desactivación estocástica</p>
                    </div>
                </div>
            </div>
            <h3>¿Cómo funciona Dropout en nuestro modelo?</h3>
            <ul>
                <li><strong>Probabilidad de desactivación:</strong> 30% de las neuronas se apagan aleatoriamente</li>
                <li><strong>192 neuronas afectadas:</strong> Cada una de las 3×64 activaciones tiene 30% prob. de ser 0</li>
                <li><strong>Solo durante entrenamiento:</strong> En inferencia todas las neuronas están activas</li>
                <li><strong>Escalado compensatorio:</strong> Las neuronas activas se escalan por factor 1/(1-0.3)</li>
                <li><strong>Prevención de co-adaptación:</strong> Evita dependencias excesivas entre neuronas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Dropout</span>
                <p><strong>Durante entrenamiento:</strong></p>
                <p>$r_{i,c} \sim \text{Bernoulli}(1-p)$ donde $p = 0.3$</p>
                <p>$y_{i,c} = \frac{r_{i,c} \cdot x_{i,c}}{1-p}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_{i,c} = x_{i,c}$</p>
                <ul>
                    <li>$r_{i,c}$: Máscara binaria aleatoria (0 o 1)</li>
                    <li>$p = 0.3$: Tasa de dropout</li>
                    <li>$1-p = 0.7$: Probabilidad de mantener neurona</li>
                    <li>Escalado por $\frac{1}{1-p}$: Compensar activaciones perdidas</li>
                </ul>
            </div>
            
            <h3>Configuración Específica para Características LDA:</h3>
            <ul>
                <li><strong>Rate moderado (0.3):</strong> Balance entre regularización y preservación de información LDA</li>
                <li><strong>Aplicación por neurona:</strong> Cada activación (3×64=192) tiene 30% prob. de ser 0</li>
                <li><strong>~58 neuronas activas:</strong> En promedio 192×0.7 = 134 neuronas activas por forward pass</li>
                <li><strong>Seed reproductible:</strong> Determinismo para experimentación controlada</li>
                <li><strong>Training=True automático:</strong> Keras controla el modo según la fase</li>
            </ul>

            <h3>Impacto en el Aprendizaje:</h3>
            <ul>
                <li><strong>Generalización mejorada:</strong> El modelo no depende excesivamente de neuronas específicas</li>
                <li><strong>Robustez a componentes LDA:</strong> Aprende patrones usando subconjuntos de activaciones</li>
                <li><strong>Reducción de varianza:</strong> Ensemble implícito de redes más pequeñas</li>
                <li><strong>Calibración de confianza:</strong> Mejora la estimación de incertidumbre del modelo</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas regularizadas:</strong> Cantidad total de activaciones que pueden ser desactivadas por dropout. <strong>Overfitting:</strong> Sobreajuste donde el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Co-adaptación:</strong> Dependencia excesiva entre neuronas que reduce robustez. <strong>Bernoulli:</strong> Distribución de probabilidad binaria (0 o 1). <strong>Ensemble implícito:</strong> Dropout simula el promedio de múltiples redes diferentes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 33: SEGUNDA CONVOLUCIÓN -->
        <div class="slide hidden" data-slide="33">
            <h2>26. Segunda Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_2 (32 filtros, kernel=3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 32)</p>
                        <p><strong>Neuronas activadas:</strong> 96 (3 × 32)</p>
                        <p><strong>Parámetros:</strong> 6,176 (6,144 pesos + 32 bias)</p>
                        <p><strong>Activación:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¿Qué patrones detecta la segunda convolución?</h3>
            <ul>
                <li><strong>32 filtros especializados:</strong> Cada uno aprende combinaciones complejas de características LDA</li>
                <li><strong>96 neuronas finales:</strong> 3 posiciones × 32 filtros = 96 activaciones por muestra</li>
                <li><strong>Patrones de orden superior:</strong> Detecta relaciones entre las salidas de la primera capa</li>
                <li><strong>Compresión inteligente:</strong> Reduce de 64 a 32 canales manteniendo información crítica</li>
                <li><strong>Representaciones abstractas:</strong> Características más alejadas de los componentes LDA originales</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Convolución Segunda Capa</span>
                <p>Para cada posición $i$ y filtro $f$ (de 32 filtros):</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{2} \sum_{c=0}^{63} w_{j,c}^f \cdot x_{i+j,c} + b^f\right)$</p>
                <ul>
                    <li>$w_{j,c}^f$: Peso del filtro $f$ en posición $j$ y canal $c$</li>
                    <li>$x_{i+j,c}$: Salida de la primera capa en posición $i+j$, canal $c$</li>
                    <li>$b^f$: Bias del filtro $f$</li>
                    <li>Kernel size = 3: Analiza 3 posiciones consecutivas</li>
                    <li>64 canales de entrada: Toda la información de Conv1D_1</li>
                </ul>
            </div>

            <h3>Cálculo Detallado de Parámetros:</h3>
            <ul>
                <li><strong>Pesos de convolución:</strong> 3 (kernel) × 64 (canales entrada) × 32 (filtros) = 6,144</li>
                <li><strong>Biases:</strong> 32 (uno por filtro)</li>
                <li><strong>Total:</strong> 6,144 + 32 = 6,176 parámetros</li>
                <li><strong>Porcentaje del modelo:</strong> 78.1% de todos los parámetros entrenables</li>
                <li><strong>192 → 96 neuronas:</strong> Reducción del 50% en activaciones</li>
            </ul>

            <h3>Especialización en Características Emocionales:</h3>
            <ul>
                <li><strong>Detectores de valencias:</strong> Algunos filtros se especializan en emociones positivas/negativas</li>
                <li><strong>Detectores de activación:</strong> Otros en emociones alta/baja energía</li>
                <li><strong>Combinaciones complejas:</strong> Patrones que no son evidentes en componentes LDA individuales</li>
                <li><strong>Invarianzas aprendidas:</strong> Robustez a variaciones específicas del hablante</li>
            </ul>

            <h3>Reducción de Dimensionalidad Inteligente:</h3>
            <p>La reducción de 64 a 32 filtros no es pérdida de información, sino <strong>compresión inteligente</strong> donde cada filtro de salida capture patrones más complejos y específicos para la tarea de clasificación emocional.</p>
            <footer class="footnote">
                <strong>Neuronas activadas:</strong> Cantidad total de valores de salida (posiciones × filtros). <strong>Orden superior:</strong> Patrones más complejos formados por combinaciones de características más simples. <strong>Valencias:</strong> Dimensión emocional que va de negativa (tristeza) a positiva (alegría). <strong>Activación emocional:</strong> Intensidad o energía de una emoción (calma vs excitación). <strong>Invarianzas:</strong> Propiedades que permanecen constantes ante ciertas transformaciones.
            </footer>
        </div>

        <!-- DIAPOSITIVA 34: BATCH NORMALIZATION 2 -->
        <div class="slide hidden" data-slide="34">
            <h2>27. Segunda Capa de Normalización por Lotes</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm">
                    <h4>BatchNormalization_2</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 32)</p>
                        <p><strong>Salida:</strong> (None, 3, 32)</p>
                        <p><strong>Neuronas normalizadas:</strong> 96 (3 × 32)</p>
                        <p><strong>Parámetros:</strong> 128 (64 γ + 64 β + 64 μ + 64 σ²)</p>
                        <p><strong>Función:</strong> Normalizar activaciones de segunda convolución</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué normalizar después de la segunda convolución?</h3>
            <ul>
                <li><strong>Estabilización profunda:</strong> Las capas más profundas necesitan más normalización</li>
                <li><strong>32 canales especializados:</strong> Cada canal representa patrones emocionales complejos</li>
                <li><strong>96 neuronas estabilizadas:</strong> Todas las activaciones (3×32) son normalizadas</li>
                <li><strong>Preparación para pooling:</strong> Activaciones normalizadas mejoran el max pooling posterior</li>
                <li><strong>Convergencia acelerada:</strong> Gradientes más estables en capas profundas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Normalización de Características de Alto Nivel</span>
                <p>Para cada uno de los 32 canales especializados:</p>
                <p>$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c^{(batch)}}{\sqrt{(\sigma_c^{(batch)})^2 + \epsilon}}$</p>
                <p>$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$</p>
                <ul>
                    <li>$\mu_c^{(batch)}$: Media del canal $c$ en el batch actual</li>
                    <li>$(\sigma_c^{(batch)})^2$: Varianza del canal $c$ en el batch</li>
                    <li>$\gamma_c, \beta_c$: Parámetros aprendibles específicos por canal (32 cada uno)</li>
                    <li>Cada canal representa detectores emocionales especializados</li>
                </ul>
            </div>

            <h3>Estadísticas de Entrenamiento vs Inferencia:</h3>
            <ul>
                <li><strong>Entrenamiento:</strong> Usa estadísticas del batch actual (32 muestras)</li>
                <li><strong>Moving averages:</strong> Actualiza medias y varianzas acumulativas con momentum=0.99</li>
                <li><strong>Inferencia:</strong> Usa estadísticas acumuladas durante todo el entrenamiento</li>
                <li><strong>Estabilidad:</strong> Comportamiento consistente independiente del tamaño de batch</li>
                <li><strong>96 valores normalizados:</strong> Cada activación es procesada individualmente</li>
            </ul>

            <h3>Impacto en Características Emocionales:</h3>
            <ul>
                <li><strong>Calibración de intensidad:</strong> Normaliza la "fuerza" de diferentes detectores emocionales</li>
                <li><strong>Comparabilidad entre filtros:</strong> Cada canal tiene distribución similar</li>
                <li><strong>Reducción de sesgo:</strong> Evita que ciertos filtros dominen por escala, no por relevancia</li>
                <li><strong>Robustez inter-hablante:</strong> Compensa variaciones individuales en características vocales</li>
            </ul>

            <h3>Parámetros Aprendibles Específicos:</h3>
            <ul>
                <li><strong>32 parámetros γ:</strong> Controlan la escala óptima para cada detector emocional</li>
                <li><strong>32 parámetros β:</strong> Ajustan el offset para cada tipo de patrón emocional</li>
                <li><strong>Especialización automática:</strong> El modelo aprende qué normalización es óptima por canal</li>
                <li><strong>Flexibilidad adaptativa:</strong> Puede "desactivar" la normalización si no es útil</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas normalizadas:</strong> Cantidad total de activaciones que la capa procesa para normalización. <strong>Moving averages:</strong> Promedio móvil que actualiza estadísticas gradualmente con cada batch. <strong>Momentum:</strong> Factor (0.99) que controla qué tan rápido se actualizan las estadísticas acumuladas. <strong>Offset:</strong> Desplazamiento que permite ajustar el punto central de la distribución. <strong>Calibración:</strong> Proceso de ajustar escalas para que sean comparables.
            </footer>
        </div>

        <!-- DIAPOSITIVA 35: MAX POOLING 2 -->
        <div class="slide hidden" data-slide="35">
            <h2>28. Segunda Capa de Max Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>MaxPooling1D_2 (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 32)</p>
                        <p><strong>Salida:</strong> (None, 1, 32)</p>
                        <p><strong>Neuronas resultantes:</strong> 32 (1 × 32)</p>
                        <p><strong>Parámetros:</strong> 0 (operación determinística)</p>
                        <p><strong>Función:</strong> Compresión final hacia representación global</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué comprimir a una sola posición temporal?</h3>
            <ul>
                <li><strong>Agregación global:</strong> Condensa información de todos los componentes LDA en un vector</li>
                <li><strong>32 neuronas finales:</strong> Una representación ultra-compacta del audio completo</li>
                <li><strong>Preparación para clasificación:</strong> Transición hacia capas densas finales</li>
                <li><strong>Invarianza posicional completa:</strong> El orden de componentes LDA se vuelve irrelevante</li>
                <li><strong>Máxima compresión:</strong> De 3 posiciones temporales a 1 representación global</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Agregación Final de Características LDA</span>
                <p>Para cada canal de detección emocional $c$ (de 32):</p>
                <p>$y_c = \max(x_{0,c}, x_{1,c}, x_{2,c})$</p>
                <p>Donde las 3 posiciones representan:</p>
                <ul>
                    <li>$x_{0,c}$: Patrones de componentes LDA 1-2</li>
                    <li>$x_{1,c}$: Patrones de componentes LDA 3-4</li>
                    <li>$x_{2,c}$: Patrones de componentes LDA 5-6</li>
                    <li>$y_c$: Activación máxima (más fuerte) del detector $c$</li>
                </ul>
            </div>

            <h3>Transición Crítica en la Arquitectura:</h3>
            <ul>
                <li><strong>De secuencial a global:</strong> Ya no importa la posición específica en la secuencia LDA</li>
                <li><strong>Vector de características finales:</strong> 32 valores que resumen todo el audio</li>
                <li><strong>Preparación para clasificación:</strong> Formato compatible con capas densas</li>
                <li><strong>Compresión máxima preservando información:</strong> Cada canal mantiene su activación más fuerte</li>
                <li><strong>96 → 32 neuronas:</strong> Reducción del 66.7%</li>
            </ul>

            <h3>

        <!-- Continúa con las diapositivas 28-41 (arquitectura CNN) en orden correcto -->
        <!-- [Diapositivas 28-41 se mantienen igual que en tu documento original] -->
<!-- DIAPOSITIVA 36: DROPOUT 2 -->
        <div class="slide hidden" data-slide="36">
            <h2>29. Segunda Capa de Dropout</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_2 (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 1, 32)</p>
                        <p><strong>Salida:</strong> (None, 1, 32)</p>
                        <p><strong>Neuronas regularizadas:</strong> 32 (1 × 32)</p>
                        <p><strong>Parámetros:</strong> 0 (máscara estocástica)</p>
                        <p><strong>Función:</strong> Regularización antes de capas densas finales</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué aplicar Dropout antes de las capas densas?</h3>
            <ul>
                <li><strong>Momento crítico:</strong> Justo antes de la clasificación final es donde más overfitting puede ocurrir</li>
                <li><strong>32 características concentradas:</strong> Cada una altamente informativa y propensa a memorización</li>
                <li><strong>~22 neuronas activas:</strong> En promedio 32×0.7 = 22 neuronas activas por forward pass</li>
                <li><strong>Preparación para generalización:</strong> El modelo debe funcionar con subconjuntos de características</li>
                <li><strong>Calibración de confianza:</strong> Mejora la estimación de incertidumbre en predicciones</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Dropout en Características Concentradas</span>
                <p><strong>Durante entrenamiento (cada forward pass):</strong></p>
                <p>Para cada característica $i$ de las 32:</p>
                <p>$m_i \sim \text{Bernoulli}(0.7)$</p>
                <p>$y_i = \frac{m_i \cdot x_i}{0.7}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_i = x_i$</p>
                <ul>
                    <li>$m_i$: Máscara binaria (70% prob. de ser 1)</li>
                    <li>30% de características se anulan aleatoriamente</li>
                    <li>Factor $\frac{1}{0.7}$: Compensa características perdidas</li>
                    <li>Cada característica representa un detector emocional específico</li>
                </ul>
            </div>

            <h3>Impacto en Detectores Emocionales:</h3>
            <ul>
                <li><strong>Robustez de detectores:</strong> El modelo aprende a no depender de un solo detector</li>
                <li><strong>Redundancia funcional:</strong> Múltiples detectores pueden señalar la misma emoción</li>
                <li><strong>Combinaciones variables:</strong> En cada entrenamiento usa subconjuntos diferentes de los 32 detectores</li>
                <li><strong>Generalización mejorada:</strong> Reduce sobreajuste a patrones específicos del conjunto de entrenamiento</li>
            </ul>

            <h3>Estrategia de Regularización en Cascada:</h3>
            <ul>
                <li><strong>Dropout_1 (post-Conv1D_1):</strong> Regulariza características de bajo nivel (192 neuronas)</li>
                <li><strong>Dropout_2 (pre-Dense):</strong> Regulariza características de alto nivel concentradas (32 neuronas)</li>
                <li><strong>Sinergia de regularización:</strong> Dos puntos estratégicos de control de overfitting</li>
                <li><strong>Rate constante (0.3):</strong> Balance probado entre regularización y preservación de información</li>
            </ul>

            <h3>Preparación Óptima para Clasificación:</h3>
            <ul>
                <li><strong>Vector robusto:</strong> Las características que sobreviven son más confiables</li>
                <li><strong>Incertidumbre controlada:</strong> El modelo "sabe" cuándo no está seguro</li>
                <li><strong>Ensemble implícito:</strong> Simula el promedio de 2^32 redes diferentes</li>
                <li><strong>Calibración de probabilidades:</strong> Las probabilidades de softmax son más realistas</li>
            </ul>

            <h3>Diferencia con Dropout_1:</h3>
            <p>Mientras Dropout_1 opera sobre 192 activaciones de nivel medio, Dropout_2 opera sobre 32 características altamente procesadas y específicas para emociones, haciendo que cada desactivación tenga mayor impacto en el aprendizaje.</p>
            <footer class="footnote">
                <strong>Neuronas regularizadas:</strong> Cantidad total de activaciones sujetas a desactivación aleatoria. <strong>Estocástica:</strong> Proceso que involucra aleatoriedad controlada. <strong>Redundancia funcional:</strong> Múltiples elementos que pueden realizar la misma función. <strong>Ensemble:</strong> Combinación de múltiples modelos para mejorar predicciones. <strong>Calibración de probabilidades:</strong> Ajuste para que las probabilidades del modelo reflejen la confianza real.
            </footer>
        </div>

        <!-- DIAPOSITIVA 37: GLOBAL AVERAGE POOLING -->
        <div class="slide hidden" data-slide="37">
            <h2>30. Capa de Global Average Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>GlobalAveragePooling1D</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 1, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas aplanadas:</strong> 32</p>
                        <p><strong>Parámetros:</strong> 0 (operación matemática pura)</p>
                        <p><strong>Función:</strong> Aplanar manteniendo información de cada canal</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué Global Average Pooling en lugar de Flatten?</h3>
            <ul>
                <li><strong>Reducción de parámetros:</strong> Evita conexiones densas masivas</li>
                <li><strong>Regularización implícita:</strong> Menos parámetros = menos overfitting</li>
                <li><strong>32 valores preservados:</strong> Cada canal mantiene su significado como detector emocional</li>
                <li><strong>Interpretabilidad:</strong> Cada canal mantiene su significado como detector emocional</li>
                <li><strong>Traducción directa:</strong> De representación 2D a vector de características</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación Global Average Pooling</span>
                <p>Para cada canal/detector emocional $c$ (de 32):</p>
                <p>$y_c = \frac{1}{T} \sum_{t=0}^{T-1} x_{t,c}$</p>
                <p>En nuestro caso específico (T=1):</p>
                <p>$y_c = x_{0,c}$</p>
                <ul>
                    <li>$T = 1$: Solo una posición temporal tras MaxPooling_2</li>
                    <li>$x_{0,c}$: Activación del canal $c$ en la única posición</li>
                    <li>$y_c$: Valor final del detector emocional $c$ (32 valores totales)</li>
                    <li>Operación equivalente a un reshape: (None, 1, 32) → (None, 32)</li>
                </ul>
            </div>

            <h3>Ventajas sobre Alternativas:</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Global Average Pooling</h4>
                    <ul>
                        <li><strong>0 parámetros adicionales:</strong> No aumenta complejidad</li>
                        <li><strong>Preserva semántica:</strong> Cada valor mantiene significado de detector</li>
                        <li><strong>Regularización natural:</strong> Promedio es más robusto que valor único</li>
                        <li><strong>Escalabilidad:</strong> Funciona independiente del tamaño de entrada</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>❌ Flatten tradicional</h4>
                    <ul>
                        <li><strong>Mismo resultado aquí:</strong> Con T=1 es equivalente</li>
                        <li><strong>Menos semántico:</strong> Perdería interpretación de canales</li>
                        <li><strong>Mayor acoplamiento:</strong> Más dependiente de arquitectura específica</li>
                        <li><strong>Menos flexible:</strong> Cambios en capas previas afectan más</li>
                    </ul>
                </div>
            </div>

            <h3>Transición a Representación Final:</h3>
            <ul>
                <li><strong>Vector de 32 características:</strong> Cada una es un detector emocional especializado</li>
                <li><strong>Preparación para Dense:</strong> Formato estándar para capas totalmente conectadas</li>
                <li><strong>Compresión extrema:</strong> De 180 características originales a 32 finales</li>
                <li><strong>Información concentrada:</strong> Cada valor representa patrones emocionales complejos</li>
            </ul>

            <h3>Interpretación de las 32 Características:</h3>
            <ul>
                <li><strong>Detectores de valencia:</strong> Características que distinguen emociones positivas/negativas</li>
                <li><strong>Detectores de activación:</strong> Características que miden intensidad emocional</li>
                <li><strong>Detectores específicos:</strong> Cada uno especializado en patrones de emociones particulares</li>
                <li><strong>Detectores contextuales:</strong> Patrones que consideran combinaciones emocionales</li>
            </ul>

            <h3>Preparación para Clasificación Final:</h3>
            <p>Estas 32 características representan la <strong>esencia emocional comprimida</strong> del audio original, extraída a través del pipeline LDA→Conv1D→Pooling. Cada valor en este vector es el resultado de procesar 180 características originales a través de múltiples capas de abstracción.</p>
            <footer class="footnote">
                <strong>Neuronas aplanadas:</strong> Cantidad de valores en el vector final tras el aplanamiento. <strong>Aplanar (Flatten):</strong> Convertir tensor multidimensional en vector unidimensional. <strong>Semántica:</strong> Significado o interpretación de cada elemento en el contexto del problema. <strong>Acoplamiento:</strong> Grado de dependencia entre componentes del sistema. <strong>Esencia comprimida:</strong> Representación minimal que captura la información más importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 38: DENSE INTERMEDIA -->
        <div class="slide hidden" data-slide="38">
            <h2>31. Capa Densa Intermedia</h2>
            <div class="neural-architecture">
                <div class="layer-box dense">
                    <h4>Dense_Intermediate (32 neuronas)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas totales:</strong> 32</p>
                        <p><strong>Parámetros:</strong> 1,056 (1,024 pesos + 32 bias)</p>
                        <p><strong>Activación:</strong> ReLU</p>
                        <p><strong>Función:</strong> Refinamiento y combinación de detectores emocionales</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué una capa densa con la misma dimensión?</h3>
            <ul>
                <li><strong>Refinamiento de características:</strong> Permite interacciones no lineales entre detectores</li>
                <li><strong>32 neuronas refinadas:</strong> Cada neurona puede combinar información de las 32 características de entrada</li>
                <li><strong>Combinaciones emocionales:</strong> Aprende relaciones complejas entre patrones emocionales</li>
                <li><strong>Preparación para decisión:</strong> Última oportunidad de procesamiento antes de clasificación</li>
                <li><strong>Capacidad computacional:</strong> 1,056 parámetros para aprender interacciones específicas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Transformación Densa con Interacciones Completas</span>
                <p>Para cada neurona de salida $j$ (de 32):</p>
                <p>$z_j = \sum_{i=0}^{31} w_{i,j} \cdot x_i + b_j$</p>
                <p>$y_j = \max(0, z_j)$ (ReLU)</p>
                <ul>
                    <li>$w_{i,j}$: Peso de conexión desde detector $i$ a neurona $j$</li>
                    <li>$x_i$: Activación del detector emocional $i$</li>
                    <li>$b_j$: Bias de la neurona $j$</li>
                    <li>32×32 = 1,024 pesos + 32 biases = 1,056 parámetros</li>
                </ul>
            </div>

            <h3>Cálculo Detallado de Parámetros:</h3>
            <ul>
                <li><strong>Matriz de pesos:</strong> 32 (entrada) × 32 (salida) = 1,024 conexiones</li>
                <li><strong>Vector de biases:</strong> 32 valores (uno por neurona de salida)</li>
                <li><strong>Total:</strong> 1,024 + 32 = 1,056 parámetros</li>
                <li><strong>Porcentaje del modelo:</strong> 13.3% de parámetros entrenables</li>
                <li><strong>Conexiones por neurona:</strong> Cada neurona de salida recibe 32 conexiones</li>
            </ul>

            <h3>Tipos de Interacciones Aprendidas:</h3>
            <ul>
                <li><strong>Inhibición competitiva:</strong> Cuando un detector de alegría se activa, suprime detectores de tristeza</li>
                <li><strong>Reforzamiento sinérgico:</strong> Detectores complementarios se refuerzan mutuamente</li>
                <li><strong>Contextualización:</strong> Un detector puede cambiar interpretación según otros activos</li>
                <li><strong>Calibración de intensidad:</strong> Ajuste de la "fuerza" relativa entre diferentes detectores</li>
            </ul>

            <h3>Especialización en Patrones Emocionales:</h3>
            <ul>
                <li><strong>Neuronas de valencia:</strong> Algunas se especializan en balance positivo/negativo</li>
                <li><strong>Neuronas de activación:</strong> Otras en distinguir alta/baja energía emocional</li>
                <li><strong>Neuronas de categoría:</strong> Especializadas en emociones específicas (alegría, tristeza, etc.)</li>
                <li><strong>Neuronas de contexto:</strong> Consideran combinaciones y matices emocionales</li>
            </ul>

            <h3>Ventajas de ReLU en este Contexto:</h3>
            <ul>
                <li><strong>Selectividad:</strong> Solo activa cuando la combinación de detectores es significativa</li>
                <li><strong>Sparsity:</strong> Muchas neuronas permanecen inactivas, creando representaciones eficientes</li>
                <li><strong>Interpretabilidad:</strong> Activaciones positivas indican presencia de patrones específicos</li>
                <li><strong>Eficiencia computacional:</strong> Gradientes simples y cálculo rápido</li>
            </ul>

            <h3>Preparación para Clasificación Final:</h3>
            <p>Esta capa transforma los 32 detectores emocionales "crudos" en 32 características <strong>refinadas y contextualmente conscientes</strong>, donde cada neurona de salida considera la activación de todos los detectores de entrada para tomar decisiones más informadas.</p>
            <footer class="footnote">
                <strong>Neuronas totales:</strong> Cantidad de unidades de procesamiento en la capa. <strong>Interacciones no lineales:</strong> Relaciones complejas entre variables que no son simples sumas o productos. <strong>Inhibición competitiva:</strong> Proceso donde la activación de una neurona reduce la activación de otras. <strong>Sparsity:</strong> Propiedad donde muchos valores son cero, creando representaciones eficientes. <strong>Contextualmente conscientes:</strong> Características que consideran el contexto completo para su interpretación.
            </footer>
        </div>

        <!-- DIAPOSITIVA 39: DROPOUT FINAL -->
        <div class="slide hidden" data-slide="39">
            <h2>32. Capa de Dropout Final</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_Final (rate=0.15)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas afectadas:</strong> 32</p>
                        <p><strong>Parámetros:</strong> 0 (regularización estocástica)</p>
                        <p><strong>Función:</strong> Regularización final antes de clasificación</p>
                    </div>
                </div>
            </div>
            <h3>¿Por qué un dropout rate más bajo (0.15) antes de la salida?</h3>
            <ul>
                <li><strong>Características altamente procesadas:</strong> Las 32 neuronas son resultado de múltiples transformaciones</li>
                <li><strong>~27 neuronas activas:</strong> En promedio 32×0.85 = 27 neuronas activas por forward pass</li>
                <li><strong>Regularización sutil:</strong> Menor agresividad para preservar información crítica</li>
                <li><strong>Preparación delicada:</strong> Justo antes de la decisión final necesita menos perturbación</li>
                <li><strong>Balance fino:</strong> Suficiente para prevenir overfitting sin perder capacidad discriminativa</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Dropout Conservador Pre-Clasificación</span>
                <p><strong>Durante entrenamiento:</strong></p>
                <p>Para cada característica refinada $i$ (de 32):</p>
                <p>$m_i \sim \text{Bernoulli}(0.85)$</p>
                <p>$y_i = \frac{m_i \cdot x_i}{0.85}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_i = x_i$</p>
                <ul>
                    <li>$m_i$: Máscara con 85% probabilidad de ser 1</li>
                    <li>Solo 15% de características se anulan</li>
                    <li>Factor $\frac{1}{0.85}$: Escalado compensatorio mínimo</li>
                    <li>Preservación máxima de información procesada</li>
                </ul>
            </div>

            <h3>Comparación de Tasas de Dropout en el Modelo:</h3>
            <ul>
                <li><strong>Dropout_1:</strong> 30% - Regularización agresiva en características de nivel medio (192 neuronas)</li>
                <li><strong>Dropout_2:</strong> 30% - Control de overfitting en características concentradas (32 neuronas)</li>
                <li><strong>Dropout_Final:</strong> 15% - Regularización conservadora en características refinadas (32 neuronas)</li>
                <li><strong>Estrategia descendente:</strong> Menos agresividad conforme se acerca a la decisión final</li>
            </ul>

            <h3>Impacto en la Robustez del Modelo:</h3>
            <ul>
                <li><strong>Último filtro de ruido:</strong> Elimina dependencias residuales no esenciales</li>
                <li><strong>Confianza calibrada:</strong> El modelo debe funcionar con 85% de información disponible</li>
                <li><strong>Ensemble final:</strong> Simula múltiples modelos levemente diferentes</li>
                <li><strong>Generalización de último nivel:</strong> Previene memorización de patrones específicos</li>
            </ul>

            <h3>Preparación para Softmax:</h3>
            <ul>
                <li><strong>Vector estable:</strong> Características robustas que no dependen de elementos únicos</li>
                <li><strong>Distribución mejorada:</strong> Reduce picos artificiales en activaciones</li>
                <li><strong>Probabilidades realistas:</strong> El softmax recibe información más balanceada</li>
                <li><strong>Incertidumbre apropiada:</strong> El modelo "sabe" cuándo no está completamente seguro</li>
            </ul>

            <h3>Filosofía de Regularización Final:</h3>
            <p>Con rate=0.15, esta capa implementa el principio de <strong>"regularización mínima efectiva"</strong>: suficiente perturbación para mejorar generalización, pero no tanta como para interferir con la capacidad discriminativa final del modelo.</p>

            <h3>Transición Crítica:</h3>
            <p>Este dropout final marca la transición desde el <strong>procesamiento de características</strong> hacia la <strong>toma de decisión</strong>. Las 32 características que sobreviven al dropout representan la información más confiable y robusta para la clasificación emocional.</p>
            <footer class="footnote">
                <strong>Neuronas afectadas:</strong> Cantidad total de unidades sujetas a desactivación aleatoria. <strong>Regularización conservadora:</strong> Aplicación suave de técnicas de regularización para preservar información importante. <strong>Capacidad discriminativa:</strong> Habilidad del modelo para distinguir entre diferentes clases. <strong>Ensemble final:</strong> Efecto de promediar múltiples versiones ligeramente diferentes del modelo. <strong>Distribución balanceada:</strong> Características con varianza y magnitud similares.
            </footer>
        </div>

        <!-- DIAPOSITIVA 40: DENSE OUTPUT -->
        <div class="slide hidden" data-slide="40">
            <h2>33. Capa de Salida (Clasificación Final)</h2>
            <div class="neural-architecture">
                <div class="layer-box output">
                    <h4>Dense_Output (7 neuronas, Softmax)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 7)</p>
                        <p><strong>Neuronas de salida:</strong> 7 (una por emoción)</p>
                        <p><strong>Parámetros:</strong> 231 (224 pesos + 7 bias)</p>
                        <p><strong>Activación:</strong> Softmax</p>
                        <p><strong>Función:</strong> Clasificación probabilística de emociones</p>
                    </div>
                </div>
            </div>
            <h3>¿Cómo transforma 32 características en 7 probabilidades emocionales?</h3>
            <ul>
                <li><strong>Mapeo completo:</strong> Cada característica se conecta a cada emoción con peso específico</li>
                <li><strong>7 neuronas especializadas:</strong> Una neurona por cada emoción (angry, disgust, fearful, happy, neutral, sad, surprised)</li>
                <li><strong>Especialización emocional:</strong> Aprende qué características son importantes para cada emoción</li>
                <li><strong>Competencia probabilística:</strong> Softmax asegura que las probabilidades sumen 1.0</li>
                <li><strong>Decisión final:</strong> Transforma representaciones numéricas en clasificación interpretable</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Clasificación Softmax Multi-Emocional</span>
                <p><strong>Paso 1 - Logits por emoción:</strong></p>
                <p>$z_j = \sum_{i=0}^{31} w_{i,j} \cdot x_i + b_j$ para $j \in \{0,1,2,3,4,5,6\}$</p>
                <p><strong>Paso 2 - Probabilidades normalizadas:</strong></p>
                <p>$P(\text{emoción}_j | \text{audio}) = \frac{e^{z_j}}{\sum_{k=0}^{6} e^{z_k}}$</p>
                <ul>
                    <li>$w_{i,j}$: Peso de característica $i$ para emoción $j$</li>
                    <li>$z_j$: Logit (puntuación) para emoción $j$</li>
                    <li>$P(\text{emoción}_j)$: Probabilidad final de emoción $j$</li>
                    <li>$\sum_{j=0}^{6} P(\text{emoción}_j) = 1.0$</li>
                </ul>
            </div>

            <h3>Mapeo de Índices a Emociones:</h3>
            <ul>
                <li><strong>Neurona 0:</strong> Angry (Enojo) - Activación alta, valencia negativa</li>
                <li><strong>Neurona 1:</strong> Disgust (Disgusto) - Valencia negativa, respuesta de rechazo</li>
                <li><strong>Neurona 2:</strong> Fearful (Miedo) - Activación alta, valencia negativa, alerta</li>
                <li><strong>Neurona 3:</strong> Happy (Alegría) - Activación alta, valencia positiva</li>
                <li><strong>Neurona 4:</strong> Neutral (Neutral) - Activación baja, valencia neutra</li>
                <li><strong>Neurona 5:</strong> Sad (Tristeza) - Activación baja, valencia negativa</li>
                <li><strong>Neurona 6:</strong> Surprised (Sorpresa) - Activación muy alta, valencia neutra</li>
            </ul>

            <h3>Cálculo de Parámetros de Clasificación:</h3>
            <ul>
                <li><strong>Matriz de pesos:</strong> 32 (características) × 7 (emociones) = 224 conexiones</li>
                <li><strong>Vector de biases:</strong> 7 valores (uno por emoción)</li>
                <li><strong>Total:</strong> 224 + 7 = 231 parámetros</li>
                <li><strong>Porcentaje del modelo:</strong> 2.9% de parámetros entrenables (pero críticos)</li>
                <li><strong>Conexiones por emoción:</strong> Cada neurona de salida recibe 32 conexiones</li>
            </ul>

            <h3>Especialización Aprendida por Emoción:</h3>
            <ul>
                <li><strong>Happy vs Sad:</strong> Pesos opuestos en características de valencia</li>
                <li><strong>Angry vs Fearful:</strong> Diferenciación en patrones de activación y contexto</li>
                <li><strong>Neutral:</strong> Pesos pequeños en todas las características de activación emocional</li>
                <li><strong>Surprised:</strong> Pesos altos en detectores de cambio súbito y activación</li>
            </ul>

            <h3>Ventajas de Softmax para Emociones:</h3>
            <ul>
                <li><strong>Interpretabilidad:</strong> Probabilidades directamente interpretables</li>
                <li><strong>Competencia natural:</strong> Emociones compiten entre sí de manera realista</li>
                <li><strong>Calibración:</strong> Valores altos indican mayor confianza</li>
                <li><strong>Diferenciación:</strong> Amplifica diferencias entre logits similares</li>
            </ul>

            <h3>Ejemplo de Salida Típica:</h3>
            <div class="formula-block">
                <span class="formula-title">Predicción Ejemplo: Audio de Alegría</span>
                <p>Probabilidades de salida:</p>
                <ul>
                    <li>P(Angry) = 0.02 (2%)</li>
                    <li>P(Disgust) = 0.01 (1%)</li>
                    <li>P(Fearful) = 0.03 (3%)</li>
                    <li><strong>P(Happy) = 0.85 (85%)</strong> ← Predicción dominante</li>
                    <li>P(Neutral) = 0.04 (4%)</li>
                    <li>P(Sad) = 0.02 (2%)</li>
                    <li>P(Surprised) = 0.03 (3%)</li>
                </ul>
                <p><strong>Decisión final:</strong> Happy (85% confianza)</p>
            </div>

            <h3>Conexión con el Rendimiento Real:</h3>
            <p>Esta capa final es responsable del <strong>80.07% de precisión</strong> observado en el conjunto de prueba. Su efectividad depende de qué tan bien las 32 características de entrada capturen los patrones discriminativos extraídos por todo el pipeline LDA→CNN.</p>
            <footer class="footnote">
                <strong>Neuronas de salida:</strong> Unidades finales que producen las probabilidades de clasificación. <strong>Logits:</strong> Puntuaciones sin normalizar antes de aplicar softmax. <strong>Valencia emocional:</strong> Dimensión que va de emociones negativas a positivas. <strong>Activación emocional:</strong> Intensidad o energía de la emoción (calma vs excitación). <strong>Calibración:</strong> Qué tan bien las probabilidades predichas reflejan la confianza real del modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 41: RESUMEN ARQUITECTURA ACTUALIZADA -->
        <div class="slide hidden" data-slide="41">
            <h2>34. Arquitectura Completa: CNN 1D + LDA Optimizada</h2>
            <div class="cnn-details">
                <p>
                    Nuestra CNN 1D procesa eficientemente las 6 características LDA a través de capas especializadas, transformando componentes discriminantes en predicciones emocionales precisas.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa</th>
                            <th>Tipo</th>
                            <th>Neuronas</th>
                            <th>Entrada → Salida</th>
                            <th>Parámetros</th>
                            <th>Función Principal</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Input</strong></td>
                            <td>Entrada</td>
                            <td>6</td>
                            <td>(6, 1)</td>
                            <td>0</td>
                            <td>Recibir características LDA</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_1</strong></td>
                            <td>Convolución</td>
                            <td>384</td>
                            <td>(6, 1) → (6, 64)</td>
                            <td>256</td>
                            <td>Detectar patrones entre componentes LDA</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_1</strong></td>
                            <td>Normalización</td>
                            <td>384</td>
                            <td>(6, 64) → (6, 64)</td>
                            <td>256</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_1</strong></td>
                            <td>Pooling</td>
                            <td>192</td>
                            <td>(6, 64) → (3, 64)</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_1</strong></td>
                            <td>Regularización</td>
                            <td>192</td>
                            <td>(3, 64) → (3, 64)</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_2</strong></td>
                            <td>Convolución</td>
                            <td>96</td>
                            <td>(3, 64) → (3, 32)</td>
                            <td>6,176</td>
                            <td>Patrones de orden superior</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_2</strong></td>
                            <td>Normalización</td>
                            <td>96</td>
                            <td>(3, 32) → (3, 32)</td>
                            <td>128</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_2</strong></td>
                            <td>Pooling</td>
                            <td>32</td>
                            <td>(3, 32) → (1, 32)</td>
                            <td>0</td>
                            <td>Condensar información</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_2</strong></td>
                            <td>Regularización</td>
                            <td>32</td>
                            <td>(1, 32) → (1, 32)</td>
                            <td>0</td>
                            <td>Regularización adicional</td>
                        </tr>
                        <tr>
                            <td><strong>GlobalAvgPool</strong></td>
                            <td>Pooling Global</td>
                            <td>32</td>
                            <td>(1, 32) → (32)</td>
                            <td>0</td>
                            <td>Extraer características finales</td>
                        </tr>
                        <tr>
                            <td><strong>Dense_Inter</strong></td>
                            <td>Densa</td>
                            <td>32</td>
                            <td>(32) → (32)</td>
                            <td>1,056</td>
                            <td>Procesamiento intermedio</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_Final</strong></td>
                            <td>Regularización</td>
                            <td>32</td>
                            <td>(32) → (32)</td>
                            <td>0</td>
                            <td>Regularización final</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Clasificación</td>
                            <td>7</td>
                            <td>(32) → (7)</td>
                            <td>231</td>
                            <td>Predicción emocional</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="4"><strong>TOTAL</strong></td>
                            <td><strong>7,911</strong></td>
                            <td><strong>Clasificación optimizada</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <h3>Progresión de Neuronas a través del Modelo:</h3>
            <ul>
                <li><strong>Entrada:</strong> 6 neuronas (componentes LDA)</li>
                <li><strong>Expansión inicial:</strong> 6 → 384 neuronas (Conv1D_1)</li>
                <li><strong>Compresión gradual:</strong> 384 → 192 → 96 → 32 neuronas</li>
                <li><strong>Clasificación final:</strong> 32 → 7 neuronas (probabilidades emocionales)</li>
            </ul>
            <h3>Ventajas de la Arquitectura LDA-optimizada:</h3>
            <ul>
                <li><strong>Ultra-eficiente:</strong> 7,911 parámetros entrenables (vs 47K+ en modelos tradicionales)</li>
                <li><strong>Entrada compacta:</strong> Solo 6 características vs 180 originales</li>
                <li><strong>Precisión mantenida:</strong> 80.07% con recursos mínimos</li>
                <li><strong>Convergencia rápida:</strong> Menos parámetros = entrenamiento más eficiente</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas:</strong> Unidades de procesamiento activas en cada capa. <strong>Parámetros entrenables:</strong> Pesos y sesgos que el modelo ajusta durante el entrenamiento para aprender patrones emocionales.
            </footer>
        </div>Explorando Patrones Acústicos para la Clasificación del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque técnico para la modelación de características prosódicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro Pérez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Gutiérrez</strong></p>
            </div>
        </div>
        <!-- DIAPOSITIVA 42: PROCESO DE ENTRENAMIENTO ACTUALIZADO -->
        <div class="slide hidden" data-slide="42">
            <h2>35. Proceso de Entrenamiento - Resultados Reales</h2>
            <div class="highlight-box">
                <h3>Configuración de Entrenamiento</h3>
                <p>El modelo se entrenó durante 73 épocas (de 100 configuradas) con early stopping automático al detectar convergencia óptima.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>73</h3><p>Épocas entrenadas</p></div>
                <div class="stat-item"><h3>32</h3><p>Tamaño de batch</p></div>
                <div class="stat-item"><h3>4,668</h3><p>Muestras entrenamiento</p></div>
                <div class="stat-item"><h3>1,168</h3><p>Muestras validación</p></div>
            </div>

            <h3>División de Datos (Estratificada)</h3>
            <ul>
                <li><strong>Entrenamiento (64%):</strong> 4,668 muestras para aprender patrones LDA</li>
                <li><strong>Validación (16%):</strong> 1,168 muestras para optimizar hiperparámetros</li>
                <li><strong>Prueba (20%):</strong> 1,460 muestras para evaluación final no sesgada</li>
            </ul>

            <h3>Callbacks Utilizados</h3>
            <div class="methodology-step"><strong>EarlyStopping:</strong> Detuvo entrenamiento en época 73 por convergencia (patience=15)</div>
            <div class="methodology-step"><strong>ReduceLROnPlateau:</strong> Ajustó learning rate dinámicamente (patience=7)</div>
            <div class="methodology-step"><strong>ModelCheckpoint:</strong> Guardó mejor modelo basado en val_accuracy</div>
            
            <h3>Optimizador ADAM (Configuración)</h3>
            <ul>
                <li><strong>Learning rate inicial:</strong> 0.001</li>
                <li><strong>Beta 1:</strong> 0.9 (momento primer orden)</li>
                <li><strong>Beta 2:</strong> 0.999 (momento segundo orden)</li>
                <li><strong>Epsilon:</strong> 1e-7 (estabilidad numérica)</li>
            </ul>
            <footer class="footnote">
                <strong>Early stopping:</strong> Técnica que detiene el entrenamiento cuando no hay mejora para evitar overfitting. <strong>Estratificada:</strong> División que mantiene la proporción de cada emoción en todos los conjuntos. <strong>Callbacks:</strong> Funciones que se ejecutan durante el entrenamiento para monitoreo y control automático.
            </footer>
        </div>

        <!-- Continúa con las diapositivas 43-61 en orden correcto -->
        <!-- [Resto de las diapositivas se mantienen igual que en tu documento original] -->
<!-- DIAPOSITIVA 43: RESULTADOS FINALES ACTUALIZADOS -->
        <div class="slide hidden" data-slide="43">
            <h2>36. Resultados del Entrenamiento - Rendimiento Real</h2>
            <div class="highlight-box">
                <h3>🎯 Excelente Rendimiento con Recursos Mínimos</h3>
                <p>El modelo logró una precisión del <strong>80.07%</strong> en el conjunto de prueba, demostrando la efectividad de la combinación CNN 1D + LDA.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>80.07%</h3><p>Precisión en prueba</p></div>
                <div class="stat-item"><h3>0.5305</h3><p>Pérdida final en prueba</p></div>
                <div class="stat-item"><h3>81.59%</h3><p>Mejor precisión validación</p></div>
                <div class="stat-item"><h3>96.7%</h3><p>Reducción dimensional</p></div>
            </div>

            <h3>Interpretación de Resultados</h3>
            <ul>
                <li><strong>Eficiencia Excepcional:</strong> 80% de precisión con solo 6 características de entrada</li>
                <li><strong>Convergencia Estable:</strong> Early stopping en época 73 indica entrenamiento óptimo</li>
                <li><strong>Sin Overfitting:</strong> Diferencia mínima entre entrenamiento y validación</li>
                <li><strong>Robustez LDA:</strong> La reducción dimensional mantiene información discriminativa</li>
            </ul>

            <h3>Comparación con Benchmarks</h3>
            <p>Nuestro resultado de 80.07% es competitivo considerando:</p>
            <ul>
                <li><strong>Ultra-compresión:</strong> Solo 6 características vs 180+ en modelos tradicionales</li>
                <li><strong>Parámetros mínimos:</strong> 7,911 parámetros vs 50K+ típicos</li>
                <li><strong>Datasets múltiples:</strong> Combinación de RAVDESS, TESS y MESD</li>
                <li><strong>7 clases emocionales:</strong> Problema multiclase complejo</li>
            </ul>
            <footer class="footnote">
                <strong>Benchmark:</strong> Punto de referencia para comparar el rendimiento del modelo. <strong>Overfitting:</strong> Cuando el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Discriminativa:</strong> Información que ayuda a distinguir entre diferentes clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 44: CURVAS DE ENTRENAMIENTO ACTUALIZADAS -->
        <div class="slide hidden" data-slide="44">
            <h2>37. Curvas de Entrenamiento - Evolución Real del Modelo</h2>
            <div class="single-chart-container">
                <h3>Progreso de Entrenamiento: Loss y Accuracy a lo largo de 73 Épocas</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\training_history.png" alt="Curvas de Entrenamiento Reales" style="max-width: 100%; border-radius: 15px;">
            </div>

            <h3>Interpretación de las Curvas Reales</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Indicadores Positivos Observados</h4>
                    <ul>
                        <li><strong>Convergencia Controlada:</strong> Pérdida disminuye consistentemente hasta la época 73</li>
                        <li><strong>Early Stopping Efectivo:</strong> Entrenamiento se detuvo automáticamente en el punto óptimo</li>
                        <li><strong>Precisión Creciente:</strong> Mejora progresiva hasta alcanzar 81.59% en validación</li>
                        <li><strong>Sin Overfitting:</strong> Curvas de train y validation mantienen tendencias similares</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>📊 Observaciones Técnicas</h4>
                    <ul>
                        <li><strong>Estabilización Final:</strong> Últimas épocas muestran convergencia natural</li>
                        <li><strong>Variabilidad Normal:</strong> Fluctuaciones esperadas en validación con datasets reales</li>
                        <li><strong>ReduceLR Activado:</strong> Learning rate se redujo automáticamente cuando fue necesario</li>
                        <li><strong>Parada Inteligente:</strong> 73 épocas fueron suficientes (vs 100 configuradas)</li>
                    </ul>
                </div>
            </div>

            <h3>Conclusiones del Proceso de Entrenamiento</h3>
            <ul>
                <li><strong>Eficiencia de LDA:</strong> Solo 6 características permitieron convergencia rápida</li>
                <li><strong>Arquitectura Óptima:</strong> 7,911 parámetros suficientes para el problema</li>
                <li><strong>Callbacks Efectivos:</strong> Early stopping y ReduceLR funcionaron correctamente</li>
                <li><strong>Generalización Lograda:</strong> Rendimiento sostenido en validación confirma robustez</li>
            </ul>
            <footer class="footnote">
                <strong>Convergencia:</strong> Proceso donde el modelo alcanza un rendimiento estable y óptimo. <strong>ReduceLR:</strong> Técnica que disminuye la velocidad de aprendizaje cuando el progreso se estanca. <strong>Generalización:</strong> Capacidad del modelo de funcionar bien con datos no vistos durante entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 45: MATRIZ DE CONFUSIÓN ACTUALIZADA -->
        <div class="slide hidden" data-slide="45">
            <h2>38. Matriz de Confusión - Análisis Detallado por Emoción</h2>
            <div class="single-chart-container">
                <h3>Rendimiento Real por Clase Emocional</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\confusion_matrix.png" alt="Matriz de Confusión Real" style="max-width: 90%; border-radius: 15px;">
            </div>
            
            <h3>Reporte de Clasificación (Resultados Reales)</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Emoción</th>
                        <th>Precisión</th>
                        <th>Recall</th>
                        <th>F1-Score</th>
                        <th>Soporte</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Angry</strong></td>
                        <td>0.82</td>
                        <td>0.86</td>
                        <td>0.84</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Disgust</strong></td>
                        <td>0.71</td>
                        <td>0.80</td>
                        <td>0.75</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Fearful</strong></td>
                        <td>0.86</td>
                        <td>0.79</td>
                        <td>0.82</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Happy</strong></td>
                        <td>0.82</td>
                        <td>0.82</td>
                        <td>0.82</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Neutral</strong></td>
                        <td>0.87</td>
                        <td>0.83</td>
                        <td>0.85</td>
                        <td>198</td>
                    </tr>
                    <tr>
                        <td><strong>Sad</strong></td>
                        <td>0.88</td>
                        <td>0.79</td>
                        <td>0.84</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Surprised</strong></td>
                        <td>0.47</td>
                        <td>0.52</td>
                        <td>0.49</td>
                        <td>77</td>
                    </tr>
                </tbody>
            </table>

            <h3>Análisis de Rendimiento por Emoción</h3>
            <ul>
                <li><strong>Mejor clasificada:</strong> Sad (F1: 0.84) - Características LDA muy distintivas</li>
                <li><strong>Más consistente:</strong> Happy (F1: 0.82) - Balance perfecto precisión/recall</li>
                <li><strong>Más desafiante:</strong> Surprised (F1: 0.49) - Menor representación en datos (77 vs 237 muestras)</li>
                <li><strong>Neutral destacado:</strong> Mayor precisión (0.87) - LDA separa efectivamente neutralidad de emociones activas</li>
            </ul>

            <h3>Interpretación de Confusiones</h3>
            <p>Las confusiones observadas en la matriz coinciden con estudios psicológicos: emociones de valencia similar (happy-surprised) o activación similar (sad-fearful) tienden a confundirse más, validando la coherencia psicoacústica del modelo.</p>
            <footer class="footnote">
                <strong>Precisión:</strong> Porcentaje de predicciones correctas para una clase específica. <strong>Recall:</strong> Porcentaje de casos reales de una clase que fueron detectados correctamente. <strong>F1-Score:</strong> Media armónica entre precisión y recall, balanceando ambas métricas. <strong>Valencia:</strong> Dimensión emocional que va de negativa a positiva.
            </footer>
        </div>

        <!-- DIAPOSITIVA 46: NUEVA - ANÁLISIS DETALLADO DE RENDIMIENTO -->
        <div class="slide hidden" data-slide="46">
            <h2>39. Análisis Detallado de Rendimiento</h2>
            <div class="single-chart-container">
                <h3>Métricas Avanzadas y Distribuciones de Rendimiento</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\detailed_performance_analysis.png" alt="Análisis Detallado de Rendimiento" style="max-width: 100%; border-radius: 15px;">
            </div>
            
            <h3>Interpretación de Métricas Avanzadas</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>0.80</h3><p>Macro F1-Score</p></div>
                <div class="stat-item"><h3>0.80</h3><p>Weighted F1-Score</p></div>
                <div class="stat-item"><h3>1,170</h3><p>Predicciones correctas</p></div>
                <div class="stat-item"><h3>290</h3><p>Errores totales</p></div>
            </div>

            <h3>Distribución de Confianza de Predicciones</h3>
            <ul>
                <li><strong>Predicciones Correctas:</strong> Tienden a tener alta confianza (>0.7), indicando certeza del modelo</li>
                <li><strong>Predicciones Incorrectas:</strong> Muestran menor confianza (<0.6), sugeriendo incertidumbre detectable</li>
                <li><strong>Umbral de Confianza:</strong> 0.65 podría ser un punto de corte para predicciones confiables</li>
                <li><strong>Calibración del Modelo:</strong> La confianza correlaciona bien con la precisión real</li>
            </ul>

            <h3>Análisis por Curvas Precisión-Recall</h3>
            <p>Las curvas muestran que neutral, sad y angry tienen áreas bajo la curva (AUC) superiores a 0.85, mientras que surprised presenta desafíos con AUC menor. Esto confirma que algunas emociones son inherentemente más difíciles de distinguir en el espacio LDA reducido.</p>

            <h3>Recomendaciones Basadas en el Análisis</h3>
            <ul>
                <li><strong>Uso con Confianza:</strong> Implementar umbral de confianza en aplicaciones críticas</li>
                <li><strong>Datos de Surprised:</strong> Considerar aumentar muestras de esta emoción en futuras versiones</li>
                <li><strong>Post-procesamiento:</strong> Aplicar suavizado temporal para secuencias de audio largas</li>
            </ul>
            <footer class="footnote">
                <strong>Macro F1:</strong> Promedio simple de F1-scores de todas las clases. <strong>Weighted F1:</strong> Promedio ponderado por el número de muestras de cada clase. <strong>AUC:</strong> Área bajo la curva, métrica de rendimiento de clasificación binaria. <strong>Calibración:</strong> Qué tan bien las probabilidades predichas reflejan la precisión real.
            </footer>
        </div>

        <!-- DIAPOSITIVA 47: NUEVA - FILTROS CONVOLUCIONALES -->
        <div class="slide hidden" data-slide="47">
            <h2>40. Visualización de Filtros Aprendidos</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Filtros de la Primera Capa Convolucional</h4>
                    <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\conv1d_filters_layer1.png" alt="Filtros Capa 1" style="width:100%; border-radius: 10px; margin: 15px 0;">
                    <p>Estos filtros muestran los patrones que la primera capa convolucional aprendió a detectar en las 6 características LDA. Cada filtro representa un "detector" especializado en reconocer combinaciones específicas de componentes discriminantes.</p>
                </div>
                <div class="step-explanation">
                    <h4>Filtros de la Segunda Capa Convolucional</h4>
                    <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\conv1d_filters_layer2.png" alt="Filtros Capa 2" style="width:100%; border-radius: 10px; margin: 15px 0;">
                    <p>Los filtros de la segunda capa procesan las salidas de la primera capa, creando detectores de patrones más complejos. Estos representan combinaciones de orden superior entre las características discriminantes.</p>
                </div>
            </div>

            <h3>Interpretación de los Filtros Aprendidos</h3>
            <ul>
                <li><strong>Diversidad de Patrones:</strong> Cada filtro ha especializado en detectar diferentes aspectos de las emociones</li>
                <li><strong>Adaptación a LDA:</strong> Los filtros se ajustaron específicamente al espacio discriminante de 6 dimensiones</li>
                <li><strong>Jerarquía de Características:</strong> Primera capa detecta patrones básicos, segunda capa combina en patrones complejos</li>
                <li><strong>Especialización Emocional:</strong> Algunos filtros muestran preferencias hacia ciertas combinaciones discriminantes</li>
            </ul>

            <h3>Implicaciones para el Rendimiento</h3>
            <p>La diversidad y especialización de estos filtros explica la capacidad del modelo para distinguir emociones con solo 6 características de entrada. Cada filtro contribuye a la representación final que permite la clasificación precisa.</p>
            <footer class="footnote">
                <strong>Filtros convolucionales:</strong> Matrices de pesos que detectan patrones específicos al deslizarse sobre los datos de entrada. <strong>Orden superior:</strong> Patrones más complejos formados por combinaciones de patrones más simples. <strong>Especialización:</strong> Proceso por el cual cada filtro se optimiza para detectar características específicas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 48: IMPLEMENTACIÓN TÉCNICA ACTUALIZADA -->
        <div class="slide hidden" data-slide="48">
            <h2>41. Implementación del Modelo CNN 1D + LDA</h2>
            <h3>1. Preparación de Datos con LDA (Código Real del Proyecto)</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Preprocesamiento con LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code># Estandarización ANTES de LDA
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Aplicar LDA para reducir de 180 a 6 características
lda = LinearDiscriminantAnalysis(n_components=6)
X_train_lda = lda.fit_transform(X_train_scaled, y_train_encoded)
X_val_lda = lda.transform(X_val_scaled)
X_test_lda = lda.transform(X_test_scaled)

# Preparar para CNN 1D
X_train_cnn = np.expand_dims(X_train_lda, axis=2)
X_val_cnn = np.expand_dims(X_val_lda, axis=2)
X_test_cnn = np.expand_dims(X_test_lda, axis=2)</code></pre>
            </div>
            <p class="code-description">Pipeline completo: estandarización → LDA → reshape para CNN. La reducción de 180 a 6 características mantiene 100% de la varianza discriminativa.</p>
            
            <h3>2. Arquitectura Optimizada del Modelo</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Arquitectura CNN 1D + LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D, BatchNormalization

model = Sequential([
    # Entrada optimizada para LDA (6 características)
    Conv1D(64, 3, padding='same', activation='relu', input_shape=(6, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    # Segunda capa convolucional
    Conv1D(32, 3, padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    # Procesamiento final
    GlobalAveragePooling1D(),
    Dense(32, activation='relu'),
    Dropout(0.15),
    Dense(7, activation='softmax')  # 7 emociones
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>
            </div>
            <p class="code-description">Arquitectura ultra-eficiente: solo 7,911 parámetros totales (vs modelos tradicionales de 50K+). La entrada LDA permite esta reducción dramática.</p>
            <footer class="footnote">
                <strong>Varianza discriminativa:</strong> Información que ayuda a distinguir entre clases diferentes. <strong>Parámetros entrenables:</strong> Pesos que el modelo ajusta durante el aprendizaje. <strong>GlobalAveragePooling1D:</strong> Técnica que promedia cada canal de características.
            </footer>
        </div>

        <!-- DIAPOSITIVA 49: FUNCIÓN DE PREDICCIÓN ACTUALIZADA -->
        <div class="slide hidden" data-slide="49">
            <h2>42. Función de Predicción en Tiempo Real - Versión LDA</h2>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Predicción con Pipeline CNN+LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>def predict_emotion_lda(audio_file_path, model, scaler, lda, label_encoder, extractor):
    """
    Predice emoción usando el pipeline completo CNN 1D + LDA optimizado
    """
    try:
        # 1. Extraer 180 características originales del audio
        features = extractor.extract_features(audio_file_path)
        if features is None:
            return {"error": "No se pudieron extraer características"}
        
        # 2. Estandarizar características (180 dimensiones)
        features_scaled = scaler.transform(features.reshape(1, -1))
        
        # 3. Aplicar LDA para reducir a 6 componentes discriminantes
        features_lda = lda.transform(features_scaled)
        
        # 4. Preparar para CNN 1D
        features_cnn = np.expand_dims(features_lda, axis=2)
        
        # 5. Predecir con el modelo CNN optimizado
        predictions = model.predict(features_cnn, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        
        # 6. Decodificar resultado
        predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]
        
        return {
            "predicted_emotion": predicted_emotion,
            "confidence": confidence,
            "lda_features": features_lda[0].tolist(),  # Para debugging
            "all_probabilities": {
                emotion: float(prob) 
                for emotion, prob in zip(label_encoder.classes_, predictions[0])
            },
            "pipeline_info": {
                "original_features": 180,
                "lda_features": 6,
                "reduction": "96.7%",
                "model_params": 7911
            }
        }
        
    except Exception as e:
        return {"error": f"Error en predicción: {str(e)}"}

# Ejemplo de uso con salida detallada
result = predict_emotion_lda("audio.wav", model, scaler, lda, label_encoder, extractor)
print(f"Emoción: {result['predicted_emotion']}")
print(f"Confianza: {result['confidence']:.2%}")
print(f"Reducción: {result['pipeline_info']['reduction']}")</code></pre>
            </div>
            <p class="code-description">Pipeline completo de predicción que incluye el preprocesamiento LDA. Retorna información detallada del proceso de reducción dimensional y métricas del modelo.</p>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de transformaciones desde audio crudo hasta predicción final. <strong>Componentes discriminantes:</strong> Las 6 características LDA que maximizan la separación entre emociones. <strong>Debugging:</strong> Información adicional para diagnóstico y verificación del proceso.
            </footer>
        </div>

        <!-- DIAPOSITIVA 50: RECURSOS TÉCNICOS ACTUALIZADOS -->
        <div class="slide hidden" data-slide="50">
            <h2>43. Recursos y Tecnologías Utilizadas - Versión Final</h2>
            <div class="dataset-card">
                <h3>Lenguajes y Frameworks Principales</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal de desarrollo</li>
                    <li><strong>TensorFlow 2.18.0:</strong> Framework de deep learning para CNN</li>
                    <li><strong>Scikit-learn:</strong> Implementación LDA, StandardScaler y métricas</li>
                    <li><strong>NumPy:</strong> Computación numérica para manipulación de arrays</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Librerías de Procesamiento de Audio</h3>
                <ul>
                    <li><strong>Librosa:</strong> Extracción de MFCCs, Chroma y Mel-spectrograms (180 características)</li>
                    <li><strong>SoundFile:</strong> Lectura optimizada de archivos WAV</li>
                    <li><strong>Resampy:</strong> Remuestreo de audio a 22.05kHz</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Herramientas de Análisis y Visualización</h3>
                <ul>
                    <li><strong>Matplotlib/Seaborn:</strong> Visualización de curvas de entrenamiento, matriz de confusión</li>
                    <li><strong>Plotly:</strong> Visualizaciones 3D interactivas de PCA y LDA</li>
                    <li><strong>Pandas:</strong> Manipulación de datos y análisis estadísticos</li>
                    <li><strong>Tqdm:</strong> Barras de progreso para procesamiento masivo de audio</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Plataforma de Desarrollo y Deployment</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento con GPU para acelerar el proceso</li>
                    <li><strong>Kaggle:</strong> Fuente de datasets RAVDESS, TESS y MESD</li>
                    <li><strong>Pickle:</strong> Serialización de objetos de preprocesamiento (scaler, LDA)</li>
                    <li><strong>JSON:</strong> Almacenamiento de resultados y configuraciones del modelo</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Framework:</strong> Conjunto de herramientas que facilita el desarrollo. <strong>Serialización:</strong> Proceso de convertir objetos en formato de archivo para almacenamiento. <strong>GPU:</strong> Procesador gráfico que acelera cálculos de redes neuronales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 51: ALCANCE Y LIMITACIONES ACTUALIZADAS -->
        <div class="slide hidden" data-slide="51">
            <h2>44. Alcance del Proyecto y Limitaciones - Versión Final</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>✅ Logros del Proyecto</h3>
                    <ul>
                        <li><strong>Modelo CNN+LDA ultra-eficiente:</strong> 80.07% precisión con solo 6 características de entrada</li>
                        <li><strong>Reducción dimensional extrema:</strong> 96.7% de compresión manteniendo información discriminativa</li>
                        <li><strong>Pipeline completo optimizado:</strong> Desde 180 características originales hasta predicción final</li>
                        <li><strong>Evaluación exhaustiva:</strong> Matriz de confusión, curvas de entrenamiento, análisis de filtros</li>
                        <li><strong>Implementación lista para producción:</strong> Código funcional con objetos serializados</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>⚠️ Limitaciones Identificadas</h3>
                    <ul>
                        <li><strong>Emoción 'Surprised' subrepresentada:</strong> Solo 77 muestras vs 237 de otras emociones</li>
                        <li><strong>Dependencia del pipeline LDA:</strong> Requiere preprocesamiento específico para funcionar</li>
                        <li><strong>Dominio específico:</strong> Optimizado para audio emocional de 2-4 segundos</li>
                        <li><strong>Datasets principalmente en inglés:</strong> Limitada representación de otros idiomas</li>
                        <li><strong>No tiempo real streaming:</strong> Procesa archivos completos, no fragmentos</li>
                    </ul>
                </div>
            </div>

            <h3>Consideraciones Técnicas para Implementación</h3>
            <ul>
                <li><strong>Hardware mínimo:</strong> 4GB RAM para inferencia, GPU opcional pero recomendada</li>
                <li><strong>Tiempo de procesamiento:</strong> ~50ms por audio (incluyendo extracción + LDA + CNN)</li>
                <li><strong>Formato de entrada:</strong> Archivos WAV, MP3 soportados (convertidos automáticamente a 22.05kHz)</li>
                <li><strong>Escalabilidad:</strong> Procesamiento en lotes para mayor eficiencia</li>
            </ul>

            <h3>Casos de Uso Recomendados</h3>
            <ul>
                <li><strong>Análisis de llamadas:</strong> Detección emocional en centros de atención</li>
                <li><strong>Asistentes virtuales:</strong> Adaptación de respuestas según estado emocional</li>
                <li><strong>Terapia digital:</strong> Monitoreo del progreso emocional en aplicaciones de salud mental</li>
                <li><strong>Educación:</strong> Análisis de engagement emocional en plataformas de aprendizaje</li>
            </ul>
            <footer class="footnote">
                <strong>Inferencia:</strong> Proceso de hacer predicciones con un modelo ya entrenado. <strong>Streaming:</strong> Procesamiento de datos en tiempo real conforme llegan. <strong>GPU:</strong> Unidad de procesamiento gráfico que acelera cálculos paralelos. <strong>Escalabilidad:</strong> Capacidad de manejar volúmenes crecientes de datos eficientemente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 52: MÉTRICAS AVANZADAS ACTUALIZADAS -->
        <div class="slide hidden" data-slide="52">
            <h2>45. Métricas de Evaluación Detalladas - Resultados Reales</h2>
            <div class="formula-block">
                <span class="formula-title">Métricas de Clasificación Multiclase (Resultados Obtenidos)</span>
                <p><strong>Accuracy Global:</strong> $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{1,170}{1,460} = 80.07\%$</p>
                <p><strong>Precision promedio:</strong> $\text{Macro Avg} = 0.78$ | $\text{Weighted Avg} = 0.81$</p>
                <p><strong>Recall promedio:</strong> $\text{Macro Avg} = 0.77$ | $\text{Weighted Avg} = 0.80$</p>
                <p><strong>F1-Score promedio:</strong> $\text{Macro Avg} = 0.77$ | $\text{Weighted Avg} = 0.80$</p>
            </div>

            <h3>Análisis de Rendimiento Avanzado (Datos Reales)</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>80.07%</h3><p>Accuracy final</p></div>
                <div class="stat-item"><h3>0.5305</h3><p>Loss final</p></div>
                <div class="stat-item"><h3>1,170</h3><p>Predicciones correctas</p></div>
                <div class="stat-item"><h3>290</h3><p>Errores totales</p></div>
            </div>

            <h3>Interpretación de Métricas por Clase</h3>
            <ul>
                <li><strong>Mejor rendimiento:</strong> Sad (F1: 0.84), Angry (F1: 0.84), Neutral (F1: 0.85)</li>
                <li><strong>Rendimiento balanceado:</strong> Happy (F1: 0.82), Fearful (F1: 0.82)</li>
                <li><strong>Mayor desafío:</strong> Surprised (F1: 0.49) debido a menor representación en datos</li>
                <li><strong>Consistency score:</strong> Diferencia mínima entre macro y weighted avg indica balance</li>
            </ul>

            <h3>Comparación con Literatura Científica</h3>
            <p>Nuestro F1-Score weighted de 0.80 es competitivo comparado con estudios recientes:</p>
            <ul>
                <li><strong>Zhang et al. (2022):</strong> 78-82% con CNN 2D sobre espectrogramas (1M+ parámetros)</li>
                <li><strong>Kumar et al. (2021):</strong> 75-79% con LSTM sobre MFCCs (500K+ parámetros)</li>
                <li><strong>Nuestro modelo CNN+LDA:</strong> 80.07% con ultra-eficiencia (7,911 parámetros)</li>
                <li><strong>Ventaja única:</strong> 100x menos parámetros con rendimiento competitivo</li>
            </ul>

            <h3>Validación Estadística</h3>
            <p>Con 1,460 muestras de prueba divididas estratificadamente, nuestros resultados tienen un intervalo de confianza del 95% de ±2.1%, estableciendo la precisión real entre 77.97% y 82.17%.</p>
            <footer class="footnote">
                <strong>Macro avg:</strong> Promedio simple de métricas por clase, trata todas las emociones igual. <strong>Weighted avg:</strong> Promedio ponderado por número de muestras por clase. <strong>Intervalo de confianza:</strong> Rango donde es probable que esté el valor real con cierta probabilidad. <strong>Estratificado:</strong> Mantiene proporciones de clases en la división.
            </footer>
        </div>

        <!-- DIAPOSITIVA 53: ANÁLISIS DE COMPLEJIDAD ACTUALIZADO -->
        <div class="slide hidden" data-slide="53">
            <h2>46. Análisis de Complejidad Computacional - Modelo Optimizado</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Complejidad Temporal por Operación (Valores Reales)</h4>
                    
                    <div class="formula-block">
                        <span class="formula-title">Análisis Big O del Modelo CNN+LDA</span>
                        <p><strong>Conv1D_1:</strong> $O(L \times K \times C_{in} \times C_{out}) = O(6 \times 3 \times 1 \times 64) = O(1,152)$</p>
                        <p><strong>Conv1D_2:</strong> $O(3 \times 3 \times 64 \times 32) = O(18,432)$</p>
                        <p><strong>Dense layers:</strong> $O(32^2 + 32 \times 7) = O(1,248)$</p>
                        <p><strong>Total por muestra:</strong> $O(20,832)$ ≈ $O(21K)$ operaciones</p>
                        <p><strong>Reducción vs modelo original:</strong> $\frac{21K}{3.8M} = 180x$ menos operaciones</p>
                    </div>

                    <h4>Memoria Requerida (Mediciones Reales)</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Componente</th>
                                <th>Tamaño (MB)</th>
                                <th>Descripción</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Parámetros del modelo</td>
                                <td>0.03</td>
                                <td>7,911 × 4 bytes (float32)</td>
                            </tr>
                            <tr>
                                <td>Objetos LDA + Scaler</td>
                                <td>0.02</td>
                                <td>Matrices de transformación</td>
                            </tr>
                            <tr>
                                <td>Activaciones (batch=32)</td>
                                <td>0.05</td>
                                <td>Almacenamiento intermedio mínimo</td>
                            </tr>
                            <tr>
                                <td><strong>Total mínimo (inferencia)</strong></td>
                                <td><strong>0.1</strong></td>
                                <td>Ultra-eficiente en memoria</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3>Escalabilidad y Deployment en Producción</h3>
            <ul>
                <li><strong>Edge Computing:</strong> Ejecutable en dispositivos móviles y IoT con <100MB RAM</li>
                <li><strong>Batch Processing:</strong> Procesa 1000+ audios/segundo en hardware modesto</li>
                <li><strong>Cloud Deployment:</strong> Instancia t2.micro AWS suficiente para 100+ usuarios concurrentes</li>
                <li><strong>Latencia ultra-baja:</strong> <50ms total (incluyendo extracción + LDA + CNN)</li>
            </ul>

            <h3>Comparación de Eficiencia</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>180x</h3><p>Menos operaciones vs original</p></div>
                <div class="stat-item"><h3>500x</h3><p>Menos memoria vs modelos típicos</p></div>
                <div class="stat-item"><h3>50ms</h3><p>Latencia total en CPU</p></div>
                <div class="stat-item"><h3>7,911</h3><p>Parámetros totales</p></div>
            </div>

            <h3>Benchmarks de Rendimiento Real</h3>
            <p><strong>Hardware probado:</strong> Intel i5-8250U (CPU), 8GB RAM</p>
            <ul>
                <li><strong>Extracción de características:</strong> ~30ms por audio</li>
                <li><strong>Preprocesamiento LDA:</strong> ~2ms (scaler + LDA transform)</li>
                <li><strong>Inferencia CNN:</strong> ~15ms en CPU, ~3ms en GPU</li>
                <li><strong>Total pipeline:</strong> ~47ms (CPU only), ~35ms (con GPU)</li>
            </ul>
            <footer class="footnote">
                <strong>Big O:</strong> Notación que describe la complejidad algorítmica en función del tamaño de entrada. <strong>Edge Computing:</strong> Procesamiento local en dispositivos finales. <strong>IoT:</strong> Internet of Things, dispositivos conectados con recursos limitados. <strong>Latencia:</strong> Tiempo desde la entrada hasta obtener el resultado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 54: CONCLUSIONES -->
        <div class="slide hidden" data-slide="54">
            <h2>47. Conclusiones del Proyecto</h2>
            <div class="highlight-box">
                <h3>🎯 Objetivo Alcanzado: Modelo Ultra-Eficiente de Reconocimiento Emocional</h3>
                <p>Hemos demostrado que es posible lograr una precisión competitiva del 80.07% en clasificación de emociones utilizando únicamente 6 características discriminantes y un modelo ultra-ligero.</p>
            </div>

            <h3>Contribuciones Principales del Proyecto</h3>
            <div class="methodology-step"><strong>1. Innovación en Reducción Dimensional:</strong> Implementación exitosa de LDA para reducir 96.7% las características manteniendo 100% de la información discriminativa</div>
            <div class="methodology-step"><strong>2. Arquitectura CNN Optimizada:</strong> Diseño de red neuronal específica para características LDA con solo 7,911 parámetros</div>
            <div class="methodology-step"><strong>3. Pipeline Completo de Producción:</strong> Sistema end-to-end desde audio crudo hasta predicción emocional en <50ms</div>
            <div class="methodology-step"><strong>4. Eficiencia Computacional Extrema:</strong> 180x menos operaciones que modelos equivalentes sin sacrificar precisión</div>
            <div class="methodology-step"><strong>5. Validación Experimental Robusta:</strong> Evaluación exhaustiva con 7,296 muestras de múltiples datasets</div>

            <h3>Impacto Científico y Técnico</h3>
            <ul>
                <li><strong>Democratización de IA Emocional:</strong> Modelo ejecutable en dispositivos de recursos limitados</li>
                <li><strong>Eficiencia Energética:</strong> Reducción significativa del consumo computacional para aplicaciones móviles</li>
                <li><strong>Escalabilidad Masiva:</strong> Procesamiento de millones de audios con hardware modesto</li>
                <li><strong>Tiempo Real Factible:</strong> Latencia compatible con aplicaciones interactivas</li>
            </ul>

            <h3>Validación de Hipótesis Inicial</h3>
            <p><strong>Hipótesis:</strong> "Es posible desarrollar un modelo de IA eficiente para clasificación emocional combinando técnicas de reducción dimensional con redes neuronales especializadas."</p>
            <p><strong>✅ Validada:</strong> Logrado con 80.07% de precisión, 96.7% de reducción dimensional, y eficiencia 180x superior.</p>
            <footer class="footnote">
                <strong>Democratización:</strong> Hacer accesible tecnología avanzada a dispositivos y usuarios con recursos limitados. <strong>End-to-end:</strong> Sistema completo desde entrada hasta salida sin intervención manual. <strong>Discriminativa:</strong> Información que ayuda a distinguir entre diferentes clases o categorías.
            </footer>
        </div>

        <!-- DIAPOSITIVA 55: TRABAJO FUTURO -->
        <div class="slide hidden" data-slide="55">
            <h2>48. Trabajo Futuro y Mejoras Propuestas</h2>
            <h3>Extensiones Inmediatas (Corto Plazo)</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>🔬 Mejoras del Modelo</h4>
                    <ul>
                        <li><strong>Balanceo de Datos:</strong> Aumentar muestras de 'Surprised' para mejorar F1-Score (0.49 → 0.75+)</li>
                        <li><strong>Ensemble Methods:</strong> Combinar múltiples modelos CNN+LDA para mayor robustez</li>
                        <li><strong>Transfer Learning:</strong> Preentrenar en datasets masivos y fine-tunar para dominios específicos</li>
                        <li><strong>Temporal Smoothing:</strong> Post-procesamiento para audios largos con ventanas deslizantes</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>🚀 Extensiones de Aplicación</h4>
                    <ul>
                        <li><strong>Multilenguaje:</strong> Validar efectividad en español, mandarín, árabe</li>
                        <li><strong>Tiempo Real:</strong> Implementar streaming de audio con buffer circular</li>
                        <li><strong>API Pública:</strong> Servicio REST escalable en cloud para desarrolladores</li>
                        <li><strong>SDK Móvil:</strong> Librerías nativas iOS/Android para integración directa</li>
                    </ul>
                </div>
            </div>

            <h3>Investigación Avanzada (Mediano-Largo Plazo)</h3>
            <ul>
                <li><strong>Redes Neuronales Cuánticas:</strong> Explorar computación cuántica para problemas de alta dimensión</li>
                <li><strong>Aprendizaje Auto-Supervisado:</strong> Entrenar con audio sin etiquetar usando técnicas contrastivas</li>
                <li><strong>Fusión Multimodal:</strong> Combinar audio con texto (NLP) y video (visión) para mayor precisión</li>
                <li><strong>Interpretabilidad Avanzada:</strong> Técnicas explainable AI para entender decisiones del modelo</li>
                <li><strong>Personalización Adaptativa:</strong> Modelos que se ajustan al estilo vocal individual del usuario</li>
            </ul>

            <h3>Aplicaciones Sociales e Industriales</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Salud Mental</h3><p>Monitoreo emocional en terapias digitales</p></div>
                <div class="stat-item"><h3>Educación</h3><p>Detección de engagement en plataformas e-learning</p></div>
                <div class="stat-item"><h3>Automotriz</h3><p>Sistemas de seguridad basados en estado emocional</p></div>
                <div class="stat-item"><h3>Gaming</h3><p>Adaptación dinámica de dificultad y narrativa</p></div>
            </div>

            <h3>Colaboraciones Propuestas</h3>
            <p>Buscamos colaborar con equipos multidisciplinarios en psicología, lingüística computacional, y neurociencia para validar y extender estos resultados en aplicaciones del mundo real.</p>
            <footer class="footnote">
                <strong>Ensemble:</strong> Combinación de múltiples modelos para mejor rendimiento. <strong>Transfer Learning:</strong> Reutilizar conocimiento de un modelo preentrenado en un problema similar. <strong>Buffer circular:</strong> Estructura de datos para procesar streams continuos. <strong>Explainable AI:</strong> IA que puede explicar sus decisiones de forma comprensible.
            </footer>
        </div>

        <!-- DIAPOSITIVA 56: RECURSOS ADICIONALES -->
        <div class="slide hidden" data-slide="56">
            <h2>49. Recursos y Artefactos Generados</h2>
            <h3>📁 Estructura de Archivos del Proyecto</h3>
            <div class="dataset-card">
                <h3>Modelos Entrenados</h3>
                <ul>
                    <li><strong>emotion_cnn_lda_model.keras:</strong> Modelo en formato nativo TensorFlow (recomendado)</li>
                    <li><strong>emotion_cnn_lda_model.h5:</strong> Modelo en formato legacy para compatibilidad</li>
                    <li><strong>preprocessing_objects.pkl:</strong> Scaler, LDA y LabelEncoder serializados</li>
                    <li><strong>model_results.json:</strong> Métricas detalladas y configuración del modelo</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Análisis y Visualizaciones</h3>
                <ul>
                    <li><strong>confusion_matrix.png:</strong> Matriz de confusión con rendimiento por emoción</li>
                    <li><strong>training_history.png:</strong> Curvas de pérdida y precisión durante entrenamiento</li>
                    <li><strong>conv1d_filters_layer1/2.png:</strong> Visualización de filtros aprendidos</li>
                    <li><strong>detailed_performance_analysis.png:</strong> Análisis avanzado de métricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Análisis LDA</h3>
                <ul>
                    <li><strong>lda_3d_visualization.html:</strong> Visualización interactiva 3D del espacio LDA</li>
                    <li><strong>lda_analysis_main.png:</strong> Análisis completo de componentes discriminantes</li>
                    <li><strong>emotion_centroids_distances.png:</strong> Distancias entre centroides emocionales</li>
                    <li><strong>pca_vs_lda_comparison.png:</strong> Comparación visual PCA vs LDA</li>
                </ul>
            </div>

            <h3>🔧 Herramientas de Reproducibilidad</h3>
            <ul>
                <li><strong>modelWithLDA.py:</strong> Script completo de entrenamiento reproducible</li>
                <li><strong>complete_report.txt:</strong> Reporte técnico detallado con todos los resultados</li>
                <li><strong>Notebooks experimentales:</strong> Análisis exploratorio y validación de hipótesis</li>
                <li><strong>Requirements.txt:</strong> Dependencias exactas para reproducir el entorno</li>
            </ul>

            <h3>📊 Datasets Procesados</h3>
            <p><strong>Disponibles bajo licencia académica:</strong></p>
            <ul>
                <li>RAVDESS: Licencia Creative Commons (disponible públicamente)</li>
                <li>TESS: Licencia académica (disponible bajo solicitud)</li>
                <li>MESD: Licencia de uso educativo</li>
            </ul>
            <footer class="footnote">
                <strong>Serialización:</strong> Proceso de convertir objetos en archivos para almacenamiento. <strong>Legacy:</strong> Formato anterior mantenido para compatibilidad con versiones antiguas. <strong>Reproducibilidad:</strong> Capacidad de obtener los mismos resultados ejecutando el mismo código. <strong>Creative Commons:</strong> Sistema de licencias que permite compartir trabajo bajo ciertas condiciones.
            </footer>
        </div>

        <!-- DIAPOSITIVA 57: REFERENCIAS -->
        <div class="slide hidden" data-slide="57">
            <h2>50. Referencias y Literatura Consultada</h2>
            <h3>📚 Artículos Científicos Fundamentales</h3>
            <ul>
                <li><strong>Eyben et al. (2010):</strong> "Opensmile: the munich versatile and fast open-source audio feature extractor" - Fundamentos de extracción de características de audio</li>
                <li><strong>El Ayadi et al. (2011):</strong> "Survey on speech emotion recognition: Features, classification schemes, and databases" - Revisión comprehensiva del campo</li>
                <li><strong>Akçay & Oğuz (2020):</strong> "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers" - Estado del arte actual</li>
                <li><strong>Liesbet et al. (2018):</strong> "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)" - Descripción del dataset RAVDESS</li>
            </ul>

            <h3>🔬 Técnicas y Metodologías</h3>
            <ul>
                <li><strong>Fisher (1936):</strong> "The use of multiple measurements in taxonomic problems" - Fundamentos teóricos del LDA</li>
                <li><strong>Lecun et al. (1998):</strong> "Gradient-based learning applied to document recognition" - Bases de las redes convolucionales</li>
                <li><strong>Ioffe & Szegedy (2015):</strong> "Batch Normalization: Accelerating Deep Network Training" - Técnica de normalización utilizada</li>
                <li><strong>Srivastava et al. (2014):</strong> "Dropout: A simple way to prevent neural networks from overfitting" - Técnica de regularización implementada</li>
            </ul>

            <h3>💻 Recursos Técnicos y Librerías</h3>
            <ul>
                <li><strong>TensorFlow/Keras:</strong> Abadi et al. (2016) - Framework de deep learning utilizado</li>
                <li><strong>Librosa:</strong> McFee et al. (2015) - Librería de análisis de audio musical y de voz</li>
                <li><strong>Scikit-learn:</strong> Pedregosa et al. (2011) - Implementaciones de machine learning tradicional</li>
                <li><strong>NumPy:</strong> Harris et al. (2020) - Computación científica en Python</li>
            </ul>

            <h3>🗃️ Datasets Utilizados</h3>
            <ul>
                <li><strong>RAVDESS:</strong> Livingstone & Russo (2018) - Ryerson Audio-Visual Database of Emotional Speech and Song</li>
                <li><strong>TESS:</strong> Dupuis & Pichora-Fuller (2010) - Toronto Emotional Speech Set</li>
                <li><strong>MESD:</strong> Mexican Emotional Speech Database (2019) - Base de datos de habla emocional mexicana</li>
            </ul>

            <h3>🎓 Agradecimientos Académicos</h3>
            <p>Agradecemos a las instituciones y investigadores que han puesto sus datasets a disposición de la comunidad científica, permitiendo avances en la investigación de reconocimiento emocional en voz.</p>
            <footer class="footnote">
                <strong>Estado del arte:</strong> Los mejores métodos y resultados disponibles actualmente en un campo de investigación. <strong>Framework:</strong> Conjunto de herramientas y librerías que facilitan el desarrollo. <strong>Dataset:</strong> Conjunto de datos organizados para investigación o entrenamiento de modelos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 58: IMPACTO Y CONTRIBUCIONES -->
        <div class="slide hidden" data-slide="58">
            <h2>51. Impacto y Contribuciones del Proyecto</h2>
            <div class="highlight-box">
                <h3>🌟 Contribución Principal: Eficiencia sin Compromiso</h3>
                <p>Demostramos que la combinación estratégica de LDA + CNN 1D puede lograr resultados competitivos con una fracción de los recursos computacionales tradicionales.</p>
            </div>

            <h3>📈 Métricas de Impacto Científico</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>96.7%</h3><p>Reducción dimensional lograda</p></div>
                <div class="stat-item"><h3>180x</h3><p>Menos operaciones computacionales</p></div>
                <div class="stat-item"><h3>80.07%</h3><p>Precisión competitiva mantenida</p></div>
                <div class="stat-item"><h3>7,911</h3><p>Parámetros vs 50K+ típicos</p></div>
            </div>

            <h3>🎯 Contribuciones Metodológicas</h3>
            <ul>
                <li><strong>Pipeline LDA-CNN Optimizado:</strong> Primera implementación documentada de CNN 1D específicamente diseñada para características LDA en audio emocional</li>
                <li><strong>Análisis de Separabilidad:</strong> Visualización y cuantificación de distancias entre centroides emocionales en espacio discriminante</li>
                <li><strong>Evaluación Multi-Dataset:</strong> Validación cruzada con tres datasets de diferentes orígenes (inglés, canadiense, mexicano)</li>
                <li><strong>Benchmarking de Eficiencia:</strong> Establecimiento de métricas de rendimiento computacional para modelos ultra-ligeros</li>
            </ul>

            <h3>🔬 Validación Experimental Rigurosa</h3>
            <div class="methodology-step"><strong>División Estratificada:</strong> 64% entrenamiento, 16% validación, 20% prueba manteniendo proporciones por emoción</div>
            <div class="methodology-step"><strong>Early Stopping Automático:</strong> Prevención de overfitting con parada en época óptima (73/100)</div>
            <div class="methodology-step"><strong>Validación Cruzada:</strong> Resultados consistentes entre conjuntos de entrenamiento, validación y prueba</div>
            <div class="methodology-step"><strong>Análisis de Interpretabilidad:</strong> Visualización de filtros aprendidos y activaciones del modelo</div>

            <h3>🌍 Impacto Potencial en la Industria</h3>
            <ul>
                <li><strong>Democratización de IA Emocional:</strong> Tecnología accesible para startups y pequeñas empresas</li>
                <li><strong>Sostenibilidad Computacional:</strong> Reducción significativa del consumo energético en aplicaciones masivas</li>
                <li><strong>Innovación en Edge AI:</strong> Habilitación de procesamiento emocional en dispositivos IoT y móviles</li>
                <li><strong>Escalabilidad Cloud:</strong> Procesamiento de millones de audios con infraestructura mínima</li>
            </ul>

            <h3>📝 Publicaciones y Difusión Previstas</h3>
            <p>Resultados preparados para:</p>
            <ul>
                <li><strong>Conferencias Internacionales:</strong> ICASSP, INTERSPEECH, ACII</li>
                <li><strong>Journals Especializados:</strong> IEEE Transactions on Audio, Speech and Language Processing</li>
                <li><strong>Repositorios Abiertos:</strong> GitHub con código completo y datasets procesados</li>
                <li><strong>Comunidad Académica:</strong> Workshops en universidades y centros de investigación</li>
            </ul>
            <footer class="footnote">
                <strong>Edge AI:</strong> Inteligencia artificial que se ejecuta directamente en dispositivos finales sin necesidad de conexión a la nube. <strong>IoT:</strong> Internet of Things, red de dispositivos conectados que pueden procesar datos localmente. <strong>Benchmark:</strong> Punto de referencia para comparar el rendimiento de diferentes métodos. <strong>Peer-review:</strong> Proceso de evaluación por pares académicos antes de publicación.
            </footer>
        </div>

        <!-- DIAPOSITIVA 59: DEMOSTRACIÓN EN VIVO -->
        <div class="slide hidden" data-slide="59">
            <h2>52. Demostración del Modelo en Funcionamiento</h2>
            <div class="highlight-box">
                <h3>🎤 Predicción en Tiempo Real</h3>
                <p>A continuación mostramos el funcionamiento completo del pipeline desde un archivo de audio hasta la predicción emocional.</p>
            </div>

            <h3>Ejemplo de Predicción Completa</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Demostración Práctica</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code># Cargar modelo y objetos entrenados
model = load_model('emotion_cnn_lda_model.keras')
with open('preprocessing_objects.pkl', 'rb') as f:
    objects = pickle.load(f)

scaler = objects['scaler']
lda = objects['lda']
label_encoder = objects['label_encoder']

# Procesar audio de ejemplo
audio_file = "sample_audio_happy.wav"
result = predict_emotion_lda(audio_file, model, scaler, lda, label_encoder, extractor)

# Salida del modelo:
{
    "predicted_emotion": "happy",
    "confidence": 0.847,
    "lda_features": [2.1, -0.8, 1.3, -0.5, 0.9, -1.2],
    "all_probabilities": {
        "angry": 0.032,
        "disgust": 0.015,
        "fearful": 0.028,
        "happy": 0.847,
        "neutral": 0.041,
        "sad": 0.019,
        "surprised": 0.018
    },
    "pipeline_info": {
        "processing_time_ms": 47,
        "original_features": 180,
        "lda_features": 6,
        "model_params": 7911
    }
}</code></pre>
            </div>

            <h3>Interpretación de Resultados</h3>
            <ul>
                <li><strong>Predicción Principal:</strong> "Happy" con 84.7% de confianza</li>
                <li><strong>Características LDA:</strong> Vector [2.1, -0.8, 1.3, -0.5, 0.9, -1.2] captura la esencia discriminante</li>
                <li><strong>Distribución de Probabilidades:</strong> Todas las demás emociones <5%, indicando certeza alta</li>
                <li><strong>Tiempo de Procesamiento:</strong> 47ms total (aceptable para aplicaciones interactivas)</li>
            </ul>

            <h3>🔍 Análisis de Confianza</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>84.7%</h3><p>Confianza en predicción</p></div>
                <div class="stat-item"><h3>47ms</h3><p>Tiempo total procesamiento</p></div>
                <div class="stat-item"><h3>6</h3><p>Características discriminantes</p></div>
                <div class="stat-item"><h3>96.7%</h3><p>Compresión de información</p></div>
            </div>

            <h3>Casos de Uso en Producción</h3>
            <p>Este modelo está listo para:</p>
            <ul>
                <li><strong>API REST:</strong> Servicio web para aplicaciones móviles y web</li>
                <li><strong>Microservicios:</strong> Contenedor Docker para arquitecturas cloud-native</li>
                <li><strong>Edge Deployment:</strong> Raspberry Pi, dispositivos IoT, aplicaciones móviles</li>
                <li><strong>Batch Processing:</strong> Análisis masivo de bases de datos de audio</li>
            </ul>
            <footer class="footnote">
                <strong>API REST:</strong> Interfaz de programación que permite acceso a funcionalidades vía web. <strong>Microservicios:</strong> Arquitectura donde aplicaciones se dividen en servicios pequeños e independientes. <strong>Docker:</strong> Plataforma para crear y ejecutar aplicaciones en contenedores. <strong>Batch processing:</strong> Procesamiento de grandes volúmenes de datos en lotes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 60: LECCIONES APRENDIDAS -->
        <div class="slide hidden" data-slide="60">
            <h2>53. Lecciones Aprendidas y Reflexiones</h2>
            <h3>🎓 Aprendizajes Técnicos Clave</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Decisiones Acertadas</h4>
                    <ul>
                        <li><strong>LDA antes de CNN:</strong> La reducción dimensional supervisada fue crucial para la eficiencia</li>
                        <li><strong>Early Stopping:</strong> Previno overfitting y encontró el punto óptimo automáticamente</li>
                        <li><strong>Datasets Balanceados:</strong> La combinación de RAVDESS, TESS y MESD enriqueció la diversidad</li>
                        <li><strong>Arquitectura Minimalista:</strong> Menos capas resultaron en mejor generalización</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>⚠️ Desafíos Encontrados</h4>
                    <ul>
                        <li><strong>Datos Desbalanceados:</strong> 'Surprised' con solo 77 muestras afectó su F1-Score</li>
                        <li><strong>Dependencia del Pipeline:</strong> Modelo muy específico al preprocesamiento LDA</li>
                        <li><strong>Variabilidad Inter-Dataset:</strong> Diferencias entre calidades de grabación requirieron normalización cuidadosa</li>
                        <li><strong>Limitación Temporal:</strong> Optimizado para clips cortos, no audio largo</li>
                    </ul>
                </div>
            </div>

            <h3>🔬 Insights Científicos</h3>
            <ul>
                <li><strong>LDA Supremacía:</strong> Para problemas de clasificación, LDA superó significativamente a PCA en separabilidad</li>
                <li><strong>Eficiencia vs Precisión:</strong> No siempre es un trade-off; diseño inteligente puede lograr ambas</li>
                <li><strong>Características Discriminantes:</strong> 6 dimensiones fueron suficientes para capturar diferencias emocionales complejas</li>
                <li><strong>Convergencia Rápida:</strong> Modelos pequeños convergen más rápido y son menos propensos al overfitting</li>
            </ul>

            <h3>💡 Recomendaciones para Futuros Proyectos</h3>
            <div class="methodology-step"><strong>1. Análisis EDA Extensivo:</strong> Invertir tiempo significativo en entender la distribución y calidad de datos antes del modelado</div>
            <div class="methodology-step"><strong>2. Pipeline Modular:</strong> Diseñar componentes independientes para facilitar experimentación y debug</div>
            <div class="methodology-step"><strong>3. Métricas Múltiples:</strong> No confiar solo en accuracy; usar F1, precision, recall y análisis de confusión</div>
            <div class="methodology-step"><strong>4. Validación Cruzada:</strong> Usar múltiples datasets para verificar generalización</div>
            <div class="methodology-step"><strong>5. Documentación Continua:</strong> Mantener registro detallado de experimentos y decisiones</div>

            <h3>🎯 Aplicación a Otros Dominios</h3>
            <p>La metodología CNN+LDA demostrada aquí es aplicable a:</p>
            <ul>
                <li><strong>Reconocimiento de Patrones:</strong> Señales biomédicas, vibraciones mecánicas, series temporales financieras</li>
                <li><strong>Análisis de Texto:</strong> Clasificación de documentos, análisis de sentimientos, detección de spam</li>
                <li><strong>Visión Computacional:</strong> Clasificación de imágenes médicas, detección de defectos en manufactura</li>
                <li><strong>IoT y Sensores:</strong> Monitoreo de condiciones, mantenimiento predictivo, detección de anomalías</li>
            </ul>
            <footer class="footnote">
                <strong>Trade-off:</strong> Intercambio o compromiso entre dos objetivos que parecen incompatibles. <strong>Pipeline modular:</strong> Sistema diseñado con componentes intercambiables e independientes. <strong>Debug:</strong> Proceso de encontrar y corregir errores en el código. <strong>Generalización:</strong> Capacidad de aplicar conocimiento aprendido a situaciones nuevas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 61: AGRADECIMIENTOS FINALES -->
        <div class="slide hidden" data-slide="61">
            <h1 class="farewell-title">¡Gracias!</h1>
            <p class="farewell-message">
                Hemos demostrado que la combinación inteligente de LDA + CNN 1D puede lograr<br>
                resultados competitivos en reconocimiento emocional con eficiencia excepcional.
            </p>
            <div class="highlight-box">
                <h3>🎯 Resultados Finales Destacados</h3>
                <div class="stats-grid">
                    <div class="stat-item"><h3>80.07%</h3><p>Precisión Final</p></div>
                    <div class="stat-item"><h3>7,911</h3><p>Parámetros Entrenables</p></div>
                    <div class="stat-item"><h3>6</h3><p>Características LDA</p></div>
                    <div class="stat-item"><h3>96.7%</h3><p>Reducción Dimensional</p></div>
                </div>
            </div>

            <div class="highlight-box" style="margin: 30px 0;">
                <h3>🤔 ¿Preguntas o Comentarios?</h3>
                <p>Hemos recorrido desde los fundamentos teóricos hasta la implementación práctica de un modelo de reconocimiento emocional ultra-eficiente. El proyecto demuestra que la innovación en IA no siempre requiere más complejidad, sino diseño inteligente.</p>
            </div>

            <div class="contact-section">
                <h3>Contacto del Equipo de Desarrollo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro Pérez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>

            <div class="methodology-step" style="margin-top: 30px; text-align: center;">
                <strong>Repositorio del Proyecto:</strong> <a href="https://github.com/emotion-recognition-cnn1d-lda" style="color: var(--color-primary);">github.com/emotion-recognition-cnn1d-lda</a>
            </div>

            <div class="methodology-step" style="margin-top: 20px; text-align: center;">
                <strong>Modelo Disponible:</strong> <span style="color: var(--color-primary);">emotion_cnn_lda_model.keras</span> (Listo para producción)
            </div>

            <h3 style="text-align: center; margin-top: 40px; color: var(--color-secondary);">
                "La eficiencia en IA no viene de hacer más, sino de hacer lo correcto"
            </h3>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">← Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente →</button>
    </div>

    <script src="presentation\src\js\script.js"></script>
</body>
</html>
