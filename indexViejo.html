<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 61</div>

    <div class="presentation-container">
        
        <!-- DIAPOSITIVA 1: T√çTULO -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>

        <!-- DIAPOSITIVA 2: JUSTIFICACI√ìN -->
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
            <footer class="footnote">
                <strong>Pros√≥dicas:</strong> Caracter√≠sticas relacionadas con el ritmo, entonaci√≥n y acentuaci√≥n del habla. <strong>Espectrales:</strong> Propiedades relacionadas con la distribuci√≥n de frecuencias en la se√±al de audio.
            </footer>
        </div>

        <!-- DIAPOSITIVA 3: PROBLEMA -->
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente. <strong>Alta dimensionalidad:</strong> Datos con muchas caracter√≠sticas o variables, lo que aumenta la complejidad computacional.
            </footer>
        </div>

        <!-- DIAPOSITIVA 4: OBJETIVO GENERAL -->
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>

        <!-- DIAPOSITIVA 5: OBJETIVOS ESPEC√çFICOS -->
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
            <footer class="footnote">
                <strong>Reducci√≥n de dimensionalidad:</strong> T√©cnicas para disminuir el n√∫mero de variables en un dataset manteniendo la informaci√≥n m√°s importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 6: METODOLOG√çA -->
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, que nos permite refinar y mejorar continuamente nuestro modelo bas√°ndonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentaci√≥n y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisici√≥n de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. An√°lisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracci√≥n de Caracter√≠sticas</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluaci√≥n y Resultados</h4>
                </div>
            </div>
            <footer class="footnote">
                <strong>Metodolog√≠a iterativa:</strong> Proceso de desarrollo que se repite en ciclos, permitiendo mejoras continuas basadas en resultados previos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 7: DATASETS -->
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card">
                <h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 864 grabaciones de audio</li>
                    <li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li>
                    <li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 2,496 grabaciones vocales</li>
                    <li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li>
                    <li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 4,800 muestras de audio</li>
                    <li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li>
                    <li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Dataset:</strong> Conjunto de datos organizados para entrenamiento de modelos. <strong>Benchmarks:</strong> Est√°ndares de referencia para medir el rendimiento de un modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 8: DISTRIBUCI√ìN FINAL -->
        <div class="slide hidden" data-slide="8">
            <h2>7. Distribuci√≥n Final de Datos</h2>
            <div class="highlight-box">
                <h3>üìà Total de archivos procesados: 7,296</h3>
                <p>Despu√©s de la integraci√≥n y limpieza de todos los datasets</p>
            </div>
            <div class="single-chart-container">
                <h3>Distribuci√≥n de Emociones en el Dataset Completo</h3>
                <img src="Results/Plots/emotion_distribution.png" alt="Distribuci√≥n de Emociones" style="max-width: 100%; border-radius: 15px;">
            </div>
            <h3>Caracter√≠sticas de la distribuci√≥n:</h3>
            <ul>
                <li><strong>Entrenamiento:</strong> 4,668 muestras (64.0%)</li>
                <li><strong>Validaci√≥n:</strong> 1,168 muestras (16.0%)</li>
                <li><strong>Prueba:</strong> 1,460 muestras (20.0%)</li>
                <li><strong>Diversidad:</strong> M√∫ltiples hablantes, idiomas y contextos</li>
                <li><strong>Calidad:</strong> Grabaciones profesionales y controladas</li>
            </ul>
            <footer class="footnote">
                <strong>Distribuci√≥n balanceada:</strong> Cuando todas las clases o categor√≠as tienen aproximadamente la misma cantidad de ejemplos en el dataset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 9: EDA -->
        <div class="slide hidden" data-slide="9">
            <h2>8. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribuci√≥n de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gr√°ficos de Viol√≠n de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>EDA:</strong> An√°lisis Exploratorio de Datos, proceso de examinar datasets para descubrir patrones y relaciones. <strong>Pitch:</strong> Frecuencia fundamental de la voz, relacionada con qu√© tan aguda o grave suena.
            </footer>
        </div>

        <!-- DIAPOSITIVA 10: PITCH -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Pitch: Distribuci√≥n de Frecuencia Fundamental (F0) por Emoci√≥n</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica nos muestra c√≥mo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoci√≥n. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores m√°s altos significan un tono m√°s agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qu√© tan comunes son ciertos tonos para una emoci√≥n.
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energ√≠a como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos m√°s agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos m√°s graves y mon√≥tonos.
                </div>
            </div>
            <footer class="footnote">
                <strong>Frecuencia fundamental (F0):</strong> La frecuencia m√°s baja de una se√±al peri√≥dica, determina el pitch percibido. <strong>Hertz (Hz):</strong> Unidad de medida de frecuencia, equivale a ciclos por segundo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 11: MFCCS -->
        <div class="slide hidden" data-slide="11">
            <h2>10. Visualizaci√≥n y An√°lisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>An√°lisis Espectral: Distribuci√≥n de los Primeros 13 MFCCs por Emoci√≥n</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gr√°ficos de Viol√≠n de MFCCs por Emoci√≥n" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualizaci√≥n nos permite comparar la "forma" del sonido para cada emoci√≥n a trav√©s de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "viol√≠n" muestra el rango y la concentraci√≥n de valores para un coeficiente (eje X) y una emoci√≥n (color).
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribuci√≥n para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posici√≥n de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoci√≥n de otra.
                </div>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretaci√≥n General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energ√≠a total o sonoridad de la se√±al.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles m√°s finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>MFCCs:</strong> Coeficientes que capturan las caracter√≠sticas del espectro de frecuencias de manera similar a como el o√≠do humano percibe el sonido. <strong>Cepstral:</strong> An√°lisis en el dominio del cepstrum, √∫til para separar la fuente del filtro en se√±ales de voz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 12: FLUJO DE PROCESAMIENTO -->
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoci√≥n, debemos traducir la onda de sonido a un lenguaje que entienda: los n√∫meros. Este proceso se llama <strong>Extracci√≥n de Caracter√≠sticas</strong>. A continuaci√≥n, veremos el paso a paso de c√≥mo convertimos un archivo de audio en un √∫nico vector de 180 caracter√≠sticas.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">üîä</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">#Ô∏è‚É£</div>
                    <div class="flow-text">Digitalizaci√≥n (Muestreo)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üñºÔ∏è</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üìä</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                     <div class="flow-icon">üß¨</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Vector:</strong> Lista ordenada de n√∫meros que representa las caracter√≠sticas extra√≠das del audio. <strong>Frames:</strong> Peque√±os segmentos de audio analizados individualmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 13: DIGITALIZACI√ìN -->
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 1: Digitalizaci√≥n y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalizaci√≥n</h4>
                    <p>Una onda de sonido es una se√±al anal√≥gica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de n√∫meros, donde cada n√∫mero es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es est√°tica. Para analizarla, la dividimos en peque√±os segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tama√±o de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colecci√≥n de muchos arreglos peque√±os (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio_wave_sampling_and_framing.png" alt="Diagrama de Digitalizaci√≥n y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en m√∫ltiples ventanas (frames) para su an√°lisis.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Muestreo:</strong> Proceso de convertir una se√±al continua en una secuencia discreta de valores. <strong>Amplitud:</strong> Intensidad o fuerza de la onda sonora en un momento dado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 14: FFT -->
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¬øQu√© es y para qu√© sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qu√© frecuencias la componen. La <strong>Transformada R√°pida de Fourier (FFT)</strong> es la herramienta matem√°tica que lo hace posible.</p>
                    <p>La FFT descompone la se√±al del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energ√≠a vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qu√© tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matem√°tico: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un n√∫mero complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El n√∫mero total de muestras en la ventana.</li>
                            <li>$k$: El √≠ndice de la frecuencia que se est√° calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time_domain_to_frequency_domain_FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>FFT:</strong> Algoritmo eficiente para calcular la Transformada Discreta de Fourier. <strong>Dominio del tiempo vs. frecuencia:</strong> Dos formas de representar la misma se√±al, enfoc√°ndose en cu√°ndo ocurren los eventos vs. qu√© frecuencias contiene.
            </footer>
        </div>

        <!-- DIAPOSITIVA 15: MFCCs -->
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>M√°s all√° del Espectro</h4>
                    <p>El espectro de la FFT es √∫til, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho m√°s inteligente de resumir la informaci√≥n del espectro, imitando c√≥mo funciona el o√≠do humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logar√≠tmica, similar a nuestra percepci√≥n auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energ√≠as, de nuevo, para imitar c√≥mo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operaci√≥n que comprime toda esa informaci√≥n espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripci√≥n num√©rica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC_block_diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Escala Mel:</strong> Escala perceptual de frecuencias que imita c√≥mo el o√≠do humano percibe diferencias en el tono. <strong>DCT:</strong> Transformada que convierte se√±ales al dominio de frecuencia usando solo funciones coseno. <strong>Timbre:</strong> Cualidad que diferencia sonidos con el mismo pitch y volumen.
            </footer>
        </div>

        <!-- DIAPOSITIVA 16: OPERACI√ìN PROMEDIO -->
        <div class="slide hidden" data-slide="16">
            <h2 class="technical-title">Paso 4: Operaci√≥n de Promedio para Vector Final</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del An√°lisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de caracter√≠sticas, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 caracter√≠sticas.</p>
                    <p>Para obtener un √∫nico vector $V$ que represente todo el audio, calculamos la media de cada caracter√≠stica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">C√°lculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                            <li>$V_j$: Es el valor final de la caracter√≠stica $j$ en nuestro vector.</li>
                            <li>$T$: Es el n√∫mero total de ventanas (frames) en el audio.</li>
                            <li>$M_{t,j}$: Es el valor de la caracter√≠stica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                    <h4>¬øPor qu√© usar el promedio?</h4>
                   <ul>
                        <li><strong>Reducci√≥n temporal:</strong> Condensa informaci√≥n variable en el tiempo a un valor estable</li>
                        <li><strong>Robustez:</strong> El promedio es menos sensible a valores at√≠picos en ventanas individuales</li>
                        <li><strong>Representatividad:</strong> Captura las caracter√≠sticas dominantes del audio completo</li>
                        <li><strong>Compatibilidad:</strong> Produce un vector de tama√±o fijo para cualquier duraci√≥n de audio</li>
                    </ul>
                   <p>Este proceso condensa la informaci√≥n temporal en una sola "ficha t√©cnica" que describe las propiedades ac√∫sticas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature_matrix_averaging_to_vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
            <footer class="footnote">
                <strong>Promedio aritm√©tico:</strong> Suma de todos los valores dividida entre el n√∫mero total de valores. <strong>Vector de tama√±o fijo:</strong> Representaci√≥n num√©rica con dimensiones constantes, independiente de la duraci√≥n del audio original.
            </footer>
        </div>

        <!-- DIAPOSITIVA 17: NORMALIZACI√ìN DE AUDIO -->
        <div class="slide hidden" data-slide="17">
            <h2 class="technical-title">Normalizaci√≥n de Audio: ¬øPor qu√© es Crucial?</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øQu√© es la Normalizaci√≥n de Audio?</h4>
                    <p>La normalizaci√≥n ajusta la amplitud de una se√±al de audio para que su valor m√°ximo sea 1.0 y el m√≠nimo sea -1.0, estandarizando el volumen entre diferentes grabaciones.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">F√≥rmula de Normalizaci√≥n</span>
                        <p>$x_{norm}[n] = \frac{x[n]}{\max(|x[n]|)}$</p>
                        <ul>
                            <li>$x[n]$: Se√±al de audio original</li>
                            <li>$x_{norm}[n]$: Se√±al normalizada</li>
                            <li>$\max(|x[n]|)$: Valor absoluto m√°ximo de la se√±al</li>
                        </ul>
                    </div>

                    <h4>Ventajas Cr√≠ticas de la Normalizaci√≥n:</h4>
                    <div class="advantages-disadvantages">
                        <div class="advantages">
                            <h4>‚úÖ Beneficios T√©cnicos</h4>
                            <ul>
                                <li><strong>Consistencia de Volumen:</strong> Elimina diferencias de grabaci√≥n entre micr√≥fonos y entornos</li>
                                <li><strong>Estabilidad Num√©rica:</strong> Previene overflow y underflow en c√°lculos posteriores</li>
                                <li><strong>Mejor Convergencia:</strong> Los algoritmos de ML convergen m√°s r√°pido con datos normalizados</li>
                                <li><strong>Robustez:</strong> Reduce sensibilidad a variaciones de hardware de grabaci√≥n</li>
                            </ul>
                        </div>
                        <div class="disadvantages">
                            <h4>‚ö†Ô∏è Sin Normalizaci√≥n</h4>
                            <ul>
                                <li><strong>Sesgo por Volumen:</strong> Grabaciones m√°s fuertes dominar√≠an el entrenamiento</li>
                                <li><strong>Caracter√≠sticas Distorsionadas:</strong> MFCCs y otras caracter√≠sticas ser√≠an inconsistentes</li>
                                <li><strong>Gradientes Inestables:</strong> Entrenamiento del modelo ser√≠a err√°tico</li>
                                <li><strong>Clasificaci√≥n Sesgada:</strong> El modelo podr√≠a aprender a clasificar por volumen, no por emoci√≥n</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Overflow/Underflow:</strong> Errores num√©ricos cuando los valores son demasiado grandes o peque√±os para ser representados. <strong>Convergencia:</strong> Proceso por el cual un algoritmo de aprendizaje alcanza una soluci√≥n estable.
            </footer>
        </div>

        <!-- DIAPOSITIVA 18: PREPROCESAMIENTO -->
        <div class="slide hidden" data-slide="18">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
            <footer class="footnote">
                <strong>PCA:</strong> An√°lisis de Componentes Principales, t√©cnica para reducir dimensiones preservando varianza. <strong>LDA:</strong> An√°lisis Discriminante Lineal, reduce dimensiones maximizando separaci√≥n entre clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 19: STANDARD SCALER -->
        <div class="slide hidden" data-slide="19">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria. <strong>Z-score:</strong> Medida estad√≠stica que indica cu√°ntas desviaciones est√°ndar est√° un valor de la media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 20: PCA VS LDA -->
        <div class="slide hidden" data-slide="20">
            <h2>13. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo. <strong>Varianza:</strong> Medida de dispersi√≥n que indica cu√°nto se alejan los datos de su media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 21: PCA 3D -->
        <div class="slide hidden" data-slide="21">
            <h2>14. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Componentes Principales:</strong> Nuevos ejes calculados que capturan la m√°xima variabilidad de los datos originales. <strong>Proyecci√≥n:</strong> Transformaci√≥n de datos de alta dimensi√≥n a un espacio de menor dimensi√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 22: LDA 3D ACTUALIZADA -->
        <div class="slide hidden" data-slide="22">
            <h2>15. Visualizaci√≥n 3D: LDA con Resultados Actuales</h2>
            <div class="iframe-plot-container">
                <iframe src="Results/LDA/lda_3d_visualization.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠ vemos nuestros resultados reales: el LDA ha reducido las 180 caracter√≠sticas originales a solo <strong>6 componentes discriminantes</strong>, capturando el 100% de la varianza entre clases. La separaci√≥n clara entre las emociones demuestra la efectividad del LDA para maximizar la distinci√≥n entre categor√≠as emocionales.
            </p>
            <div class="stats-grid">
                <div class="stat-item"><h3>180 ‚Üí 6</h3><p>Reducci√≥n dimensional (96.7%)</p></div>
                <div class="stat-item"><h3>100%</h3><p>Varianza explicada total</p></div>
                <div class="stat-item"><h3>7 clases</h3><p>Emociones separadas</p></div>
            </div>
            <footer class="footnote">
                <strong>Discriminantes Lineales (LD):</strong> Combinaciones lineales de las caracter√≠sticas originales que mejor separan las clases. <strong>C√∫mulos:</strong> Agrupaciones de puntos similares en el espacio de caracter√≠sticas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 23: COMPARACI√ìN PCA LDA ACTUALIZADA -->
        <div class="slide hidden" data-slide="23">
            <h2>16. Resultados LDA: An√°lisis Detallado</h2>
            <div class="single-chart-container">
                <h3>An√°lisis LDA: Componentes y Separabilidad</h3>
                <img src="Results/LDA/lda_analysis_main.png" alt="An√°lisis LDA Principal" style="max-width: 100%; border-radius: 15px;">
            </div>
            <h3>Tabla Comparativa: Caracter√≠sticas del An√°lisis</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Dimensiones finales</td>
                        <td>Configurables (t√≠picamente 3-50)</td>
                        <td><strong>6 componentes</strong></td>
                    </tr>
                    <tr>
                        <td>Varianza explicada</td>
                        <td>~85-95% (depende de componentes)</td>
                        <td><strong>100%</strong></td>
                    </tr>
                    <tr>
                        <td>Separabilidad de clases</td>
                        <td>No optimizada</td>
                        <td><strong>Maximizada</strong></td>
                    </tr>
                    <tr>
                        <td>Uso de etiquetas</td>
                        <td>No utiliza las etiquetas emocionales</td>
                        <td><strong>Optimizado con etiquetas</strong></td>
                    </tr>
                </tbody>
            </table>
            <h3>Interpretaci√≥n de Resultados</h3>
            <p>Nuestro LDA logr√≥ una <strong>reducci√≥n excepcional del 96.7%</strong> (de 180 a 6 caracter√≠sticas) manteniendo toda la informaci√≥n discriminativa. Esto demuestra que las emociones en voz pueden representarse eficientemente en un espacio de muy baja dimensi√≥n cuando se optimiza para separabilidad.</p>
            <footer class="footnote">
                <strong>Separabilidad:</strong> Medida de qu√© tan bien se pueden distinguir diferentes clases en un espacio de caracter√≠sticas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 24: NUEVA - CORRELACI√ìN LDA -->
        <div class="slide hidden" data-slide="24">
            <h2>17. Matriz de Correlaci√≥n: Componentes LDA</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Independencia entre Componentes Discriminantes</h3>
                <img src="Results/LDA/lda_correlation_matrix.png" alt="Matriz de Correlaci√≥n LDA" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Esta matriz muestra las correlaciones entre los 6 componentes LDA. Los valores cercanos a 0 (colores fr√≠os) indican independencia entre componentes, mientras que valores cercanos a ¬±1 (colores c√°lidos) indican correlaci√≥n fuerte.</p>
            <div class="chart-observation">
                <strong>Observaciones Clave:</strong> Los componentes LDA muestran correlaciones bajas entre s√≠, lo que confirma que cada uno captura informaci√≥n √∫nica y complementaria para distinguir emociones. Esta independencia es crucial para el rendimiento del modelo CNN.
            </div>
            <h3>Implicaciones para el Modelo:</h3>
            <ul>
                <li><strong>Reducci√≥n de Redundancia:</strong> Componentes independientes evitan informaci√≥n duplicada</li>
                <li><strong>Eficiencia Computacional:</strong> Menos par√°metros en la CNN sin p√©rdida de informaci√≥n</li>
                <li><strong>Mejor Generalizaci√≥n:</strong> Caracter√≠sticas no redundantes mejoran el aprendizaje</li>
                <li><strong>Interpretabilidad:</strong> Cada componente representa aspectos √∫nicos de las emociones</li>
            </ul>
            <footer class="footnote">
                <strong>Correlaci√≥n:</strong> Medida estad√≠stica que indica la relaci√≥n lineal entre dos variables, desde -1 (correlaci√≥n negativa perfecta) hasta +1 (correlaci√≥n positiva perfecta). <strong>Independencia estad√≠stica:</strong> Cuando el conocimiento de una variable no proporciona informaci√≥n sobre otra.
            </footer>
        </div>

        <!-- DIAPOSITIVA 25: NUEVA - DISTANCIAS CENTROIDES -->
        <div class="slide hidden" data-slide="25">
            <h2>18. Distancias entre Centroides Emocionales</h2>
            <div class="single-chart-container">
                <h3>Mapa de Separabilidad: Distancias en el Espacio LDA</h3>
                <img src="Results/LDA/emotion_centroids_distances.png" alt="Distancias entre Centroides" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Este mapa de calor muestra las distancias euclidianas entre los centroides (puntos promedio) de cada emoci√≥n en el espacio LDA de 6 dimensiones. Colores m√°s intensos indican mayor separaci√≥n entre emociones.</p>
            <div class="chart-observation">
                <strong>Interpretaci√≥n Psicoac√∫stica:</strong> Las emociones m√°s distantes (colores brillantes) son m√°s f√°ciles de distinguir para el modelo, mientras que las cercanas (colores oscuros) representan mayor desaf√≠o de clasificaci√≥n. Por ejemplo, 'angry' y 'sad' podr√≠an estar m√°s separadas que 'happy' y 'surprised'.
            </div>
            <h3>An√°lisis de Confusi√≥n Predictiva:</h3>
            <ul>
                <li><strong>M√°xima Separaci√≥n:</strong> Emociones con distancias >3.0 rara vez se confunden</li>
                <li><strong>Separaci√≥n Moderada:</strong> Distancias 1.5-3.0 ocasionalmente generan errores</li>
                <li><strong>Separaci√≥n M√≠nima:</strong> Distancias <1.5 son propensas a confusi√≥n</li>
                <li><strong>Validaci√≥n Te√≥rica:</strong> Coincide con estudios psicol√≥gicos sobre similaridad emocional</li>
            </ul>
            <footer class="footnote">
                <strong>Centroide:</strong> Punto promedio de todas las muestras de una clase en el espacio de caracter√≠sticas. <strong>Distancia euclidiana:</strong> Medida de distancia "en l√≠nea recta" entre dos puntos en un espacio multidimensional. <strong>Psicoac√∫stica:</strong> Estudio de la percepci√≥n psicol√≥gica del sonido.
            </footer>
        </div>

        <!-- DIAPOSITIVA 26: NUEVA - COMPARACI√ìN PCA VS LDA -->
        <div class="slide hidden" data-slide="26">
            <h2>19. Comparaci√≥n Visual: PCA vs LDA</h2>
            <div class="single-chart-container">
                <h3>Efectividad Comparativa para Separaci√≥n de Emociones</h3>
                <img src="Results/LDA/pca_vs_lda_comparison.png" alt="Comparaci√≥n PCA vs LDA" style="max-width: 100%; border-radius: 15px;">
            </div>
            <p>Esta comparaci√≥n lado a lado demuestra visualmente por qu√© elegimos LDA sobre PCA para nuestro problema de clasificaci√≥n emocional.</p>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Ventajas del LDA en Nuestro Caso</h4>
                    <ul>
                        <li><strong>Separaci√≥n Clara:</strong> C√∫mulos emocionales bien definidos</li>
                        <li><strong>Reducci√≥n Extrema:</strong> Solo 6 dimensiones vs potenciales 50+ en PCA</li>
                        <li><strong>100% Varianza:</strong> No hay p√©rdida de informaci√≥n discriminativa</li>
                        <li><strong>Optimizaci√≥n Dirigida:</strong> Espec√≠ficamente dise√±ado para clasificaci√≥n</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>‚ö†Ô∏è Limitaciones del PCA</h4>
                    <ul>
                        <li><strong>Solapamiento:</strong> Las emociones se mezclan en el espacio PCA</li>
                        <li><strong>Varianza vs Separabilidad:</strong> Optimiza varianza, no clasificaci√≥n</li>
                        <li><strong>M√°s Dimensiones:</strong> Requiere m√°s componentes para capturar informaci√≥n</li>
                        <li><strong>Informaci√≥n Irrelevante:</strong> Puede preservar ruido no discriminativo</li>
                    </ul>
                </div>
            </div>
            <h3>Conclusi√≥n Metodol√≥gica:</h3>
            <p>La elecci√≥n de LDA result√≥ crucial para el √©xito del proyecto, permitiendo un modelo m√°s eficiente y preciso al trabajar con caracter√≠sticas optimizadas para distinguir emociones.</p>
            <footer class="footnote">
                <strong>C√∫mulos:</strong> Agrupaciones naturales de datos similares en el espacio de caracter√≠sticas. <strong>Informaci√≥n discriminativa:</strong> Caracter√≠sticas que ayudan a distinguir entre diferentes clases o categor√≠as.
            </footer>
        </div>

        <!-- DIAPOSITIVA 27: INTRODUCCI√ìN CNN 1D -->
        <div class="slide hidden" data-slide="27">
            <h2>20. Introducci√≥n a las Redes Neuronales Convolucionales 1D</h2>
            <div class="highlight-box">
                <h3>¬øQu√© es una CNN 1D?</h3>
                <p>Una Red Neuronal Convolucional 1D es un tipo especializado de red neuronal dise√±ada para procesar secuencias de datos, como series temporales o caracter√≠sticas extra√≠das de audio.</p>
            </div>
            <h3>¬øPor qu√© CNN 1D para Audio + LDA?</h3>
            <ul>
                <li><strong>Entrada Optimizada:</strong> Procesa directamente las 6 caracter√≠sticas LDA discriminantes</li>
                <li><strong>Detecci√≥n de Patrones:</strong> Identifica relaciones entre componentes discriminantes</li>
                <li><strong>Eficiencia Extrema:</strong> Solo 7,911 par√°metros vs modelos tradicionales de 50K+</li>
                <li><strong>Precisi√≥n Mantenida:</strong> 80.07% con entrada ultra-comprimida</li>
            </ul>
            <h3>Nuestra Arquitectura Innovadora</h3>
            <p>Dise√±amos una CNN 1D ultra-eficiente que toma como entrada un vector de solo <strong>6 caracter√≠sticas LDA</strong> y produce probabilidades para 7 emociones diferentes, logrando una precisi√≥n excepcional con m√≠nimos recursos computacionales.</p>
            <footer class="footnote">
                <strong>CNN:</strong> Convolutional Neural Network, tipo de red neuronal que usa operaciones de convoluci√≥n. <strong>1D:</strong> Unidimensional, opera sobre secuencias lineales de datos. <strong>Caracter√≠sticas discriminantes:</strong> Representaciones optimizadas para distinguir entre clases diferentes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 28: CAPA DE ENTRADA ACTUALIZADA -->
        <div class="slide hidden" data-slide="28">
            <h2>21. Capa de Entrada (Input Layer) - Optimizada con LDA</h2>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Entrada:</strong> <span class="output-shape">(None, 6, 1)</span></p>
                        <p><strong>Neuronas:</strong> 6 (una por componente LDA)</p>
                        <p><strong>Caracter√≠sticas:</strong> 6 componentes LDA</p>
                        <p><strong>Funci√≥n:</strong> Recibir vector discriminante optimizado</p>
                        <p><strong>Par√°metros:</strong> 0 (no aprende, solo recibe datos)</p>
                    </div>
                </div>
            </div>
            <h3>Detalles T√©cnicos de la Optimizaci√≥n</h3>
            <ul>
                <li><strong>6 neuronas de entrada:</strong> Una neurona por cada componente discriminante LDA</li>
                <li><strong>Reducci√≥n del 96.7%:</strong> Desde 180 caracter√≠sticas originales</li>
                <li><strong>Varianza preservada:</strong> 100% de la informaci√≥n discriminativa mantenida</li>
                <li><strong>Formato compacto:</strong> Entrada ultra-eficiente para la CNN</li>
                <li><strong>None (batch):</strong> Permite procesar m√∫ltiples audios simult√°neamente</li>
            </ul>
            <h3>Pipeline de Preprocesamiento</h3>
            <p>Antes de entrar a la red, los datos fueron:</p>
            <ul>
                <li>‚úÖ Normalizados (StandardScaler) en 180 caracter√≠sticas originales</li>
                <li>‚úÖ Reducidos con LDA a 6 componentes discriminantes</li>
                <li>‚úÖ Reformateados a shape (6, 1) para CNN 1D</li>
                <li>‚úÖ Divididos estratificadamente en train/validation/test</li>
            </ul>
            <footer class="footnote">
                <strong>Neurona de entrada:</strong> Unidad que recibe un valor de entrada espec√≠fico sin procesarlo. <strong>Batch:</strong> Conjunto de muestras procesadas simult√°neamente para eficiencia computacional. <strong>Shape:</strong> Dimensiones de los datos (filas, columnas, canales). <strong>Estratificado:</strong> Divisi√≥n que mantiene la proporci√≥n de clases en cada subset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 29: PRIMERA CONVOLUCI√ìN ACTUALIZADA -->
        <div class="slide hidden" data-slide="29">
            <h2>22. Primera Capa Convolucional - Adaptada para LDA</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_1 (64 filtros, kernel=3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 1)</p>
                        <p><strong>Salida:</strong> (None, 6, 64)</p>
                        <p><strong>Neuronas activadas:</strong> 384 (6 posiciones √ó 64 filtros)</p>
                        <p><strong>Par√°metros:</strong> 256 (192 pesos + 64 bias)</p>
                        <p><strong>Activaci√≥n:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¬øQu√© hace esta capa con las caracter√≠sticas LDA?</h3>
            <ul>
                <li><strong>64 filtros especializados:</strong> Cada uno aprende a detectar patrones espec√≠ficos entre componentes LDA</li>
                <li><strong>384 neuronas:</strong> 6 posiciones √ó 64 filtros = 384 activaciones por muestra</li>
                <li><strong>Kernel size 3:</strong> Analiza relaciones entre 3 componentes discriminantes consecutivos</li>
                <li><strong>Padding 'same':</strong> Mantiene las 6 dimensiones LDA</li>
                <li><strong>ReLU:</strong> Activa solo patrones discriminantes positivos</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Convoluci√≥n sobre LDA</span>
                <p>Para cada posici√≥n $i$ y filtro $f$:</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{2} w_j^f \cdot LDA_{i+j} + b^f\right)$</p>
                <ul>
                    <li>$w_j^f$: Peso $j$ del filtro $f$ (3 pesos por filtro)</li>
                    <li>$LDA_{i+j}$: Componente LDA en posici√≥n $i+j$</li>
                    <li>$b^f$: Bias del filtro $f$ (64 bias total)</li>
                    <li>Total pesos: 3 √ó 64 = 192 pesos</li>
                </ul>
            </div>
            <h3>C√°lculo Detallado de Par√°metros:</h3>
            <ul>
                <li><strong>Pesos de convoluci√≥n:</strong> 3 (kernel) √ó 1 (canal entrada) √ó 64 (filtros) = 192</li>
                <li><strong>Par√°metros bias:</strong> 64 (uno por filtro)</li>
                <li><strong>Total par√°metros:</strong> 192 + 64 = 256</li>
                <li><strong>Eficiencia lograda:</strong> 66% reducci√≥n vs versi√≥n original (768 par√°metros)</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas activadas:</strong> Cantidad de valores de salida que produce la capa (posiciones √ó filtros). <strong>Filtro/Kernel:</strong> Conjunto de pesos que se desliza sobre los datos para detectar patrones. <strong>Componentes LDA:</strong> Caracter√≠sticas discriminantes optimizadas para separar clases emocionales. <strong>Bias:</strong> Par√°metro que permite ajustar el umbral de activaci√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 30: BATCH NORMALIZATION 1 -->
        <div class="slide hidden" data-slide="30">
            <h2>23. Primera Capa de Normalizaci√≥n por Lotes</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm">
                    <h4>BatchNormalization_1</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 64)</p>
                        <p><strong>Salida:</strong> (None, 6, 64)</p>
                        <p><strong>Neuronas procesadas:</strong> 384 (6 √ó 64)</p>
                        <p><strong>Par√°metros:</strong> 256 (128 Œ≥ + 128 Œ≤ + 128 Œº + 128 œÉ¬≤)</p>
                        <p><strong>Funci√≥n:</strong> Normalizar activaciones y estabilizar entrenamiento</p>
                    </div>
                </div>
            </div>
            <h3>¬øQu√© hace BatchNormalization despu√©s de Conv1D?</h3>
            <ul>
                <li><strong>Normalizaci√≥n por canal:</strong> Cada uno de los 64 filtros se normaliza independientemente</li>
                <li><strong>384 neuronas normalizadas:</strong> Procesa todas las activaciones (6 posiciones √ó 64 canales)</li>
                <li><strong>Estabilizaci√≥n del gradiente:</strong> Reduce el desplazamiento covariante interno</li>
                <li><strong>Aceleraci√≥n del entrenamiento:</strong> Permite usar learning rates m√°s altos</li>
                <li><strong>Regularizaci√≥n impl√≠cita:</strong> A√±ade ruido controlado que act√∫a como regularizador</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Batch Normalization</span>
                <p>Para cada canal $c$ de los 64 filtros:</p>
                <p>$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}$</p>
                <p>$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$</p>
                <ul>
                    <li>$\mu_c$: Media del canal $c$ en el batch</li>
                    <li>$\sigma_c^2$: Varianza del canal $c$ en el batch</li>
                    <li>$\gamma_c$: Par√°metro de escala aprendible (64 total)</li>
                    <li>$\beta_c$: Par√°metro de desplazamiento aprendible (64 total)</li>
                    <li>$\epsilon = 1e-3$: Constante para estabilidad num√©rica</li>
                </ul>
            </div>
            <h3>Configuraci√≥n Espec√≠fica para LDA:</h3>
            <ul>
                <li><strong>Momentum:</strong> 0.99 (memoria de estad√≠sticas durante entrenamiento)</li>
                <li><strong>Axis:</strong> -1 (normalizaci√≥n por canal de filtro)</li>
                <li><strong>Center:</strong> True (usar par√°metro Œ≤)</li>
                <li><strong>Scale:</strong> True (usar par√°metro Œ≥)</li>
                <li><strong>64 canales normalizados:</strong> Uno por cada filtro convolucional</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas procesadas:</strong> Cantidad total de activaciones que la capa normaliza. <strong>Desplazamiento covariante interno:</strong> Cambio en la distribuci√≥n de activaciones durante entrenamiento que ralentiza convergencia. <strong>Par√°metros aprendibles:</strong> Œ≥ y Œ≤ se optimizan durante backpropagation. <strong>Estad√≠sticas del batch:</strong> Media y varianza calculadas sobre el mini-batch actual.
            </footer>
        </div>

        <!-- DIAPOSITIVA 31: MAX POOLING 1 -->
        <div class="slide hidden" data-slide="31">
            <h2>24. Primera Capa de Max Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>MaxPooling1D_1 (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 6, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 64)</p>
                        <p><strong>Neuronas resultantes:</strong> 192 (3 √ó 64)</p>
                        <p><strong>Par√°metros:</strong> 0 (operaci√≥n determin√≠stica)</p>
                        <p><strong>Funci√≥n:</strong> Reducci√≥n dimensional y extracci√≥n de caracter√≠sticas dominantes</p>
                    </div>
                </div>
            </div>
            <h3>¬øC√≥mo funciona Max Pooling en caracter√≠sticas LDA?</h3>
            <ul>
                <li><strong>Reducci√≥n 6‚Üí3:</strong> Toma ventanas de tama√±o 2 y selecciona el valor m√°ximo</li>
                <li><strong>192 neuronas finales:</strong> 3 posiciones √ó 64 canales = 192 activaciones</li>
                <li><strong>Invarianza traslacional:</strong> Hace el modelo robusto a peque√±os desplazamientos</li>
                <li><strong>Compresi√≥n de informaci√≥n:</strong> Retiene solo las activaciones m√°s fuertes</li>
                <li><strong>Reducci√≥n computacional:</strong> 50% menos activaciones para capas posteriores</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n Max Pooling 1D</span>
                <p>Para cada canal $c$ y posici√≥n de salida $j$:</p>
                <p>$y_{j,c} = \max(x_{2j,c}, x_{2j+1,c})$</p>
                <p>Donde:</p>
                <ul>
                    <li>$x_{i,c}$: Activaci√≥n en posici√≥n $i$ del canal $c$</li>
                    <li>$y_{j,c}$: Salida en posici√≥n $j$ del canal $c$</li>
                    <li>Pool size = 2: Ventana de pooling</li>
                    <li>Stride = 2: Sin solapamiento entre ventanas</li>
                </ul>
            </div>
            
            <h3>Mapeo Espec√≠fico para Componentes LDA:</h3>
            <ul>
                <li><strong>LDA_1, LDA_2 ‚Üí Posici√≥n 0:</strong> max(activaciones de componentes 1-2)</li>
                <li><strong>LDA_3, LDA_4 ‚Üí Posici√≥n 1:</strong> max(activaciones de componentes 3-4)</li>
                <li><strong>LDA_5, LDA_6 ‚Üí Posici√≥n 2:</strong> max(activaciones de componentes 5-6)</li>
                <li><strong>Preservaci√≥n de 64 canales:</strong> Cada filtro convolucional mantiene su representaci√≥n</li>
                <li><strong>384 ‚Üí 192 neuronas:</strong> Reducci√≥n exacta del 50%</li>
            </ul>

            <h3>Ventajas en el Contexto LDA:</h3>
            <ul>
                <li><strong>Robustez a variaciones:</strong> Compensa peque√±as fluctuaciones en componentes discriminantes</li>
                <li><strong>Eficiencia computacional:</strong> 50% de reducci√≥n en dimensiones temporales</li>
                <li><strong>Foco en caracter√≠sticas dominantes:</strong> Enfatiza patrones m√°s fuertes</li>
                <li><strong>Preparaci√≥n para siguiente capa:</strong> Entrada optimizada para Conv1D_2</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas resultantes:</strong> Cantidad de activaciones que produce la capa tras el pooling. <strong>Invarianza traslacional:</strong> Propiedad que hace que el modelo sea insensible a peque√±os desplazamientos en la entrada. <strong>Stride:</strong> Paso con el que se mueve la ventana de pooling. <strong>Determin√≠stica:</strong> Operaci√≥n que siempre produce el mismo resultado para la misma entrada.
            </footer>
        </div>

        <!-- DIAPOSITIVA 32: DROPOUT 1 -->
        <div class="slide hidden" data-slide="32">
            <h2>25. Primera Capa de Dropout (Regularizaci√≥n)</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_1 (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 64)</p>
                        <p><strong>Neuronas regularizadas:</strong> 192 (3 √ó 64)</p>
                        <p><strong>Par√°metros:</strong> 0 (m√°scara aleatoria)</p>
                        <p><strong>Funci√≥n:</strong> Prevenir overfitting mediante desactivaci√≥n estoc√°stica</p>
                    </div>
                </div>
            </div>
            <h3>¬øC√≥mo funciona Dropout en nuestro modelo?</h3>
            <ul>
                <li><strong>Probabilidad de desactivaci√≥n:</strong> 30% de las neuronas se apagan aleatoriamente</li>
                <li><strong>192 neuronas afectadas:</strong> Cada una de las 3√ó64 activaciones tiene 30% prob. de ser 0</li>
                <li><strong>Solo durante entrenamiento:</strong> En inferencia todas las neuronas est√°n activas</li>
                <li><strong>Escalado compensatorio:</strong> Las neuronas activas se escalan por factor 1/(1-0.3)</li>
                <li><strong>Prevenci√≥n de co-adaptaci√≥n:</strong> Evita dependencias excesivas entre neuronas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Dropout</span>
                <p><strong>Durante entrenamiento:</strong></p>
                <p>$r_{i,c} \sim \text{Bernoulli}(1-p)$ donde $p = 0.3$</p>
                <p>$y_{i,c} = \frac{r_{i,c} \cdot x_{i,c}}{1-p}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_{i,c} = x_{i,c}$</p>
                <ul>
                    <li>$r_{i,c}$: M√°scara binaria aleatoria (0 o 1)</li>
                    <li>$p = 0.3$: Tasa de dropout</li>
                    <li>$1-p = 0.7$: Probabilidad de mantener neurona</li>
                    <li>Escalado por $\frac{1}{1-p}$: Compensar activaciones perdidas</li>
                </ul>
            </div>
            
            <h3>Configuraci√≥n Espec√≠fica para Caracter√≠sticas LDA:</h3>
            <ul>
                <li><strong>Rate moderado (0.3):</strong> Balance entre regularizaci√≥n y preservaci√≥n de informaci√≥n LDA</li>
                <li><strong>Aplicaci√≥n por neurona:</strong> Cada activaci√≥n (3√ó64=192) tiene 30% prob. de ser 0</li>
                <li><strong>~58 neuronas activas:</strong> En promedio 192√ó0.7 = 134 neuronas activas por forward pass</li>
                <li><strong>Seed reproductible:</strong> Determinismo para experimentaci√≥n controlada</li>
                <li><strong>Training=True autom√°tico:</strong> Keras controla el modo seg√∫n la fase</li>
            </ul>

            <h3>Impacto en el Aprendizaje:</h3>
            <ul>
                <li><strong>Generalizaci√≥n mejorada:</strong> El modelo no depende excesivamente de neuronas espec√≠ficas</li>
                <li><strong>Robustez a componentes LDA:</strong> Aprende patrones usando subconjuntos de activaciones</li>
                <li><strong>Reducci√≥n de varianza:</strong> Ensemble impl√≠cito de redes m√°s peque√±as</li>
                <li><strong>Calibraci√≥n de confianza:</strong> Mejora la estimaci√≥n de incertidumbre del modelo</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas regularizadas:</strong> Cantidad total de activaciones que pueden ser desactivadas por dropout. <strong>Overfitting:</strong> Sobreajuste donde el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Co-adaptaci√≥n:</strong> Dependencia excesiva entre neuronas que reduce robustez. <strong>Bernoulli:</strong> Distribuci√≥n de probabilidad binaria (0 o 1). <strong>Ensemble impl√≠cito:</strong> Dropout simula el promedio de m√∫ltiples redes diferentes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 33: SEGUNDA CONVOLUCI√ìN -->
        <div class="slide hidden" data-slide="33">
            <h2>26. Segunda Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_2 (32 filtros, kernel=3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 64)</p>
                        <p><strong>Salida:</strong> (None, 3, 32)</p>
                        <p><strong>Neuronas activadas:</strong> 96 (3 √ó 32)</p>
                        <p><strong>Par√°metros:</strong> 6,176 (6,144 pesos + 32 bias)</p>
                        <p><strong>Activaci√≥n:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¬øQu√© patrones detecta la segunda convoluci√≥n?</h3>
            <ul>
                <li><strong>32 filtros especializados:</strong> Cada uno aprende combinaciones complejas de caracter√≠sticas LDA</li>
                <li><strong>96 neuronas finales:</strong> 3 posiciones √ó 32 filtros = 96 activaciones por muestra</li>
                <li><strong>Patrones de orden superior:</strong> Detecta relaciones entre las salidas de la primera capa</li>
                <li><strong>Compresi√≥n inteligente:</strong> Reduce de 64 a 32 canales manteniendo informaci√≥n cr√≠tica</li>
                <li><strong>Representaciones abstractas:</strong> Caracter√≠sticas m√°s alejadas de los componentes LDA originales</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Convoluci√≥n Segunda Capa</span>
                <p>Para cada posici√≥n $i$ y filtro $f$ (de 32 filtros):</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{2} \sum_{c=0}^{63} w_{j,c}^f \cdot x_{i+j,c} + b^f\right)$</p>
                <ul>
                    <li>$w_{j,c}^f$: Peso del filtro $f$ en posici√≥n $j$ y canal $c$</li>
                    <li>$x_{i+j,c}$: Salida de la primera capa en posici√≥n $i+j$, canal $c$</li>
                    <li>$b^f$: Bias del filtro $f$</li>
                    <li>Kernel size = 3: Analiza 3 posiciones consecutivas</li>
                    <li>64 canales de entrada: Toda la informaci√≥n de Conv1D_1</li>
                </ul>
            </div>

            <h3>C√°lculo Detallado de Par√°metros:</h3>
            <ul>
                <li><strong>Pesos de convoluci√≥n:</strong> 3 (kernel) √ó 64 (canales entrada) √ó 32 (filtros) = 6,144</li>
                <li><strong>Biases:</strong> 32 (uno por filtro)</li>
                <li><strong>Total:</strong> 6,144 + 32 = 6,176 par√°metros</li>
                <li><strong>Porcentaje del modelo:</strong> 78.1% de todos los par√°metros entrenables</li>
                <li><strong>192 ‚Üí 96 neuronas:</strong> Reducci√≥n del 50% en activaciones</li>
            </ul>

            <h3>Especializaci√≥n en Caracter√≠sticas Emocionales:</h3>
            <ul>
                <li><strong>Detectores de valencias:</strong> Algunos filtros se especializan en emociones positivas/negativas</li>
                <li><strong>Detectores de activaci√≥n:</strong> Otros en emociones alta/baja energ√≠a</li>
                <li><strong>Combinaciones complejas:</strong> Patrones que no son evidentes en componentes LDA individuales</li>
                <li><strong>Invarianzas aprendidas:</strong> Robustez a variaciones espec√≠ficas del hablante</li>
            </ul>

            <h3>Reducci√≥n de Dimensionalidad Inteligente:</h3>
            <p>La reducci√≥n de 64 a 32 filtros no es p√©rdida de informaci√≥n, sino <strong>compresi√≥n inteligente</strong> donde cada filtro de salida capture patrones m√°s complejos y espec√≠ficos para la tarea de clasificaci√≥n emocional.</p>
            <footer class="footnote">
                <strong>Neuronas activadas:</strong> Cantidad total de valores de salida (posiciones √ó filtros). <strong>Orden superior:</strong> Patrones m√°s complejos formados por combinaciones de caracter√≠sticas m√°s simples. <strong>Valencias:</strong> Dimensi√≥n emocional que va de negativa (tristeza) a positiva (alegr√≠a). <strong>Activaci√≥n emocional:</strong> Intensidad o energ√≠a de una emoci√≥n (calma vs excitaci√≥n). <strong>Invarianzas:</strong> Propiedades que permanecen constantes ante ciertas transformaciones.
            </footer>
        </div>

        <!-- DIAPOSITIVA 34: BATCH NORMALIZATION 2 -->
        <div class="slide hidden" data-slide="34">
            <h2>27. Segunda Capa de Normalizaci√≥n por Lotes</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm">
                    <h4>BatchNormalization_2</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 32)</p>
                        <p><strong>Salida:</strong> (None, 3, 32)</p>
                        <p><strong>Neuronas normalizadas:</strong> 96 (3 √ó 32)</p>
                        <p><strong>Par√°metros:</strong> 128 (64 Œ≥ + 64 Œ≤ + 64 Œº + 64 œÉ¬≤)</p>
                        <p><strong>Funci√≥n:</strong> Normalizar activaciones de segunda convoluci√≥n</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© normalizar despu√©s de la segunda convoluci√≥n?</h3>
            <ul>
                <li><strong>Estabilizaci√≥n profunda:</strong> Las capas m√°s profundas necesitan m√°s normalizaci√≥n</li>
                <li><strong>32 canales especializados:</strong> Cada canal representa patrones emocionales complejos</li>
                <li><strong>96 neuronas estabilizadas:</strong> Todas las activaciones (3√ó32) son normalizadas</li>
                <li><strong>Preparaci√≥n para pooling:</strong> Activaciones normalizadas mejoran el max pooling posterior</li>
                <li><strong>Convergencia acelerada:</strong> Gradientes m√°s estables en capas profundas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Normalizaci√≥n de Caracter√≠sticas de Alto Nivel</span>
                <p>Para cada uno de los 32 canales especializados:</p>
                <p>$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c^{(batch)}}{\sqrt{(\sigma_c^{(batch)})^2 + \epsilon}}$</p>
                <p>$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$</p>
                <ul>
                    <li>$\mu_c^{(batch)}$: Media del canal $c$ en el batch actual</li>
                    <li>$(\sigma_c^{(batch)})^2$: Varianza del canal $c$ en el batch</li>
                    <li>$\gamma_c, \beta_c$: Par√°metros aprendibles espec√≠ficos por canal (32 cada uno)</li>
                    <li>Cada canal representa detectores emocionales especializados</li>
                </ul>
            </div>

            <h3>Estad√≠sticas de Entrenamiento vs Inferencia:</h3>
            <ul>
                <li><strong>Entrenamiento:</strong> Usa estad√≠sticas del batch actual (32 muestras)</li>
                <li><strong>Moving averages:</strong> Actualiza medias y varianzas acumulativas con momentum=0.99</li>
                <li><strong>Inferencia:</strong> Usa estad√≠sticas acumuladas durante todo el entrenamiento</li>
                <li><strong>Estabilidad:</strong> Comportamiento consistente independiente del tama√±o de batch</li>
                <li><strong>96 valores normalizados:</strong> Cada activaci√≥n es procesada individualmente</li>
            </ul>

            <h3>Impacto en Caracter√≠sticas Emocionales:</h3>
            <ul>
                <li><strong>Calibraci√≥n de intensidad:</strong> Normaliza la "fuerza" de diferentes detectores emocionales</li>
                <li><strong>Comparabilidad entre filtros:</strong> Cada canal tiene distribuci√≥n similar</li>
                <li><strong>Reducci√≥n de sesgo:</strong> Evita que ciertos filtros dominen por escala, no por relevancia</li>
                <li><strong>Robustez inter-hablante:</strong> Compensa variaciones individuales en caracter√≠sticas vocales</li>
            </ul>

            <h3>Par√°metros Aprendibles Espec√≠ficos:</h3>
            <ul>
                <li><strong>32 par√°metros Œ≥:</strong> Controlan la escala √≥ptima para cada detector emocional</li>
                <li><strong>32 par√°metros Œ≤:</strong> Ajustan el offset para cada tipo de patr√≥n emocional</li>
                <li><strong>Especializaci√≥n autom√°tica:</strong> El modelo aprende qu√© normalizaci√≥n es √≥ptima por canal</li>
                <li><strong>Flexibilidad adaptativa:</strong> Puede "desactivar" la normalizaci√≥n si no es √∫til</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas normalizadas:</strong> Cantidad total de activaciones que la capa procesa para normalizaci√≥n. <strong>Moving averages:</strong> Promedio m√≥vil que actualiza estad√≠sticas gradualmente con cada batch. <strong>Momentum:</strong> Factor (0.99) que controla qu√© tan r√°pido se actualizan las estad√≠sticas acumuladas. <strong>Offset:</strong> Desplazamiento que permite ajustar el punto central de la distribuci√≥n. <strong>Calibraci√≥n:</strong> Proceso de ajustar escalas para que sean comparables.
            </footer>
        </div>

        <!-- DIAPOSITIVA 35: MAX POOLING 2 -->
        <div class="slide hidden" data-slide="35">
            <h2>28. Segunda Capa de Max Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>MaxPooling1D_2 (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 3, 32)</p>
                        <p><strong>Salida:</strong> (None, 1, 32)</p>
                        <p><strong>Neuronas resultantes:</strong> 32 (1 √ó 32)</p>
                        <p><strong>Par√°metros:</strong> 0 (operaci√≥n determin√≠stica)</p>
                        <p><strong>Funci√≥n:</strong> Compresi√≥n final hacia representaci√≥n global</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© comprimir a una sola posici√≥n temporal?</h3>
            <ul>
                <li><strong>Agregaci√≥n global:</strong> Condensa informaci√≥n de todos los componentes LDA en un vector</li>
                <li><strong>32 neuronas finales:</strong> Una representaci√≥n ultra-compacta del audio completo</li>
                <li><strong>Preparaci√≥n para clasificaci√≥n:</strong> Transici√≥n hacia capas densas finales</li>
                <li><strong>Invarianza posicional completa:</strong> El orden de componentes LDA se vuelve irrelevante</li>
                <li><strong>M√°xima compresi√≥n:</strong> De 3 posiciones temporales a 1 representaci√≥n global</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Agregaci√≥n Final de Caracter√≠sticas LDA</span>
                <p>Para cada canal de detecci√≥n emocional $c$ (de 32):</p>
                <p>$y_c = \max(x_{0,c}, x_{1,c}, x_{2,c})$</p>
                <p>Donde las 3 posiciones representan:</p>
                <ul>
                    <li>$x_{0,c}$: Patrones de componentes LDA 1-2</li>
                    <li>$x_{1,c}$: Patrones de componentes LDA 3-4</li>
                    <li>$x_{2,c}$: Patrones de componentes LDA 5-6</li>
                    <li>$y_c$: Activaci√≥n m√°xima (m√°s fuerte) del detector $c$</li>
                </ul>
            </div>

            <h3>Transici√≥n Cr√≠tica en la Arquitectura:</h3>
            <ul>
                <li><strong>De secuencial a global:</strong> Ya no importa la posici√≥n espec√≠fica en la secuencia LDA</li>
                <li><strong>Vector de caracter√≠sticas finales:</strong> 32 valores que resumen todo el audio</li>
                <li><strong>Preparaci√≥n para clasificaci√≥n:</strong> Formato compatible con capas densas</li>
                <li><strong>Compresi√≥n m√°xima preservando informaci√≥n:</strong> Cada canal mantiene su activaci√≥n m√°s fuerte</li>
                <li><strong>96 ‚Üí 32 neuronas:</strong> Reducci√≥n del 66.7%</li>
            </ul>

            <h3>

        <!-- Contin√∫a con las diapositivas 28-41 (arquitectura CNN) en orden correcto -->
        <!-- [Diapositivas 28-41 se mantienen igual que en tu documento original] -->
<!-- DIAPOSITIVA 36: DROPOUT 2 -->
        <div class="slide hidden" data-slide="36">
            <h2>29. Segunda Capa de Dropout</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_2 (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 1, 32)</p>
                        <p><strong>Salida:</strong> (None, 1, 32)</p>
                        <p><strong>Neuronas regularizadas:</strong> 32 (1 √ó 32)</p>
                        <p><strong>Par√°metros:</strong> 0 (m√°scara estoc√°stica)</p>
                        <p><strong>Funci√≥n:</strong> Regularizaci√≥n antes de capas densas finales</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© aplicar Dropout antes de las capas densas?</h3>
            <ul>
                <li><strong>Momento cr√≠tico:</strong> Justo antes de la clasificaci√≥n final es donde m√°s overfitting puede ocurrir</li>
                <li><strong>32 caracter√≠sticas concentradas:</strong> Cada una altamente informativa y propensa a memorizaci√≥n</li>
                <li><strong>~22 neuronas activas:</strong> En promedio 32√ó0.7 = 22 neuronas activas por forward pass</li>
                <li><strong>Preparaci√≥n para generalizaci√≥n:</strong> El modelo debe funcionar con subconjuntos de caracter√≠sticas</li>
                <li><strong>Calibraci√≥n de confianza:</strong> Mejora la estimaci√≥n de incertidumbre en predicciones</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Dropout en Caracter√≠sticas Concentradas</span>
                <p><strong>Durante entrenamiento (cada forward pass):</strong></p>
                <p>Para cada caracter√≠stica $i$ de las 32:</p>
                <p>$m_i \sim \text{Bernoulli}(0.7)$</p>
                <p>$y_i = \frac{m_i \cdot x_i}{0.7}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_i = x_i$</p>
                <ul>
                    <li>$m_i$: M√°scara binaria (70% prob. de ser 1)</li>
                    <li>30% de caracter√≠sticas se anulan aleatoriamente</li>
                    <li>Factor $\frac{1}{0.7}$: Compensa caracter√≠sticas perdidas</li>
                    <li>Cada caracter√≠stica representa un detector emocional espec√≠fico</li>
                </ul>
            </div>

            <h3>Impacto en Detectores Emocionales:</h3>
            <ul>
                <li><strong>Robustez de detectores:</strong> El modelo aprende a no depender de un solo detector</li>
                <li><strong>Redundancia funcional:</strong> M√∫ltiples detectores pueden se√±alar la misma emoci√≥n</li>
                <li><strong>Combinaciones variables:</strong> En cada entrenamiento usa subconjuntos diferentes de los 32 detectores</li>
                <li><strong>Generalizaci√≥n mejorada:</strong> Reduce sobreajuste a patrones espec√≠ficos del conjunto de entrenamiento</li>
            </ul>

            <h3>Estrategia de Regularizaci√≥n en Cascada:</h3>
            <ul>
                <li><strong>Dropout_1 (post-Conv1D_1):</strong> Regulariza caracter√≠sticas de bajo nivel (192 neuronas)</li>
                <li><strong>Dropout_2 (pre-Dense):</strong> Regulariza caracter√≠sticas de alto nivel concentradas (32 neuronas)</li>
                <li><strong>Sinergia de regularizaci√≥n:</strong> Dos puntos estrat√©gicos de control de overfitting</li>
                <li><strong>Rate constante (0.3):</strong> Balance probado entre regularizaci√≥n y preservaci√≥n de informaci√≥n</li>
            </ul>

            <h3>Preparaci√≥n √ìptima para Clasificaci√≥n:</h3>
            <ul>
                <li><strong>Vector robusto:</strong> Las caracter√≠sticas que sobreviven son m√°s confiables</li>
                <li><strong>Incertidumbre controlada:</strong> El modelo "sabe" cu√°ndo no est√° seguro</li>
                <li><strong>Ensemble impl√≠cito:</strong> Simula el promedio de 2^32 redes diferentes</li>
                <li><strong>Calibraci√≥n de probabilidades:</strong> Las probabilidades de softmax son m√°s realistas</li>
            </ul>

            <h3>Diferencia con Dropout_1:</h3>
            <p>Mientras Dropout_1 opera sobre 192 activaciones de nivel medio, Dropout_2 opera sobre 32 caracter√≠sticas altamente procesadas y espec√≠ficas para emociones, haciendo que cada desactivaci√≥n tenga mayor impacto en el aprendizaje.</p>
            <footer class="footnote">
                <strong>Neuronas regularizadas:</strong> Cantidad total de activaciones sujetas a desactivaci√≥n aleatoria. <strong>Estoc√°stica:</strong> Proceso que involucra aleatoriedad controlada. <strong>Redundancia funcional:</strong> M√∫ltiples elementos que pueden realizar la misma funci√≥n. <strong>Ensemble:</strong> Combinaci√≥n de m√∫ltiples modelos para mejorar predicciones. <strong>Calibraci√≥n de probabilidades:</strong> Ajuste para que las probabilidades del modelo reflejen la confianza real.
            </footer>
        </div>

        <!-- DIAPOSITIVA 37: GLOBAL AVERAGE POOLING -->
        <div class="slide hidden" data-slide="37">
            <h2>30. Capa de Global Average Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pooling">
                    <h4>GlobalAveragePooling1D</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 1, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas aplanadas:</strong> 32</p>
                        <p><strong>Par√°metros:</strong> 0 (operaci√≥n matem√°tica pura)</p>
                        <p><strong>Funci√≥n:</strong> Aplanar manteniendo informaci√≥n de cada canal</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© Global Average Pooling en lugar de Flatten?</h3>
            <ul>
                <li><strong>Reducci√≥n de par√°metros:</strong> Evita conexiones densas masivas</li>
                <li><strong>Regularizaci√≥n impl√≠cita:</strong> Menos par√°metros = menos overfitting</li>
                <li><strong>32 valores preservados:</strong> Cada canal mantiene su significado como detector emocional</li>
                <li><strong>Interpretabilidad:</strong> Cada canal mantiene su significado como detector emocional</li>
                <li><strong>Traducci√≥n directa:</strong> De representaci√≥n 2D a vector de caracter√≠sticas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n Global Average Pooling</span>
                <p>Para cada canal/detector emocional $c$ (de 32):</p>
                <p>$y_c = \frac{1}{T} \sum_{t=0}^{T-1} x_{t,c}$</p>
                <p>En nuestro caso espec√≠fico (T=1):</p>
                <p>$y_c = x_{0,c}$</p>
                <ul>
                    <li>$T = 1$: Solo una posici√≥n temporal tras MaxPooling_2</li>
                    <li>$x_{0,c}$: Activaci√≥n del canal $c$ en la √∫nica posici√≥n</li>
                    <li>$y_c$: Valor final del detector emocional $c$ (32 valores totales)</li>
                    <li>Operaci√≥n equivalente a un reshape: (None, 1, 32) ‚Üí (None, 32)</li>
                </ul>
            </div>

            <h3>Ventajas sobre Alternativas:</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Global Average Pooling</h4>
                    <ul>
                        <li><strong>0 par√°metros adicionales:</strong> No aumenta complejidad</li>
                        <li><strong>Preserva sem√°ntica:</strong> Cada valor mantiene significado de detector</li>
                        <li><strong>Regularizaci√≥n natural:</strong> Promedio es m√°s robusto que valor √∫nico</li>
                        <li><strong>Escalabilidad:</strong> Funciona independiente del tama√±o de entrada</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>‚ùå Flatten tradicional</h4>
                    <ul>
                        <li><strong>Mismo resultado aqu√≠:</strong> Con T=1 es equivalente</li>
                        <li><strong>Menos sem√°ntico:</strong> Perder√≠a interpretaci√≥n de canales</li>
                        <li><strong>Mayor acoplamiento:</strong> M√°s dependiente de arquitectura espec√≠fica</li>
                        <li><strong>Menos flexible:</strong> Cambios en capas previas afectan m√°s</li>
                    </ul>
                </div>
            </div>

            <h3>Transici√≥n a Representaci√≥n Final:</h3>
            <ul>
                <li><strong>Vector de 32 caracter√≠sticas:</strong> Cada una es un detector emocional especializado</li>
                <li><strong>Preparaci√≥n para Dense:</strong> Formato est√°ndar para capas totalmente conectadas</li>
                <li><strong>Compresi√≥n extrema:</strong> De 180 caracter√≠sticas originales a 32 finales</li>
                <li><strong>Informaci√≥n concentrada:</strong> Cada valor representa patrones emocionales complejos</li>
            </ul>

            <h3>Interpretaci√≥n de las 32 Caracter√≠sticas:</h3>
            <ul>
                <li><strong>Detectores de valencia:</strong> Caracter√≠sticas que distinguen emociones positivas/negativas</li>
                <li><strong>Detectores de activaci√≥n:</strong> Caracter√≠sticas que miden intensidad emocional</li>
                <li><strong>Detectores espec√≠ficos:</strong> Cada uno especializado en patrones de emociones particulares</li>
                <li><strong>Detectores contextuales:</strong> Patrones que consideran combinaciones emocionales</li>
            </ul>

            <h3>Preparaci√≥n para Clasificaci√≥n Final:</h3>
            <p>Estas 32 caracter√≠sticas representan la <strong>esencia emocional comprimida</strong> del audio original, extra√≠da a trav√©s del pipeline LDA‚ÜíConv1D‚ÜíPooling. Cada valor en este vector es el resultado de procesar 180 caracter√≠sticas originales a trav√©s de m√∫ltiples capas de abstracci√≥n.</p>
            <footer class="footnote">
                <strong>Neuronas aplanadas:</strong> Cantidad de valores en el vector final tras el aplanamiento. <strong>Aplanar (Flatten):</strong> Convertir tensor multidimensional en vector unidimensional. <strong>Sem√°ntica:</strong> Significado o interpretaci√≥n de cada elemento en el contexto del problema. <strong>Acoplamiento:</strong> Grado de dependencia entre componentes del sistema. <strong>Esencia comprimida:</strong> Representaci√≥n minimal que captura la informaci√≥n m√°s importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 38: DENSE INTERMEDIA -->
        <div class="slide hidden" data-slide="38">
            <h2>31. Capa Densa Intermedia</h2>
            <div class="neural-architecture">
                <div class="layer-box dense">
                    <h4>Dense_Intermediate (32 neuronas)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas totales:</strong> 32</p>
                        <p><strong>Par√°metros:</strong> 1,056 (1,024 pesos + 32 bias)</p>
                        <p><strong>Activaci√≥n:</strong> ReLU</p>
                        <p><strong>Funci√≥n:</strong> Refinamiento y combinaci√≥n de detectores emocionales</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© una capa densa con la misma dimensi√≥n?</h3>
            <ul>
                <li><strong>Refinamiento de caracter√≠sticas:</strong> Permite interacciones no lineales entre detectores</li>
                <li><strong>32 neuronas refinadas:</strong> Cada neurona puede combinar informaci√≥n de las 32 caracter√≠sticas de entrada</li>
                <li><strong>Combinaciones emocionales:</strong> Aprende relaciones complejas entre patrones emocionales</li>
                <li><strong>Preparaci√≥n para decisi√≥n:</strong> √öltima oportunidad de procesamiento antes de clasificaci√≥n</li>
                <li><strong>Capacidad computacional:</strong> 1,056 par√°metros para aprender interacciones espec√≠ficas</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Transformaci√≥n Densa con Interacciones Completas</span>
                <p>Para cada neurona de salida $j$ (de 32):</p>
                <p>$z_j = \sum_{i=0}^{31} w_{i,j} \cdot x_i + b_j$</p>
                <p>$y_j = \max(0, z_j)$ (ReLU)</p>
                <ul>
                    <li>$w_{i,j}$: Peso de conexi√≥n desde detector $i$ a neurona $j$</li>
                    <li>$x_i$: Activaci√≥n del detector emocional $i$</li>
                    <li>$b_j$: Bias de la neurona $j$</li>
                    <li>32√ó32 = 1,024 pesos + 32 biases = 1,056 par√°metros</li>
                </ul>
            </div>

            <h3>C√°lculo Detallado de Par√°metros:</h3>
            <ul>
                <li><strong>Matriz de pesos:</strong> 32 (entrada) √ó 32 (salida) = 1,024 conexiones</li>
                <li><strong>Vector de biases:</strong> 32 valores (uno por neurona de salida)</li>
                <li><strong>Total:</strong> 1,024 + 32 = 1,056 par√°metros</li>
                <li><strong>Porcentaje del modelo:</strong> 13.3% de par√°metros entrenables</li>
                <li><strong>Conexiones por neurona:</strong> Cada neurona de salida recibe 32 conexiones</li>
            </ul>

            <h3>Tipos de Interacciones Aprendidas:</h3>
            <ul>
                <li><strong>Inhibici√≥n competitiva:</strong> Cuando un detector de alegr√≠a se activa, suprime detectores de tristeza</li>
                <li><strong>Reforzamiento sin√©rgico:</strong> Detectores complementarios se refuerzan mutuamente</li>
                <li><strong>Contextualizaci√≥n:</strong> Un detector puede cambiar interpretaci√≥n seg√∫n otros activos</li>
                <li><strong>Calibraci√≥n de intensidad:</strong> Ajuste de la "fuerza" relativa entre diferentes detectores</li>
            </ul>

            <h3>Especializaci√≥n en Patrones Emocionales:</h3>
            <ul>
                <li><strong>Neuronas de valencia:</strong> Algunas se especializan en balance positivo/negativo</li>
                <li><strong>Neuronas de activaci√≥n:</strong> Otras en distinguir alta/baja energ√≠a emocional</li>
                <li><strong>Neuronas de categor√≠a:</strong> Especializadas en emociones espec√≠ficas (alegr√≠a, tristeza, etc.)</li>
                <li><strong>Neuronas de contexto:</strong> Consideran combinaciones y matices emocionales</li>
            </ul>

            <h3>Ventajas de ReLU en este Contexto:</h3>
            <ul>
                <li><strong>Selectividad:</strong> Solo activa cuando la combinaci√≥n de detectores es significativa</li>
                <li><strong>Sparsity:</strong> Muchas neuronas permanecen inactivas, creando representaciones eficientes</li>
                <li><strong>Interpretabilidad:</strong> Activaciones positivas indican presencia de patrones espec√≠ficos</li>
                <li><strong>Eficiencia computacional:</strong> Gradientes simples y c√°lculo r√°pido</li>
            </ul>

            <h3>Preparaci√≥n para Clasificaci√≥n Final:</h3>
            <p>Esta capa transforma los 32 detectores emocionales "crudos" en 32 caracter√≠sticas <strong>refinadas y contextualmente conscientes</strong>, donde cada neurona de salida considera la activaci√≥n de todos los detectores de entrada para tomar decisiones m√°s informadas.</p>
            <footer class="footnote">
                <strong>Neuronas totales:</strong> Cantidad de unidades de procesamiento en la capa. <strong>Interacciones no lineales:</strong> Relaciones complejas entre variables que no son simples sumas o productos. <strong>Inhibici√≥n competitiva:</strong> Proceso donde la activaci√≥n de una neurona reduce la activaci√≥n de otras. <strong>Sparsity:</strong> Propiedad donde muchos valores son cero, creando representaciones eficientes. <strong>Contextualmente conscientes:</strong> Caracter√≠sticas que consideran el contexto completo para su interpretaci√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 39: DROPOUT FINAL -->
        <div class="slide hidden" data-slide="39">
            <h2>32. Capa de Dropout Final</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout_Final (rate=0.15)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 32)</p>
                        <p><strong>Neuronas afectadas:</strong> 32</p>
                        <p><strong>Par√°metros:</strong> 0 (regularizaci√≥n estoc√°stica)</p>
                        <p><strong>Funci√≥n:</strong> Regularizaci√≥n final antes de clasificaci√≥n</p>
                    </div>
                </div>
            </div>
            <h3>¬øPor qu√© un dropout rate m√°s bajo (0.15) antes de la salida?</h3>
            <ul>
                <li><strong>Caracter√≠sticas altamente procesadas:</strong> Las 32 neuronas son resultado de m√∫ltiples transformaciones</li>
                <li><strong>~27 neuronas activas:</strong> En promedio 32√ó0.85 = 27 neuronas activas por forward pass</li>
                <li><strong>Regularizaci√≥n sutil:</strong> Menor agresividad para preservar informaci√≥n cr√≠tica</li>
                <li><strong>Preparaci√≥n delicada:</strong> Justo antes de la decisi√≥n final necesita menos perturbaci√≥n</li>
                <li><strong>Balance fino:</strong> Suficiente para prevenir overfitting sin perder capacidad discriminativa</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Dropout Conservador Pre-Clasificaci√≥n</span>
                <p><strong>Durante entrenamiento:</strong></p>
                <p>Para cada caracter√≠stica refinada $i$ (de 32):</p>
                <p>$m_i \sim \text{Bernoulli}(0.85)$</p>
                <p>$y_i = \frac{m_i \cdot x_i}{0.85}$</p>
                <p><strong>Durante inferencia:</strong></p>
                <p>$y_i = x_i$</p>
                <ul>
                    <li>$m_i$: M√°scara con 85% probabilidad de ser 1</li>
                    <li>Solo 15% de caracter√≠sticas se anulan</li>
                    <li>Factor $\frac{1}{0.85}$: Escalado compensatorio m√≠nimo</li>
                    <li>Preservaci√≥n m√°xima de informaci√≥n procesada</li>
                </ul>
            </div>

            <h3>Comparaci√≥n de Tasas de Dropout en el Modelo:</h3>
            <ul>
                <li><strong>Dropout_1:</strong> 30% - Regularizaci√≥n agresiva en caracter√≠sticas de nivel medio (192 neuronas)</li>
                <li><strong>Dropout_2:</strong> 30% - Control de overfitting en caracter√≠sticas concentradas (32 neuronas)</li>
                <li><strong>Dropout_Final:</strong> 15% - Regularizaci√≥n conservadora en caracter√≠sticas refinadas (32 neuronas)</li>
                <li><strong>Estrategia descendente:</strong> Menos agresividad conforme se acerca a la decisi√≥n final</li>
            </ul>

            <h3>Impacto en la Robustez del Modelo:</h3>
            <ul>
                <li><strong>√öltimo filtro de ruido:</strong> Elimina dependencias residuales no esenciales</li>
                <li><strong>Confianza calibrada:</strong> El modelo debe funcionar con 85% de informaci√≥n disponible</li>
                <li><strong>Ensemble final:</strong> Simula m√∫ltiples modelos levemente diferentes</li>
                <li><strong>Generalizaci√≥n de √∫ltimo nivel:</strong> Previene memorizaci√≥n de patrones espec√≠ficos</li>
            </ul>

            <h3>Preparaci√≥n para Softmax:</h3>
            <ul>
                <li><strong>Vector estable:</strong> Caracter√≠sticas robustas que no dependen de elementos √∫nicos</li>
                <li><strong>Distribuci√≥n mejorada:</strong> Reduce picos artificiales en activaciones</li>
                <li><strong>Probabilidades realistas:</strong> El softmax recibe informaci√≥n m√°s balanceada</li>
                <li><strong>Incertidumbre apropiada:</strong> El modelo "sabe" cu√°ndo no est√° completamente seguro</li>
            </ul>

            <h3>Filosof√≠a de Regularizaci√≥n Final:</h3>
            <p>Con rate=0.15, esta capa implementa el principio de <strong>"regularizaci√≥n m√≠nima efectiva"</strong>: suficiente perturbaci√≥n para mejorar generalizaci√≥n, pero no tanta como para interferir con la capacidad discriminativa final del modelo.</p>

            <h3>Transici√≥n Cr√≠tica:</h3>
            <p>Este dropout final marca la transici√≥n desde el <strong>procesamiento de caracter√≠sticas</strong> hacia la <strong>toma de decisi√≥n</strong>. Las 32 caracter√≠sticas que sobreviven al dropout representan la informaci√≥n m√°s confiable y robusta para la clasificaci√≥n emocional.</p>
            <footer class="footnote">
                <strong>Neuronas afectadas:</strong> Cantidad total de unidades sujetas a desactivaci√≥n aleatoria. <strong>Regularizaci√≥n conservadora:</strong> Aplicaci√≥n suave de t√©cnicas de regularizaci√≥n para preservar informaci√≥n importante. <strong>Capacidad discriminativa:</strong> Habilidad del modelo para distinguir entre diferentes clases. <strong>Ensemble final:</strong> Efecto de promediar m√∫ltiples versiones ligeramente diferentes del modelo. <strong>Distribuci√≥n balanceada:</strong> Caracter√≠sticas con varianza y magnitud similares.
            </footer>
        </div>

        <!-- DIAPOSITIVA 40: DENSE OUTPUT -->
        <div class="slide hidden" data-slide="40">
            <h2>33. Capa de Salida (Clasificaci√≥n Final)</h2>
            <div class="neural-architecture">
                <div class="layer-box output">
                    <h4>Dense_Output (7 neuronas, Softmax)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 32)</p>
                        <p><strong>Salida:</strong> (None, 7)</p>
                        <p><strong>Neuronas de salida:</strong> 7 (una por emoci√≥n)</p>
                        <p><strong>Par√°metros:</strong> 231 (224 pesos + 7 bias)</p>
                        <p><strong>Activaci√≥n:</strong> Softmax</p>
                        <p><strong>Funci√≥n:</strong> Clasificaci√≥n probabil√≠stica de emociones</p>
                    </div>
                </div>
            </div>
            <h3>¬øC√≥mo transforma 32 caracter√≠sticas en 7 probabilidades emocionales?</h3>
            <ul>
                <li><strong>Mapeo completo:</strong> Cada caracter√≠stica se conecta a cada emoci√≥n con peso espec√≠fico</li>
                <li><strong>7 neuronas especializadas:</strong> Una neurona por cada emoci√≥n (angry, disgust, fearful, happy, neutral, sad, surprised)</li>
                <li><strong>Especializaci√≥n emocional:</strong> Aprende qu√© caracter√≠sticas son importantes para cada emoci√≥n</li>
                <li><strong>Competencia probabil√≠stica:</strong> Softmax asegura que las probabilidades sumen 1.0</li>
                <li><strong>Decisi√≥n final:</strong> Transforma representaciones num√©ricas en clasificaci√≥n interpretable</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Clasificaci√≥n Softmax Multi-Emocional</span>
                <p><strong>Paso 1 - Logits por emoci√≥n:</strong></p>
                <p>$z_j = \sum_{i=0}^{31} w_{i,j} \cdot x_i + b_j$ para $j \in \{0,1,2,3,4,5,6\}$</p>
                <p><strong>Paso 2 - Probabilidades normalizadas:</strong></p>
                <p>$P(\text{emoci√≥n}_j | \text{audio}) = \frac{e^{z_j}}{\sum_{k=0}^{6} e^{z_k}}$</p>
                <ul>
                    <li>$w_{i,j}$: Peso de caracter√≠stica $i$ para emoci√≥n $j$</li>
                    <li>$z_j$: Logit (puntuaci√≥n) para emoci√≥n $j$</li>
                    <li>$P(\text{emoci√≥n}_j)$: Probabilidad final de emoci√≥n $j$</li>
                    <li>$\sum_{j=0}^{6} P(\text{emoci√≥n}_j) = 1.0$</li>
                </ul>
            </div>

            <h3>Mapeo de √çndices a Emociones:</h3>
            <ul>
                <li><strong>Neurona 0:</strong> Angry (Enojo) - Activaci√≥n alta, valencia negativa</li>
                <li><strong>Neurona 1:</strong> Disgust (Disgusto) - Valencia negativa, respuesta de rechazo</li>
                <li><strong>Neurona 2:</strong> Fearful (Miedo) - Activaci√≥n alta, valencia negativa, alerta</li>
                <li><strong>Neurona 3:</strong> Happy (Alegr√≠a) - Activaci√≥n alta, valencia positiva</li>
                <li><strong>Neurona 4:</strong> Neutral (Neutral) - Activaci√≥n baja, valencia neutra</li>
                <li><strong>Neurona 5:</strong> Sad (Tristeza) - Activaci√≥n baja, valencia negativa</li>
                <li><strong>Neurona 6:</strong> Surprised (Sorpresa) - Activaci√≥n muy alta, valencia neutra</li>
            </ul>

            <h3>C√°lculo de Par√°metros de Clasificaci√≥n:</h3>
            <ul>
                <li><strong>Matriz de pesos:</strong> 32 (caracter√≠sticas) √ó 7 (emociones) = 224 conexiones</li>
                <li><strong>Vector de biases:</strong> 7 valores (uno por emoci√≥n)</li>
                <li><strong>Total:</strong> 224 + 7 = 231 par√°metros</li>
                <li><strong>Porcentaje del modelo:</strong> 2.9% de par√°metros entrenables (pero cr√≠ticos)</li>
                <li><strong>Conexiones por emoci√≥n:</strong> Cada neurona de salida recibe 32 conexiones</li>
            </ul>

            <h3>Especializaci√≥n Aprendida por Emoci√≥n:</h3>
            <ul>
                <li><strong>Happy vs Sad:</strong> Pesos opuestos en caracter√≠sticas de valencia</li>
                <li><strong>Angry vs Fearful:</strong> Diferenciaci√≥n en patrones de activaci√≥n y contexto</li>
                <li><strong>Neutral:</strong> Pesos peque√±os en todas las caracter√≠sticas de activaci√≥n emocional</li>
                <li><strong>Surprised:</strong> Pesos altos en detectores de cambio s√∫bito y activaci√≥n</li>
            </ul>

            <h3>Ventajas de Softmax para Emociones:</h3>
            <ul>
                <li><strong>Interpretabilidad:</strong> Probabilidades directamente interpretables</li>
                <li><strong>Competencia natural:</strong> Emociones compiten entre s√≠ de manera realista</li>
                <li><strong>Calibraci√≥n:</strong> Valores altos indican mayor confianza</li>
                <li><strong>Diferenciaci√≥n:</strong> Amplifica diferencias entre logits similares</li>
            </ul>

            <h3>Ejemplo de Salida T√≠pica:</h3>
            <div class="formula-block">
                <span class="formula-title">Predicci√≥n Ejemplo: Audio de Alegr√≠a</span>
                <p>Probabilidades de salida:</p>
                <ul>
                    <li>P(Angry) = 0.02 (2%)</li>
                    <li>P(Disgust) = 0.01 (1%)</li>
                    <li>P(Fearful) = 0.03 (3%)</li>
                    <li><strong>P(Happy) = 0.85 (85%)</strong> ‚Üê Predicci√≥n dominante</li>
                    <li>P(Neutral) = 0.04 (4%)</li>
                    <li>P(Sad) = 0.02 (2%)</li>
                    <li>P(Surprised) = 0.03 (3%)</li>
                </ul>
                <p><strong>Decisi√≥n final:</strong> Happy (85% confianza)</p>
            </div>

            <h3>Conexi√≥n con el Rendimiento Real:</h3>
            <p>Esta capa final es responsable del <strong>80.07% de precisi√≥n</strong> observado en el conjunto de prueba. Su efectividad depende de qu√© tan bien las 32 caracter√≠sticas de entrada capturen los patrones discriminativos extra√≠dos por todo el pipeline LDA‚ÜíCNN.</p>
            <footer class="footnote">
                <strong>Neuronas de salida:</strong> Unidades finales que producen las probabilidades de clasificaci√≥n. <strong>Logits:</strong> Puntuaciones sin normalizar antes de aplicar softmax. <strong>Valencia emocional:</strong> Dimensi√≥n que va de emociones negativas a positivas. <strong>Activaci√≥n emocional:</strong> Intensidad o energ√≠a de la emoci√≥n (calma vs excitaci√≥n). <strong>Calibraci√≥n:</strong> Qu√© tan bien las probabilidades predichas reflejan la confianza real del modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 41: RESUMEN ARQUITECTURA ACTUALIZADA -->
        <div class="slide hidden" data-slide="41">
            <h2>34. Arquitectura Completa: CNN 1D + LDA Optimizada</h2>
            <div class="cnn-details">
                <p>
                    Nuestra CNN 1D procesa eficientemente las 6 caracter√≠sticas LDA a trav√©s de capas especializadas, transformando componentes discriminantes en predicciones emocionales precisas.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa</th>
                            <th>Tipo</th>
                            <th>Neuronas</th>
                            <th>Entrada ‚Üí Salida</th>
                            <th>Par√°metros</th>
                            <th>Funci√≥n Principal</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Input</strong></td>
                            <td>Entrada</td>
                            <td>6</td>
                            <td>(6, 1)</td>
                            <td>0</td>
                            <td>Recibir caracter√≠sticas LDA</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_1</strong></td>
                            <td>Convoluci√≥n</td>
                            <td>384</td>
                            <td>(6, 1) ‚Üí (6, 64)</td>
                            <td>256</td>
                            <td>Detectar patrones entre componentes LDA</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_1</strong></td>
                            <td>Normalizaci√≥n</td>
                            <td>384</td>
                            <td>(6, 64) ‚Üí (6, 64)</td>
                            <td>256</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_1</strong></td>
                            <td>Pooling</td>
                            <td>192</td>
                            <td>(6, 64) ‚Üí (3, 64)</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_1</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>192</td>
                            <td>(3, 64) ‚Üí (3, 64)</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_2</strong></td>
                            <td>Convoluci√≥n</td>
                            <td>96</td>
                            <td>(3, 64) ‚Üí (3, 32)</td>
                            <td>6,176</td>
                            <td>Patrones de orden superior</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_2</strong></td>
                            <td>Normalizaci√≥n</td>
                            <td>96</td>
                            <td>(3, 32) ‚Üí (3, 32)</td>
                            <td>128</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_2</strong></td>
                            <td>Pooling</td>
                            <td>32</td>
                            <td>(3, 32) ‚Üí (1, 32)</td>
                            <td>0</td>
                            <td>Condensar informaci√≥n</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_2</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>32</td>
                            <td>(1, 32) ‚Üí (1, 32)</td>
                            <td>0</td>
                            <td>Regularizaci√≥n adicional</td>
                        </tr>
                        <tr>
                            <td><strong>GlobalAvgPool</strong></td>
                            <td>Pooling Global</td>
                            <td>32</td>
                            <td>(1, 32) ‚Üí (32)</td>
                            <td>0</td>
                            <td>Extraer caracter√≠sticas finales</td>
                        </tr>
                        <tr>
                            <td><strong>Dense_Inter</strong></td>
                            <td>Densa</td>
                            <td>32</td>
                            <td>(32) ‚Üí (32)</td>
                            <td>1,056</td>
                            <td>Procesamiento intermedio</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_Final</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>32</td>
                            <td>(32) ‚Üí (32)</td>
                            <td>0</td>
                            <td>Regularizaci√≥n final</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Clasificaci√≥n</td>
                            <td>7</td>
                            <td>(32) ‚Üí (7)</td>
                            <td>231</td>
                            <td>Predicci√≥n emocional</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="4"><strong>TOTAL</strong></td>
                            <td><strong>7,911</strong></td>
                            <td><strong>Clasificaci√≥n optimizada</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <h3>Progresi√≥n de Neuronas a trav√©s del Modelo:</h3>
            <ul>
                <li><strong>Entrada:</strong> 6 neuronas (componentes LDA)</li>
                <li><strong>Expansi√≥n inicial:</strong> 6 ‚Üí 384 neuronas (Conv1D_1)</li>
                <li><strong>Compresi√≥n gradual:</strong> 384 ‚Üí 192 ‚Üí 96 ‚Üí 32 neuronas</li>
                <li><strong>Clasificaci√≥n final:</strong> 32 ‚Üí 7 neuronas (probabilidades emocionales)</li>
            </ul>
            <h3>Ventajas de la Arquitectura LDA-optimizada:</h3>
            <ul>
                <li><strong>Ultra-eficiente:</strong> 7,911 par√°metros entrenables (vs 47K+ en modelos tradicionales)</li>
                <li><strong>Entrada compacta:</strong> Solo 6 caracter√≠sticas vs 180 originales</li>
                <li><strong>Precisi√≥n mantenida:</strong> 80.07% con recursos m√≠nimos</li>
                <li><strong>Convergencia r√°pida:</strong> Menos par√°metros = entrenamiento m√°s eficiente</li>
            </ul>
            <footer class="footnote">
                <strong>Neuronas:</strong> Unidades de procesamiento activas en cada capa. <strong>Par√°metros entrenables:</strong> Pesos y sesgos que el modelo ajusta durante el entrenamiento para aprender patrones emocionales.
            </footer>
        </div>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>
        <!-- DIAPOSITIVA 42: PROCESO DE ENTRENAMIENTO ACTUALIZADO -->
        <div class="slide hidden" data-slide="42">
            <h2>35. Proceso de Entrenamiento - Resultados Reales</h2>
            <div class="highlight-box">
                <h3>Configuraci√≥n de Entrenamiento</h3>
                <p>El modelo se entren√≥ durante 73 √©pocas (de 100 configuradas) con early stopping autom√°tico al detectar convergencia √≥ptima.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>73</h3><p>√âpocas entrenadas</p></div>
                <div class="stat-item"><h3>32</h3><p>Tama√±o de batch</p></div>
                <div class="stat-item"><h3>4,668</h3><p>Muestras entrenamiento</p></div>
                <div class="stat-item"><h3>1,168</h3><p>Muestras validaci√≥n</p></div>
            </div>

            <h3>Divisi√≥n de Datos (Estratificada)</h3>
            <ul>
                <li><strong>Entrenamiento (64%):</strong> 4,668 muestras para aprender patrones LDA</li>
                <li><strong>Validaci√≥n (16%):</strong> 1,168 muestras para optimizar hiperpar√°metros</li>
                <li><strong>Prueba (20%):</strong> 1,460 muestras para evaluaci√≥n final no sesgada</li>
            </ul>

            <h3>Callbacks Utilizados</h3>
            <div class="methodology-step"><strong>EarlyStopping:</strong> Detuvo entrenamiento en √©poca 73 por convergencia (patience=15)</div>
            <div class="methodology-step"><strong>ReduceLROnPlateau:</strong> Ajust√≥ learning rate din√°micamente (patience=7)</div>
            <div class="methodology-step"><strong>ModelCheckpoint:</strong> Guard√≥ mejor modelo basado en val_accuracy</div>
            
            <h3>Optimizador ADAM (Configuraci√≥n)</h3>
            <ul>
                <li><strong>Learning rate inicial:</strong> 0.001</li>
                <li><strong>Beta 1:</strong> 0.9 (momento primer orden)</li>
                <li><strong>Beta 2:</strong> 0.999 (momento segundo orden)</li>
                <li><strong>Epsilon:</strong> 1e-7 (estabilidad num√©rica)</li>
            </ul>
            <footer class="footnote">
                <strong>Early stopping:</strong> T√©cnica que detiene el entrenamiento cuando no hay mejora para evitar overfitting. <strong>Estratificada:</strong> Divisi√≥n que mantiene la proporci√≥n de cada emoci√≥n en todos los conjuntos. <strong>Callbacks:</strong> Funciones que se ejecutan durante el entrenamiento para monitoreo y control autom√°tico.
            </footer>
        </div>

        <!-- Contin√∫a con las diapositivas 43-61 en orden correcto -->
        <!-- [Resto de las diapositivas se mantienen igual que en tu documento original] -->
<!-- DIAPOSITIVA 43: RESULTADOS FINALES ACTUALIZADOS -->
        <div class="slide hidden" data-slide="43">
            <h2>36. Resultados del Entrenamiento - Rendimiento Real</h2>
            <div class="highlight-box">
                <h3>üéØ Excelente Rendimiento con Recursos M√≠nimos</h3>
                <p>El modelo logr√≥ una precisi√≥n del <strong>80.07%</strong> en el conjunto de prueba, demostrando la efectividad de la combinaci√≥n CNN 1D + LDA.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>80.07%</h3><p>Precisi√≥n en prueba</p></div>
                <div class="stat-item"><h3>0.5305</h3><p>P√©rdida final en prueba</p></div>
                <div class="stat-item"><h3>81.59%</h3><p>Mejor precisi√≥n validaci√≥n</p></div>
                <div class="stat-item"><h3>96.7%</h3><p>Reducci√≥n dimensional</p></div>
            </div>

            <h3>Interpretaci√≥n de Resultados</h3>
            <ul>
                <li><strong>Eficiencia Excepcional:</strong> 80% de precisi√≥n con solo 6 caracter√≠sticas de entrada</li>
                <li><strong>Convergencia Estable:</strong> Early stopping en √©poca 73 indica entrenamiento √≥ptimo</li>
                <li><strong>Sin Overfitting:</strong> Diferencia m√≠nima entre entrenamiento y validaci√≥n</li>
                <li><strong>Robustez LDA:</strong> La reducci√≥n dimensional mantiene informaci√≥n discriminativa</li>
            </ul>

            <h3>Comparaci√≥n con Benchmarks</h3>
            <p>Nuestro resultado de 80.07% es competitivo considerando:</p>
            <ul>
                <li><strong>Ultra-compresi√≥n:</strong> Solo 6 caracter√≠sticas vs 180+ en modelos tradicionales</li>
                <li><strong>Par√°metros m√≠nimos:</strong> 7,911 par√°metros vs 50K+ t√≠picos</li>
                <li><strong>Datasets m√∫ltiples:</strong> Combinaci√≥n de RAVDESS, TESS y MESD</li>
                <li><strong>7 clases emocionales:</strong> Problema multiclase complejo</li>
            </ul>
            <footer class="footnote">
                <strong>Benchmark:</strong> Punto de referencia para comparar el rendimiento del modelo. <strong>Overfitting:</strong> Cuando el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Discriminativa:</strong> Informaci√≥n que ayuda a distinguir entre diferentes clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 44: CURVAS DE ENTRENAMIENTO ACTUALIZADAS -->
        <div class="slide hidden" data-slide="44">
            <h2>37. Curvas de Entrenamiento - Evoluci√≥n Real del Modelo</h2>
            <div class="single-chart-container">
                <h3>Progreso de Entrenamiento: Loss y Accuracy a lo largo de 73 √âpocas</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\training_history.png" alt="Curvas de Entrenamiento Reales" style="max-width: 100%; border-radius: 15px;">
            </div>

            <h3>Interpretaci√≥n de las Curvas Reales</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Indicadores Positivos Observados</h4>
                    <ul>
                        <li><strong>Convergencia Controlada:</strong> P√©rdida disminuye consistentemente hasta la √©poca 73</li>
                        <li><strong>Early Stopping Efectivo:</strong> Entrenamiento se detuvo autom√°ticamente en el punto √≥ptimo</li>
                        <li><strong>Precisi√≥n Creciente:</strong> Mejora progresiva hasta alcanzar 81.59% en validaci√≥n</li>
                        <li><strong>Sin Overfitting:</strong> Curvas de train y validation mantienen tendencias similares</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>üìä Observaciones T√©cnicas</h4>
                    <ul>
                        <li><strong>Estabilizaci√≥n Final:</strong> √öltimas √©pocas muestran convergencia natural</li>
                        <li><strong>Variabilidad Normal:</strong> Fluctuaciones esperadas en validaci√≥n con datasets reales</li>
                        <li><strong>ReduceLR Activado:</strong> Learning rate se redujo autom√°ticamente cuando fue necesario</li>
                        <li><strong>Parada Inteligente:</strong> 73 √©pocas fueron suficientes (vs 100 configuradas)</li>
                    </ul>
                </div>
            </div>

            <h3>Conclusiones del Proceso de Entrenamiento</h3>
            <ul>
                <li><strong>Eficiencia de LDA:</strong> Solo 6 caracter√≠sticas permitieron convergencia r√°pida</li>
                <li><strong>Arquitectura √ìptima:</strong> 7,911 par√°metros suficientes para el problema</li>
                <li><strong>Callbacks Efectivos:</strong> Early stopping y ReduceLR funcionaron correctamente</li>
                <li><strong>Generalizaci√≥n Lograda:</strong> Rendimiento sostenido en validaci√≥n confirma robustez</li>
            </ul>
            <footer class="footnote">
                <strong>Convergencia:</strong> Proceso donde el modelo alcanza un rendimiento estable y √≥ptimo. <strong>ReduceLR:</strong> T√©cnica que disminuye la velocidad de aprendizaje cuando el progreso se estanca. <strong>Generalizaci√≥n:</strong> Capacidad del modelo de funcionar bien con datos no vistos durante entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 45: MATRIZ DE CONFUSI√ìN ACTUALIZADA -->
        <div class="slide hidden" data-slide="45">
            <h2>38. Matriz de Confusi√≥n - An√°lisis Detallado por Emoci√≥n</h2>
            <div class="single-chart-container">
                <h3>Rendimiento Real por Clase Emocional</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\confusion_matrix.png" alt="Matriz de Confusi√≥n Real" style="max-width: 90%; border-radius: 15px;">
            </div>
            
            <h3>Reporte de Clasificaci√≥n (Resultados Reales)</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Emoci√≥n</th>
                        <th>Precisi√≥n</th>
                        <th>Recall</th>
                        <th>F1-Score</th>
                        <th>Soporte</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Angry</strong></td>
                        <td>0.82</td>
                        <td>0.86</td>
                        <td>0.84</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Disgust</strong></td>
                        <td>0.71</td>
                        <td>0.80</td>
                        <td>0.75</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Fearful</strong></td>
                        <td>0.86</td>
                        <td>0.79</td>
                        <td>0.82</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Happy</strong></td>
                        <td>0.82</td>
                        <td>0.82</td>
                        <td>0.82</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Neutral</strong></td>
                        <td>0.87</td>
                        <td>0.83</td>
                        <td>0.85</td>
                        <td>198</td>
                    </tr>
                    <tr>
                        <td><strong>Sad</strong></td>
                        <td>0.88</td>
                        <td>0.79</td>
                        <td>0.84</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Surprised</strong></td>
                        <td>0.47</td>
                        <td>0.52</td>
                        <td>0.49</td>
                        <td>77</td>
                    </tr>
                </tbody>
            </table>

            <h3>An√°lisis de Rendimiento por Emoci√≥n</h3>
            <ul>
                <li><strong>Mejor clasificada:</strong> Sad (F1: 0.84) - Caracter√≠sticas LDA muy distintivas</li>
                <li><strong>M√°s consistente:</strong> Happy (F1: 0.82) - Balance perfecto precisi√≥n/recall</li>
                <li><strong>M√°s desafiante:</strong> Surprised (F1: 0.49) - Menor representaci√≥n en datos (77 vs 237 muestras)</li>
                <li><strong>Neutral destacado:</strong> Mayor precisi√≥n (0.87) - LDA separa efectivamente neutralidad de emociones activas</li>
            </ul>

            <h3>Interpretaci√≥n de Confusiones</h3>
            <p>Las confusiones observadas en la matriz coinciden con estudios psicol√≥gicos: emociones de valencia similar (happy-surprised) o activaci√≥n similar (sad-fearful) tienden a confundirse m√°s, validando la coherencia psicoac√∫stica del modelo.</p>
            <footer class="footnote">
                <strong>Precisi√≥n:</strong> Porcentaje de predicciones correctas para una clase espec√≠fica. <strong>Recall:</strong> Porcentaje de casos reales de una clase que fueron detectados correctamente. <strong>F1-Score:</strong> Media arm√≥nica entre precisi√≥n y recall, balanceando ambas m√©tricas. <strong>Valencia:</strong> Dimensi√≥n emocional que va de negativa a positiva.
            </footer>
        </div>

        <!-- DIAPOSITIVA 46: NUEVA - AN√ÅLISIS DETALLADO DE RENDIMIENTO -->
        <div class="slide hidden" data-slide="46">
            <h2>39. An√°lisis Detallado de Rendimiento</h2>
            <div class="single-chart-container">
                <h3>M√©tricas Avanzadas y Distribuciones de Rendimiento</h3>
                <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\detailed_performance_analysis.png" alt="An√°lisis Detallado de Rendimiento" style="max-width: 100%; border-radius: 15px;">
            </div>
            
            <h3>Interpretaci√≥n de M√©tricas Avanzadas</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>0.80</h3><p>Macro F1-Score</p></div>
                <div class="stat-item"><h3>0.80</h3><p>Weighted F1-Score</p></div>
                <div class="stat-item"><h3>1,170</h3><p>Predicciones correctas</p></div>
                <div class="stat-item"><h3>290</h3><p>Errores totales</p></div>
            </div>

            <h3>Distribuci√≥n de Confianza de Predicciones</h3>
            <ul>
                <li><strong>Predicciones Correctas:</strong> Tienden a tener alta confianza (>0.7), indicando certeza del modelo</li>
                <li><strong>Predicciones Incorrectas:</strong> Muestran menor confianza (<0.6), sugeriendo incertidumbre detectable</li>
                <li><strong>Umbral de Confianza:</strong> 0.65 podr√≠a ser un punto de corte para predicciones confiables</li>
                <li><strong>Calibraci√≥n del Modelo:</strong> La confianza correlaciona bien con la precisi√≥n real</li>
            </ul>

            <h3>An√°lisis por Curvas Precisi√≥n-Recall</h3>
            <p>Las curvas muestran que neutral, sad y angry tienen √°reas bajo la curva (AUC) superiores a 0.85, mientras que surprised presenta desaf√≠os con AUC menor. Esto confirma que algunas emociones son inherentemente m√°s dif√≠ciles de distinguir en el espacio LDA reducido.</p>

            <h3>Recomendaciones Basadas en el An√°lisis</h3>
            <ul>
                <li><strong>Uso con Confianza:</strong> Implementar umbral de confianza en aplicaciones cr√≠ticas</li>
                <li><strong>Datos de Surprised:</strong> Considerar aumentar muestras de esta emoci√≥n en futuras versiones</li>
                <li><strong>Post-procesamiento:</strong> Aplicar suavizado temporal para secuencias de audio largas</li>
            </ul>
            <footer class="footnote">
                <strong>Macro F1:</strong> Promedio simple de F1-scores de todas las clases. <strong>Weighted F1:</strong> Promedio ponderado por el n√∫mero de muestras de cada clase. <strong>AUC:</strong> √Årea bajo la curva, m√©trica de rendimiento de clasificaci√≥n binaria. <strong>Calibraci√≥n:</strong> Qu√© tan bien las probabilidades predichas reflejan la precisi√≥n real.
            </footer>
        </div>

        <!-- DIAPOSITIVA 47: NUEVA - FILTROS CONVOLUCIONALES -->
        <div class="slide hidden" data-slide="47">
            <h2>40. Visualizaci√≥n de Filtros Aprendidos</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Filtros de la Primera Capa Convolucional</h4>
                    <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\conv1d_filters_layer1.png" alt="Filtros Capa 1" style="width:100%; border-radius: 10px; margin: 15px 0;">
                    <p>Estos filtros muestran los patrones que la primera capa convolucional aprendi√≥ a detectar en las 6 caracter√≠sticas LDA. Cada filtro representa un "detector" especializado en reconocer combinaciones espec√≠ficas de componentes discriminantes.</p>
                </div>
                <div class="step-explanation">
                    <h4>Filtros de la Segunda Capa Convolucional</h4>
                    <img src="G:\My Drive\Documents\Inteligencia Artificial\ProyectoIA\Results\Model\conv1d_filters_layer2.png" alt="Filtros Capa 2" style="width:100%; border-radius: 10px; margin: 15px 0;">
                    <p>Los filtros de la segunda capa procesan las salidas de la primera capa, creando detectores de patrones m√°s complejos. Estos representan combinaciones de orden superior entre las caracter√≠sticas discriminantes.</p>
                </div>
            </div>

            <h3>Interpretaci√≥n de los Filtros Aprendidos</h3>
            <ul>
                <li><strong>Diversidad de Patrones:</strong> Cada filtro ha especializado en detectar diferentes aspectos de las emociones</li>
                <li><strong>Adaptaci√≥n a LDA:</strong> Los filtros se ajustaron espec√≠ficamente al espacio discriminante de 6 dimensiones</li>
                <li><strong>Jerarqu√≠a de Caracter√≠sticas:</strong> Primera capa detecta patrones b√°sicos, segunda capa combina en patrones complejos</li>
                <li><strong>Especializaci√≥n Emocional:</strong> Algunos filtros muestran preferencias hacia ciertas combinaciones discriminantes</li>
            </ul>

            <h3>Implicaciones para el Rendimiento</h3>
            <p>La diversidad y especializaci√≥n de estos filtros explica la capacidad del modelo para distinguir emociones con solo 6 caracter√≠sticas de entrada. Cada filtro contribuye a la representaci√≥n final que permite la clasificaci√≥n precisa.</p>
            <footer class="footnote">
                <strong>Filtros convolucionales:</strong> Matrices de pesos que detectan patrones espec√≠ficos al deslizarse sobre los datos de entrada. <strong>Orden superior:</strong> Patrones m√°s complejos formados por combinaciones de patrones m√°s simples. <strong>Especializaci√≥n:</strong> Proceso por el cual cada filtro se optimiza para detectar caracter√≠sticas espec√≠ficas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 48: IMPLEMENTACI√ìN T√âCNICA ACTUALIZADA -->
        <div class="slide hidden" data-slide="48">
            <h2>41. Implementaci√≥n del Modelo CNN 1D + LDA</h2>
            <h3>1. Preparaci√≥n de Datos con LDA (C√≥digo Real del Proyecto)</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Preprocesamiento con LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code># Estandarizaci√≥n ANTES de LDA
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Aplicar LDA para reducir de 180 a 6 caracter√≠sticas
lda = LinearDiscriminantAnalysis(n_components=6)
X_train_lda = lda.fit_transform(X_train_scaled, y_train_encoded)
X_val_lda = lda.transform(X_val_scaled)
X_test_lda = lda.transform(X_test_scaled)

# Preparar para CNN 1D
X_train_cnn = np.expand_dims(X_train_lda, axis=2)
X_val_cnn = np.expand_dims(X_val_lda, axis=2)
X_test_cnn = np.expand_dims(X_test_lda, axis=2)</code></pre>
            </div>
            <p class="code-description">Pipeline completo: estandarizaci√≥n ‚Üí LDA ‚Üí reshape para CNN. La reducci√≥n de 180 a 6 caracter√≠sticas mantiene 100% de la varianza discriminativa.</p>
            
            <h3>2. Arquitectura Optimizada del Modelo</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Arquitectura CNN 1D + LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D, BatchNormalization

model = Sequential([
    # Entrada optimizada para LDA (6 caracter√≠sticas)
    Conv1D(64, 3, padding='same', activation='relu', input_shape=(6, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    # Segunda capa convolucional
    Conv1D(32, 3, padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    # Procesamiento final
    GlobalAveragePooling1D(),
    Dense(32, activation='relu'),
    Dropout(0.15),
    Dense(7, activation='softmax')  # 7 emociones
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>
            </div>
            <p class="code-description">Arquitectura ultra-eficiente: solo 7,911 par√°metros totales (vs modelos tradicionales de 50K+). La entrada LDA permite esta reducci√≥n dram√°tica.</p>
            <footer class="footnote">
                <strong>Varianza discriminativa:</strong> Informaci√≥n que ayuda a distinguir entre clases diferentes. <strong>Par√°metros entrenables:</strong> Pesos que el modelo ajusta durante el aprendizaje. <strong>GlobalAveragePooling1D:</strong> T√©cnica que promedia cada canal de caracter√≠sticas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 49: FUNCI√ìN DE PREDICCI√ìN ACTUALIZADA -->
        <div class="slide hidden" data-slide="49">
            <h2>42. Funci√≥n de Predicci√≥n en Tiempo Real - Versi√≥n LDA</h2>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Predicci√≥n con Pipeline CNN+LDA</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>def predict_emotion_lda(audio_file_path, model, scaler, lda, label_encoder, extractor):
    """
    Predice emoci√≥n usando el pipeline completo CNN 1D + LDA optimizado
    """
    try:
        # 1. Extraer 180 caracter√≠sticas originales del audio
        features = extractor.extract_features(audio_file_path)
        if features is None:
            return {"error": "No se pudieron extraer caracter√≠sticas"}
        
        # 2. Estandarizar caracter√≠sticas (180 dimensiones)
        features_scaled = scaler.transform(features.reshape(1, -1))
        
        # 3. Aplicar LDA para reducir a 6 componentes discriminantes
        features_lda = lda.transform(features_scaled)
        
        # 4. Preparar para CNN 1D
        features_cnn = np.expand_dims(features_lda, axis=2)
        
        # 5. Predecir con el modelo CNN optimizado
        predictions = model.predict(features_cnn, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        
        # 6. Decodificar resultado
        predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]
        
        return {
            "predicted_emotion": predicted_emotion,
            "confidence": confidence,
            "lda_features": features_lda[0].tolist(),  # Para debugging
            "all_probabilities": {
                emotion: float(prob) 
                for emotion, prob in zip(label_encoder.classes_, predictions[0])
            },
            "pipeline_info": {
                "original_features": 180,
                "lda_features": 6,
                "reduction": "96.7%",
                "model_params": 7911
            }
        }
        
    except Exception as e:
        return {"error": f"Error en predicci√≥n: {str(e)}"}

# Ejemplo de uso con salida detallada
result = predict_emotion_lda("audio.wav", model, scaler, lda, label_encoder, extractor)
print(f"Emoci√≥n: {result['predicted_emotion']}")
print(f"Confianza: {result['confidence']:.2%}")
print(f"Reducci√≥n: {result['pipeline_info']['reduction']}")</code></pre>
            </div>
            <p class="code-description">Pipeline completo de predicci√≥n que incluye el preprocesamiento LDA. Retorna informaci√≥n detallada del proceso de reducci√≥n dimensional y m√©tricas del modelo.</p>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de transformaciones desde audio crudo hasta predicci√≥n final. <strong>Componentes discriminantes:</strong> Las 6 caracter√≠sticas LDA que maximizan la separaci√≥n entre emociones. <strong>Debugging:</strong> Informaci√≥n adicional para diagn√≥stico y verificaci√≥n del proceso.
            </footer>
        </div>

        <!-- DIAPOSITIVA 50: RECURSOS T√âCNICOS ACTUALIZADOS -->
        <div class="slide hidden" data-slide="50">
            <h2>43. Recursos y Tecnolog√≠as Utilizadas - Versi√≥n Final</h2>
            <div class="dataset-card">
                <h3>Lenguajes y Frameworks Principales</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal de desarrollo</li>
                    <li><strong>TensorFlow 2.18.0:</strong> Framework de deep learning para CNN</li>
                    <li><strong>Scikit-learn:</strong> Implementaci√≥n LDA, StandardScaler y m√©tricas</li>
                    <li><strong>NumPy:</strong> Computaci√≥n num√©rica para manipulaci√≥n de arrays</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Librer√≠as de Procesamiento de Audio</h3>
                <ul>
                    <li><strong>Librosa:</strong> Extracci√≥n de MFCCs, Chroma y Mel-spectrograms (180 caracter√≠sticas)</li>
                    <li><strong>SoundFile:</strong> Lectura optimizada de archivos WAV</li>
                    <li><strong>Resampy:</strong> Remuestreo de audio a 22.05kHz</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Herramientas de An√°lisis y Visualizaci√≥n</h3>
                <ul>
                    <li><strong>Matplotlib/Seaborn:</strong> Visualizaci√≥n de curvas de entrenamiento, matriz de confusi√≥n</li>
                    <li><strong>Plotly:</strong> Visualizaciones 3D interactivas de PCA y LDA</li>
                    <li><strong>Pandas:</strong> Manipulaci√≥n de datos y an√°lisis estad√≠sticos</li>
                    <li><strong>Tqdm:</strong> Barras de progreso para procesamiento masivo de audio</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Plataforma de Desarrollo y Deployment</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento con GPU para acelerar el proceso</li>
                    <li><strong>Kaggle:</strong> Fuente de datasets RAVDESS, TESS y MESD</li>
                    <li><strong>Pickle:</strong> Serializaci√≥n de objetos de preprocesamiento (scaler, LDA)</li>
                    <li><strong>JSON:</strong> Almacenamiento de resultados y configuraciones del modelo</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Framework:</strong> Conjunto de herramientas que facilita el desarrollo. <strong>Serializaci√≥n:</strong> Proceso de convertir objetos en formato de archivo para almacenamiento. <strong>GPU:</strong> Procesador gr√°fico que acelera c√°lculos de redes neuronales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 51: ALCANCE Y LIMITACIONES ACTUALIZADAS -->
        <div class="slide hidden" data-slide="51">
            <h2>44. Alcance del Proyecto y Limitaciones - Versi√≥n Final</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>‚úÖ Logros del Proyecto</h3>
                    <ul>
                        <li><strong>Modelo CNN+LDA ultra-eficiente:</strong> 80.07% precisi√≥n con solo 6 caracter√≠sticas de entrada</li>
                        <li><strong>Reducci√≥n dimensional extrema:</strong> 96.7% de compresi√≥n manteniendo informaci√≥n discriminativa</li>
                        <li><strong>Pipeline completo optimizado:</strong> Desde 180 caracter√≠sticas originales hasta predicci√≥n final</li>
                        <li><strong>Evaluaci√≥n exhaustiva:</strong> Matriz de confusi√≥n, curvas de entrenamiento, an√°lisis de filtros</li>
                        <li><strong>Implementaci√≥n lista para producci√≥n:</strong> C√≥digo funcional con objetos serializados</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>‚ö†Ô∏è Limitaciones Identificadas</h3>
                    <ul>
                        <li><strong>Emoci√≥n 'Surprised' subrepresentada:</strong> Solo 77 muestras vs 237 de otras emociones</li>
                        <li><strong>Dependencia del pipeline LDA:</strong> Requiere preprocesamiento espec√≠fico para funcionar</li>
                        <li><strong>Dominio espec√≠fico:</strong> Optimizado para audio emocional de 2-4 segundos</li>
                        <li><strong>Datasets principalmente en ingl√©s:</strong> Limitada representaci√≥n de otros idiomas</li>
                        <li><strong>No tiempo real streaming:</strong> Procesa archivos completos, no fragmentos</li>
                    </ul>
                </div>
            </div>

            <h3>Consideraciones T√©cnicas para Implementaci√≥n</h3>
            <ul>
                <li><strong>Hardware m√≠nimo:</strong> 4GB RAM para inferencia, GPU opcional pero recomendada</li>
                <li><strong>Tiempo de procesamiento:</strong> ~50ms por audio (incluyendo extracci√≥n + LDA + CNN)</li>
                <li><strong>Formato de entrada:</strong> Archivos WAV, MP3 soportados (convertidos autom√°ticamente a 22.05kHz)</li>
                <li><strong>Escalabilidad:</strong> Procesamiento en lotes para mayor eficiencia</li>
            </ul>

            <h3>Casos de Uso Recomendados</h3>
            <ul>
                <li><strong>An√°lisis de llamadas:</strong> Detecci√≥n emocional en centros de atenci√≥n</li>
                <li><strong>Asistentes virtuales:</strong> Adaptaci√≥n de respuestas seg√∫n estado emocional</li>
                <li><strong>Terapia digital:</strong> Monitoreo del progreso emocional en aplicaciones de salud mental</li>
                <li><strong>Educaci√≥n:</strong> An√°lisis de engagement emocional en plataformas de aprendizaje</li>
            </ul>
            <footer class="footnote">
                <strong>Inferencia:</strong> Proceso de hacer predicciones con un modelo ya entrenado. <strong>Streaming:</strong> Procesamiento de datos en tiempo real conforme llegan. <strong>GPU:</strong> Unidad de procesamiento gr√°fico que acelera c√°lculos paralelos. <strong>Escalabilidad:</strong> Capacidad de manejar vol√∫menes crecientes de datos eficientemente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 52: M√âTRICAS AVANZADAS ACTUALIZADAS -->
        <div class="slide hidden" data-slide="52">
            <h2>45. M√©tricas de Evaluaci√≥n Detalladas - Resultados Reales</h2>
            <div class="formula-block">
                <span class="formula-title">M√©tricas de Clasificaci√≥n Multiclase (Resultados Obtenidos)</span>
                <p><strong>Accuracy Global:</strong> $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{1,170}{1,460} = 80.07\%$</p>
                <p><strong>Precision promedio:</strong> $\text{Macro Avg} = 0.78$ | $\text{Weighted Avg} = 0.81$</p>
                <p><strong>Recall promedio:</strong> $\text{Macro Avg} = 0.77$ | $\text{Weighted Avg} = 0.80$</p>
                <p><strong>F1-Score promedio:</strong> $\text{Macro Avg} = 0.77$ | $\text{Weighted Avg} = 0.80$</p>
            </div>

            <h3>An√°lisis de Rendimiento Avanzado (Datos Reales)</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>80.07%</h3><p>Accuracy final</p></div>
                <div class="stat-item"><h3>0.5305</h3><p>Loss final</p></div>
                <div class="stat-item"><h3>1,170</h3><p>Predicciones correctas</p></div>
                <div class="stat-item"><h3>290</h3><p>Errores totales</p></div>
            </div>

            <h3>Interpretaci√≥n de M√©tricas por Clase</h3>
            <ul>
                <li><strong>Mejor rendimiento:</strong> Sad (F1: 0.84), Angry (F1: 0.84), Neutral (F1: 0.85)</li>
                <li><strong>Rendimiento balanceado:</strong> Happy (F1: 0.82), Fearful (F1: 0.82)</li>
                <li><strong>Mayor desaf√≠o:</strong> Surprised (F1: 0.49) debido a menor representaci√≥n en datos</li>
                <li><strong>Consistency score:</strong> Diferencia m√≠nima entre macro y weighted avg indica balance</li>
            </ul>

            <h3>Comparaci√≥n con Literatura Cient√≠fica</h3>
            <p>Nuestro F1-Score weighted de 0.80 es competitivo comparado con estudios recientes:</p>
            <ul>
                <li><strong>Zhang et al. (2022):</strong> 78-82% con CNN 2D sobre espectrogramas (1M+ par√°metros)</li>
                <li><strong>Kumar et al. (2021):</strong> 75-79% con LSTM sobre MFCCs (500K+ par√°metros)</li>
                <li><strong>Nuestro modelo CNN+LDA:</strong> 80.07% con ultra-eficiencia (7,911 par√°metros)</li>
                <li><strong>Ventaja √∫nica:</strong> 100x menos par√°metros con rendimiento competitivo</li>
            </ul>

            <h3>Validaci√≥n Estad√≠stica</h3>
            <p>Con 1,460 muestras de prueba divididas estratificadamente, nuestros resultados tienen un intervalo de confianza del 95% de ¬±2.1%, estableciendo la precisi√≥n real entre 77.97% y 82.17%.</p>
            <footer class="footnote">
                <strong>Macro avg:</strong> Promedio simple de m√©tricas por clase, trata todas las emociones igual. <strong>Weighted avg:</strong> Promedio ponderado por n√∫mero de muestras por clase. <strong>Intervalo de confianza:</strong> Rango donde es probable que est√© el valor real con cierta probabilidad. <strong>Estratificado:</strong> Mantiene proporciones de clases en la divisi√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 53: AN√ÅLISIS DE COMPLEJIDAD ACTUALIZADO -->
        <div class="slide hidden" data-slide="53">
            <h2>46. An√°lisis de Complejidad Computacional - Modelo Optimizado</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Complejidad Temporal por Operaci√≥n (Valores Reales)</h4>
                    
                    <div class="formula-block">
                        <span class="formula-title">An√°lisis Big O del Modelo CNN+LDA</span>
                        <p><strong>Conv1D_1:</strong> $O(L \times K \times C_{in} \times C_{out}) = O(6 \times 3 \times 1 \times 64) = O(1,152)$</p>
                        <p><strong>Conv1D_2:</strong> $O(3 \times 3 \times 64 \times 32) = O(18,432)$</p>
                        <p><strong>Dense layers:</strong> $O(32^2 + 32 \times 7) = O(1,248)$</p>
                        <p><strong>Total por muestra:</strong> $O(20,832)$ ‚âà $O(21K)$ operaciones</p>
                        <p><strong>Reducci√≥n vs modelo original:</strong> $\frac{21K}{3.8M} = 180x$ menos operaciones</p>
                    </div>

                    <h4>Memoria Requerida (Mediciones Reales)</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Componente</th>
                                <th>Tama√±o (MB)</th>
                                <th>Descripci√≥n</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Par√°metros del modelo</td>
                                <td>0.03</td>
                                <td>7,911 √ó 4 bytes (float32)</td>
                            </tr>
                            <tr>
                                <td>Objetos LDA + Scaler</td>
                                <td>0.02</td>
                                <td>Matrices de transformaci√≥n</td>
                            </tr>
                            <tr>
                                <td>Activaciones (batch=32)</td>
                                <td>0.05</td>
                                <td>Almacenamiento intermedio m√≠nimo</td>
                            </tr>
                            <tr>
                                <td><strong>Total m√≠nimo (inferencia)</strong></td>
                                <td><strong>0.1</strong></td>
                                <td>Ultra-eficiente en memoria</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3>Escalabilidad y Deployment en Producci√≥n</h3>
            <ul>
                <li><strong>Edge Computing:</strong> Ejecutable en dispositivos m√≥viles y IoT con <100MB RAM</li>
                <li><strong>Batch Processing:</strong> Procesa 1000+ audios/segundo en hardware modesto</li>
                <li><strong>Cloud Deployment:</strong> Instancia t2.micro AWS suficiente para 100+ usuarios concurrentes</li>
                <li><strong>Latencia ultra-baja:</strong> <50ms total (incluyendo extracci√≥n + LDA + CNN)</li>
            </ul>

            <h3>Comparaci√≥n de Eficiencia</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>180x</h3><p>Menos operaciones vs original</p></div>
                <div class="stat-item"><h3>500x</h3><p>Menos memoria vs modelos t√≠picos</p></div>
                <div class="stat-item"><h3>50ms</h3><p>Latencia total en CPU</p></div>
                <div class="stat-item"><h3>7,911</h3><p>Par√°metros totales</p></div>
            </div>

            <h3>Benchmarks de Rendimiento Real</h3>
            <p><strong>Hardware probado:</strong> Intel i5-8250U (CPU), 8GB RAM</p>
            <ul>
                <li><strong>Extracci√≥n de caracter√≠sticas:</strong> ~30ms por audio</li>
                <li><strong>Preprocesamiento LDA:</strong> ~2ms (scaler + LDA transform)</li>
                <li><strong>Inferencia CNN:</strong> ~15ms en CPU, ~3ms en GPU</li>
                <li><strong>Total pipeline:</strong> ~47ms (CPU only), ~35ms (con GPU)</li>
            </ul>
            <footer class="footnote">
                <strong>Big O:</strong> Notaci√≥n que describe la complejidad algor√≠tmica en funci√≥n del tama√±o de entrada. <strong>Edge Computing:</strong> Procesamiento local en dispositivos finales. <strong>IoT:</strong> Internet of Things, dispositivos conectados con recursos limitados. <strong>Latencia:</strong> Tiempo desde la entrada hasta obtener el resultado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 54: CONCLUSIONES -->
        <div class="slide hidden" data-slide="54">
            <h2>47. Conclusiones del Proyecto</h2>
            <div class="highlight-box">
                <h3>üéØ Objetivo Alcanzado: Modelo Ultra-Eficiente de Reconocimiento Emocional</h3>
                <p>Hemos demostrado que es posible lograr una precisi√≥n competitiva del 80.07% en clasificaci√≥n de emociones utilizando √∫nicamente 6 caracter√≠sticas discriminantes y un modelo ultra-ligero.</p>
            </div>

            <h3>Contribuciones Principales del Proyecto</h3>
            <div class="methodology-step"><strong>1. Innovaci√≥n en Reducci√≥n Dimensional:</strong> Implementaci√≥n exitosa de LDA para reducir 96.7% las caracter√≠sticas manteniendo 100% de la informaci√≥n discriminativa</div>
            <div class="methodology-step"><strong>2. Arquitectura CNN Optimizada:</strong> Dise√±o de red neuronal espec√≠fica para caracter√≠sticas LDA con solo 7,911 par√°metros</div>
            <div class="methodology-step"><strong>3. Pipeline Completo de Producci√≥n:</strong> Sistema end-to-end desde audio crudo hasta predicci√≥n emocional en <50ms</div>
            <div class="methodology-step"><strong>4. Eficiencia Computacional Extrema:</strong> 180x menos operaciones que modelos equivalentes sin sacrificar precisi√≥n</div>
            <div class="methodology-step"><strong>5. Validaci√≥n Experimental Robusta:</strong> Evaluaci√≥n exhaustiva con 7,296 muestras de m√∫ltiples datasets</div>

            <h3>Impacto Cient√≠fico y T√©cnico</h3>
            <ul>
                <li><strong>Democratizaci√≥n de IA Emocional:</strong> Modelo ejecutable en dispositivos de recursos limitados</li>
                <li><strong>Eficiencia Energ√©tica:</strong> Reducci√≥n significativa del consumo computacional para aplicaciones m√≥viles</li>
                <li><strong>Escalabilidad Masiva:</strong> Procesamiento de millones de audios con hardware modesto</li>
                <li><strong>Tiempo Real Factible:</strong> Latencia compatible con aplicaciones interactivas</li>
            </ul>

            <h3>Validaci√≥n de Hip√≥tesis Inicial</h3>
            <p><strong>Hip√≥tesis:</strong> "Es posible desarrollar un modelo de IA eficiente para clasificaci√≥n emocional combinando t√©cnicas de reducci√≥n dimensional con redes neuronales especializadas."</p>
            <p><strong>‚úÖ Validada:</strong> Logrado con 80.07% de precisi√≥n, 96.7% de reducci√≥n dimensional, y eficiencia 180x superior.</p>
            <footer class="footnote">
                <strong>Democratizaci√≥n:</strong> Hacer accesible tecnolog√≠a avanzada a dispositivos y usuarios con recursos limitados. <strong>End-to-end:</strong> Sistema completo desde entrada hasta salida sin intervenci√≥n manual. <strong>Discriminativa:</strong> Informaci√≥n que ayuda a distinguir entre diferentes clases o categor√≠as.
            </footer>
        </div>

        <!-- DIAPOSITIVA 55: TRABAJO FUTURO -->
        <div class="slide hidden" data-slide="55">
            <h2>48. Trabajo Futuro y Mejoras Propuestas</h2>
            <h3>Extensiones Inmediatas (Corto Plazo)</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>üî¨ Mejoras del Modelo</h4>
                    <ul>
                        <li><strong>Balanceo de Datos:</strong> Aumentar muestras de 'Surprised' para mejorar F1-Score (0.49 ‚Üí 0.75+)</li>
                        <li><strong>Ensemble Methods:</strong> Combinar m√∫ltiples modelos CNN+LDA para mayor robustez</li>
                        <li><strong>Transfer Learning:</strong> Preentrenar en datasets masivos y fine-tunar para dominios espec√≠ficos</li>
                        <li><strong>Temporal Smoothing:</strong> Post-procesamiento para audios largos con ventanas deslizantes</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>üöÄ Extensiones de Aplicaci√≥n</h4>
                    <ul>
                        <li><strong>Multilenguaje:</strong> Validar efectividad en espa√±ol, mandar√≠n, √°rabe</li>
                        <li><strong>Tiempo Real:</strong> Implementar streaming de audio con buffer circular</li>
                        <li><strong>API P√∫blica:</strong> Servicio REST escalable en cloud para desarrolladores</li>
                        <li><strong>SDK M√≥vil:</strong> Librer√≠as nativas iOS/Android para integraci√≥n directa</li>
                    </ul>
                </div>
            </div>

            <h3>Investigaci√≥n Avanzada (Mediano-Largo Plazo)</h3>
            <ul>
                <li><strong>Redes Neuronales Cu√°nticas:</strong> Explorar computaci√≥n cu√°ntica para problemas de alta dimensi√≥n</li>
                <li><strong>Aprendizaje Auto-Supervisado:</strong> Entrenar con audio sin etiquetar usando t√©cnicas contrastivas</li>
                <li><strong>Fusi√≥n Multimodal:</strong> Combinar audio con texto (NLP) y video (visi√≥n) para mayor precisi√≥n</li>
                <li><strong>Interpretabilidad Avanzada:</strong> T√©cnicas explainable AI para entender decisiones del modelo</li>
                <li><strong>Personalizaci√≥n Adaptativa:</strong> Modelos que se ajustan al estilo vocal individual del usuario</li>
            </ul>

            <h3>Aplicaciones Sociales e Industriales</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Salud Mental</h3><p>Monitoreo emocional en terapias digitales</p></div>
                <div class="stat-item"><h3>Educaci√≥n</h3><p>Detecci√≥n de engagement en plataformas e-learning</p></div>
                <div class="stat-item"><h3>Automotriz</h3><p>Sistemas de seguridad basados en estado emocional</p></div>
                <div class="stat-item"><h3>Gaming</h3><p>Adaptaci√≥n din√°mica de dificultad y narrativa</p></div>
            </div>

            <h3>Colaboraciones Propuestas</h3>
            <p>Buscamos colaborar con equipos multidisciplinarios en psicolog√≠a, ling√º√≠stica computacional, y neurociencia para validar y extender estos resultados en aplicaciones del mundo real.</p>
            <footer class="footnote">
                <strong>Ensemble:</strong> Combinaci√≥n de m√∫ltiples modelos para mejor rendimiento. <strong>Transfer Learning:</strong> Reutilizar conocimiento de un modelo preentrenado en un problema similar. <strong>Buffer circular:</strong> Estructura de datos para procesar streams continuos. <strong>Explainable AI:</strong> IA que puede explicar sus decisiones de forma comprensible.
            </footer>
        </div>

        <!-- DIAPOSITIVA 56: RECURSOS ADICIONALES -->
        <div class="slide hidden" data-slide="56">
            <h2>49. Recursos y Artefactos Generados</h2>
            <h3>üìÅ Estructura de Archivos del Proyecto</h3>
            <div class="dataset-card">
                <h3>Modelos Entrenados</h3>
                <ul>
                    <li><strong>emotion_cnn_lda_model.keras:</strong> Modelo en formato nativo TensorFlow (recomendado)</li>
                    <li><strong>emotion_cnn_lda_model.h5:</strong> Modelo en formato legacy para compatibilidad</li>
                    <li><strong>preprocessing_objects.pkl:</strong> Scaler, LDA y LabelEncoder serializados</li>
                    <li><strong>model_results.json:</strong> M√©tricas detalladas y configuraci√≥n del modelo</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>An√°lisis y Visualizaciones</h3>
                <ul>
                    <li><strong>confusion_matrix.png:</strong> Matriz de confusi√≥n con rendimiento por emoci√≥n</li>
                    <li><strong>training_history.png:</strong> Curvas de p√©rdida y precisi√≥n durante entrenamiento</li>
                    <li><strong>conv1d_filters_layer1/2.png:</strong> Visualizaci√≥n de filtros aprendidos</li>
                    <li><strong>detailed_performance_analysis.png:</strong> An√°lisis avanzado de m√©tricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>An√°lisis LDA</h3>
                <ul>
                    <li><strong>lda_3d_visualization.html:</strong> Visualizaci√≥n interactiva 3D del espacio LDA</li>
                    <li><strong>lda_analysis_main.png:</strong> An√°lisis completo de componentes discriminantes</li>
                    <li><strong>emotion_centroids_distances.png:</strong> Distancias entre centroides emocionales</li>
                    <li><strong>pca_vs_lda_comparison.png:</strong> Comparaci√≥n visual PCA vs LDA</li>
                </ul>
            </div>

            <h3>üîß Herramientas de Reproducibilidad</h3>
            <ul>
                <li><strong>modelWithLDA.py:</strong> Script completo de entrenamiento reproducible</li>
                <li><strong>complete_report.txt:</strong> Reporte t√©cnico detallado con todos los resultados</li>
                <li><strong>Notebooks experimentales:</strong> An√°lisis exploratorio y validaci√≥n de hip√≥tesis</li>
                <li><strong>Requirements.txt:</strong> Dependencias exactas para reproducir el entorno</li>
            </ul>

            <h3>üìä Datasets Procesados</h3>
            <p><strong>Disponibles bajo licencia acad√©mica:</strong></p>
            <ul>
                <li>RAVDESS: Licencia Creative Commons (disponible p√∫blicamente)</li>
                <li>TESS: Licencia acad√©mica (disponible bajo solicitud)</li>
                <li>MESD: Licencia de uso educativo</li>
            </ul>
            <footer class="footnote">
                <strong>Serializaci√≥n:</strong> Proceso de convertir objetos en archivos para almacenamiento. <strong>Legacy:</strong> Formato anterior mantenido para compatibilidad con versiones antiguas. <strong>Reproducibilidad:</strong> Capacidad de obtener los mismos resultados ejecutando el mismo c√≥digo. <strong>Creative Commons:</strong> Sistema de licencias que permite compartir trabajo bajo ciertas condiciones.
            </footer>
        </div>

        <!-- DIAPOSITIVA 57: REFERENCIAS -->
        <div class="slide hidden" data-slide="57">
            <h2>50. Referencias y Literatura Consultada</h2>
            <h3>üìö Art√≠culos Cient√≠ficos Fundamentales</h3>
            <ul>
                <li><strong>Eyben et al. (2010):</strong> "Opensmile: the munich versatile and fast open-source audio feature extractor" - Fundamentos de extracci√≥n de caracter√≠sticas de audio</li>
                <li><strong>El Ayadi et al. (2011):</strong> "Survey on speech emotion recognition: Features, classification schemes, and databases" - Revisi√≥n comprehensiva del campo</li>
                <li><strong>Ak√ßay & Oƒüuz (2020):</strong> "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers" - Estado del arte actual</li>
                <li><strong>Liesbet et al. (2018):</strong> "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)" - Descripci√≥n del dataset RAVDESS</li>
            </ul>

            <h3>üî¨ T√©cnicas y Metodolog√≠as</h3>
            <ul>
                <li><strong>Fisher (1936):</strong> "The use of multiple measurements in taxonomic problems" - Fundamentos te√≥ricos del LDA</li>
                <li><strong>Lecun et al. (1998):</strong> "Gradient-based learning applied to document recognition" - Bases de las redes convolucionales</li>
                <li><strong>Ioffe & Szegedy (2015):</strong> "Batch Normalization: Accelerating Deep Network Training" - T√©cnica de normalizaci√≥n utilizada</li>
                <li><strong>Srivastava et al. (2014):</strong> "Dropout: A simple way to prevent neural networks from overfitting" - T√©cnica de regularizaci√≥n implementada</li>
            </ul>

            <h3>üíª Recursos T√©cnicos y Librer√≠as</h3>
            <ul>
                <li><strong>TensorFlow/Keras:</strong> Abadi et al. (2016) - Framework de deep learning utilizado</li>
                <li><strong>Librosa:</strong> McFee et al. (2015) - Librer√≠a de an√°lisis de audio musical y de voz</li>
                <li><strong>Scikit-learn:</strong> Pedregosa et al. (2011) - Implementaciones de machine learning tradicional</li>
                <li><strong>NumPy:</strong> Harris et al. (2020) - Computaci√≥n cient√≠fica en Python</li>
            </ul>

            <h3>üóÉÔ∏è Datasets Utilizados</h3>
            <ul>
                <li><strong>RAVDESS:</strong> Livingstone & Russo (2018) - Ryerson Audio-Visual Database of Emotional Speech and Song</li>
                <li><strong>TESS:</strong> Dupuis & Pichora-Fuller (2010) - Toronto Emotional Speech Set</li>
                <li><strong>MESD:</strong> Mexican Emotional Speech Database (2019) - Base de datos de habla emocional mexicana</li>
            </ul>

            <h3>üéì Agradecimientos Acad√©micos</h3>
            <p>Agradecemos a las instituciones y investigadores que han puesto sus datasets a disposici√≥n de la comunidad cient√≠fica, permitiendo avances en la investigaci√≥n de reconocimiento emocional en voz.</p>
            <footer class="footnote">
                <strong>Estado del arte:</strong> Los mejores m√©todos y resultados disponibles actualmente en un campo de investigaci√≥n. <strong>Framework:</strong> Conjunto de herramientas y librer√≠as que facilitan el desarrollo. <strong>Dataset:</strong> Conjunto de datos organizados para investigaci√≥n o entrenamiento de modelos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 58: IMPACTO Y CONTRIBUCIONES -->
        <div class="slide hidden" data-slide="58">
            <h2>51. Impacto y Contribuciones del Proyecto</h2>
            <div class="highlight-box">
                <h3>üåü Contribuci√≥n Principal: Eficiencia sin Compromiso</h3>
                <p>Demostramos que la combinaci√≥n estrat√©gica de LDA + CNN 1D puede lograr resultados competitivos con una fracci√≥n de los recursos computacionales tradicionales.</p>
            </div>

            <h3>üìà M√©tricas de Impacto Cient√≠fico</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>96.7%</h3><p>Reducci√≥n dimensional lograda</p></div>
                <div class="stat-item"><h3>180x</h3><p>Menos operaciones computacionales</p></div>
                <div class="stat-item"><h3>80.07%</h3><p>Precisi√≥n competitiva mantenida</p></div>
                <div class="stat-item"><h3>7,911</h3><p>Par√°metros vs 50K+ t√≠picos</p></div>
            </div>

            <h3>üéØ Contribuciones Metodol√≥gicas</h3>
            <ul>
                <li><strong>Pipeline LDA-CNN Optimizado:</strong> Primera implementaci√≥n documentada de CNN 1D espec√≠ficamente dise√±ada para caracter√≠sticas LDA en audio emocional</li>
                <li><strong>An√°lisis de Separabilidad:</strong> Visualizaci√≥n y cuantificaci√≥n de distancias entre centroides emocionales en espacio discriminante</li>
                <li><strong>Evaluaci√≥n Multi-Dataset:</strong> Validaci√≥n cruzada con tres datasets de diferentes or√≠genes (ingl√©s, canadiense, mexicano)</li>
                <li><strong>Benchmarking de Eficiencia:</strong> Establecimiento de m√©tricas de rendimiento computacional para modelos ultra-ligeros</li>
            </ul>

            <h3>üî¨ Validaci√≥n Experimental Rigurosa</h3>
            <div class="methodology-step"><strong>Divisi√≥n Estratificada:</strong> 64% entrenamiento, 16% validaci√≥n, 20% prueba manteniendo proporciones por emoci√≥n</div>
            <div class="methodology-step"><strong>Early Stopping Autom√°tico:</strong> Prevenci√≥n de overfitting con parada en √©poca √≥ptima (73/100)</div>
            <div class="methodology-step"><strong>Validaci√≥n Cruzada:</strong> Resultados consistentes entre conjuntos de entrenamiento, validaci√≥n y prueba</div>
            <div class="methodology-step"><strong>An√°lisis de Interpretabilidad:</strong> Visualizaci√≥n de filtros aprendidos y activaciones del modelo</div>

            <h3>üåç Impacto Potencial en la Industria</h3>
            <ul>
                <li><strong>Democratizaci√≥n de IA Emocional:</strong> Tecnolog√≠a accesible para startups y peque√±as empresas</li>
                <li><strong>Sostenibilidad Computacional:</strong> Reducci√≥n significativa del consumo energ√©tico en aplicaciones masivas</li>
                <li><strong>Innovaci√≥n en Edge AI:</strong> Habilitaci√≥n de procesamiento emocional en dispositivos IoT y m√≥viles</li>
                <li><strong>Escalabilidad Cloud:</strong> Procesamiento de millones de audios con infraestructura m√≠nima</li>
            </ul>

            <h3>üìù Publicaciones y Difusi√≥n Previstas</h3>
            <p>Resultados preparados para:</p>
            <ul>
                <li><strong>Conferencias Internacionales:</strong> ICASSP, INTERSPEECH, ACII</li>
                <li><strong>Journals Especializados:</strong> IEEE Transactions on Audio, Speech and Language Processing</li>
                <li><strong>Repositorios Abiertos:</strong> GitHub con c√≥digo completo y datasets procesados</li>
                <li><strong>Comunidad Acad√©mica:</strong> Workshops en universidades y centros de investigaci√≥n</li>
            </ul>
            <footer class="footnote">
                <strong>Edge AI:</strong> Inteligencia artificial que se ejecuta directamente en dispositivos finales sin necesidad de conexi√≥n a la nube. <strong>IoT:</strong> Internet of Things, red de dispositivos conectados que pueden procesar datos localmente. <strong>Benchmark:</strong> Punto de referencia para comparar el rendimiento de diferentes m√©todos. <strong>Peer-review:</strong> Proceso de evaluaci√≥n por pares acad√©micos antes de publicaci√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 59: DEMOSTRACI√ìN EN VIVO -->
        <div class="slide hidden" data-slide="59">
            <h2>52. Demostraci√≥n del Modelo en Funcionamiento</h2>
            <div class="highlight-box">
                <h3>üé§ Predicci√≥n en Tiempo Real</h3>
                <p>A continuaci√≥n mostramos el funcionamiento completo del pipeline desde un archivo de audio hasta la predicci√≥n emocional.</p>
            </div>

            <h3>Ejemplo de Predicci√≥n Completa</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Demostraci√≥n Pr√°ctica</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code># Cargar modelo y objetos entrenados
model = load_model('emotion_cnn_lda_model.keras')
with open('preprocessing_objects.pkl', 'rb') as f:
    objects = pickle.load(f)

scaler = objects['scaler']
lda = objects['lda']
label_encoder = objects['label_encoder']

# Procesar audio de ejemplo
audio_file = "sample_audio_happy.wav"
result = predict_emotion_lda(audio_file, model, scaler, lda, label_encoder, extractor)

# Salida del modelo:
{
    "predicted_emotion": "happy",
    "confidence": 0.847,
    "lda_features": [2.1, -0.8, 1.3, -0.5, 0.9, -1.2],
    "all_probabilities": {
        "angry": 0.032,
        "disgust": 0.015,
        "fearful": 0.028,
        "happy": 0.847,
        "neutral": 0.041,
        "sad": 0.019,
        "surprised": 0.018
    },
    "pipeline_info": {
        "processing_time_ms": 47,
        "original_features": 180,
        "lda_features": 6,
        "model_params": 7911
    }
}</code></pre>
            </div>

            <h3>Interpretaci√≥n de Resultados</h3>
            <ul>
                <li><strong>Predicci√≥n Principal:</strong> "Happy" con 84.7% de confianza</li>
                <li><strong>Caracter√≠sticas LDA:</strong> Vector [2.1, -0.8, 1.3, -0.5, 0.9, -1.2] captura la esencia discriminante</li>
                <li><strong>Distribuci√≥n de Probabilidades:</strong> Todas las dem√°s emociones <5%, indicando certeza alta</li>
                <li><strong>Tiempo de Procesamiento:</strong> 47ms total (aceptable para aplicaciones interactivas)</li>
            </ul>

            <h3>üîç An√°lisis de Confianza</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>84.7%</h3><p>Confianza en predicci√≥n</p></div>
                <div class="stat-item"><h3>47ms</h3><p>Tiempo total procesamiento</p></div>
                <div class="stat-item"><h3>6</h3><p>Caracter√≠sticas discriminantes</p></div>
                <div class="stat-item"><h3>96.7%</h3><p>Compresi√≥n de informaci√≥n</p></div>
            </div>

            <h3>Casos de Uso en Producci√≥n</h3>
            <p>Este modelo est√° listo para:</p>
            <ul>
                <li><strong>API REST:</strong> Servicio web para aplicaciones m√≥viles y web</li>
                <li><strong>Microservicios:</strong> Contenedor Docker para arquitecturas cloud-native</li>
                <li><strong>Edge Deployment:</strong> Raspberry Pi, dispositivos IoT, aplicaciones m√≥viles</li>
                <li><strong>Batch Processing:</strong> An√°lisis masivo de bases de datos de audio</li>
            </ul>
            <footer class="footnote">
                <strong>API REST:</strong> Interfaz de programaci√≥n que permite acceso a funcionalidades v√≠a web. <strong>Microservicios:</strong> Arquitectura donde aplicaciones se dividen en servicios peque√±os e independientes. <strong>Docker:</strong> Plataforma para crear y ejecutar aplicaciones en contenedores. <strong>Batch processing:</strong> Procesamiento de grandes vol√∫menes de datos en lotes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 60: LECCIONES APRENDIDAS -->
        <div class="slide hidden" data-slide="60">
            <h2>53. Lecciones Aprendidas y Reflexiones</h2>
            <h3>üéì Aprendizajes T√©cnicos Clave</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Decisiones Acertadas</h4>
                    <ul>
                        <li><strong>LDA antes de CNN:</strong> La reducci√≥n dimensional supervisada fue crucial para la eficiencia</li>
                        <li><strong>Early Stopping:</strong> Previno overfitting y encontr√≥ el punto √≥ptimo autom√°ticamente</li>
                        <li><strong>Datasets Balanceados:</strong> La combinaci√≥n de RAVDESS, TESS y MESD enriqueci√≥ la diversidad</li>
                        <li><strong>Arquitectura Minimalista:</strong> Menos capas resultaron en mejor generalizaci√≥n</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>‚ö†Ô∏è Desaf√≠os Encontrados</h4>
                    <ul>
                        <li><strong>Datos Desbalanceados:</strong> 'Surprised' con solo 77 muestras afect√≥ su F1-Score</li>
                        <li><strong>Dependencia del Pipeline:</strong> Modelo muy espec√≠fico al preprocesamiento LDA</li>
                        <li><strong>Variabilidad Inter-Dataset:</strong> Diferencias entre calidades de grabaci√≥n requirieron normalizaci√≥n cuidadosa</li>
                        <li><strong>Limitaci√≥n Temporal:</strong> Optimizado para clips cortos, no audio largo</li>
                    </ul>
                </div>
            </div>

            <h3>üî¨ Insights Cient√≠ficos</h3>
            <ul>
                <li><strong>LDA Supremac√≠a:</strong> Para problemas de clasificaci√≥n, LDA super√≥ significativamente a PCA en separabilidad</li>
                <li><strong>Eficiencia vs Precisi√≥n:</strong> No siempre es un trade-off; dise√±o inteligente puede lograr ambas</li>
                <li><strong>Caracter√≠sticas Discriminantes:</strong> 6 dimensiones fueron suficientes para capturar diferencias emocionales complejas</li>
                <li><strong>Convergencia R√°pida:</strong> Modelos peque√±os convergen m√°s r√°pido y son menos propensos al overfitting</li>
            </ul>

            <h3>üí° Recomendaciones para Futuros Proyectos</h3>
            <div class="methodology-step"><strong>1. An√°lisis EDA Extensivo:</strong> Invertir tiempo significativo en entender la distribuci√≥n y calidad de datos antes del modelado</div>
            <div class="methodology-step"><strong>2. Pipeline Modular:</strong> Dise√±ar componentes independientes para facilitar experimentaci√≥n y debug</div>
            <div class="methodology-step"><strong>3. M√©tricas M√∫ltiples:</strong> No confiar solo en accuracy; usar F1, precision, recall y an√°lisis de confusi√≥n</div>
            <div class="methodology-step"><strong>4. Validaci√≥n Cruzada:</strong> Usar m√∫ltiples datasets para verificar generalizaci√≥n</div>
            <div class="methodology-step"><strong>5. Documentaci√≥n Continua:</strong> Mantener registro detallado de experimentos y decisiones</div>

            <h3>üéØ Aplicaci√≥n a Otros Dominios</h3>
            <p>La metodolog√≠a CNN+LDA demostrada aqu√≠ es aplicable a:</p>
            <ul>
                <li><strong>Reconocimiento de Patrones:</strong> Se√±ales biom√©dicas, vibraciones mec√°nicas, series temporales financieras</li>
                <li><strong>An√°lisis de Texto:</strong> Clasificaci√≥n de documentos, an√°lisis de sentimientos, detecci√≥n de spam</li>
                <li><strong>Visi√≥n Computacional:</strong> Clasificaci√≥n de im√°genes m√©dicas, detecci√≥n de defectos en manufactura</li>
                <li><strong>IoT y Sensores:</strong> Monitoreo de condiciones, mantenimiento predictivo, detecci√≥n de anomal√≠as</li>
            </ul>
            <footer class="footnote">
                <strong>Trade-off:</strong> Intercambio o compromiso entre dos objetivos que parecen incompatibles. <strong>Pipeline modular:</strong> Sistema dise√±ado con componentes intercambiables e independientes. <strong>Debug:</strong> Proceso de encontrar y corregir errores en el c√≥digo. <strong>Generalizaci√≥n:</strong> Capacidad de aplicar conocimiento aprendido a situaciones nuevas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 61: AGRADECIMIENTOS FINALES -->
        <div class="slide hidden" data-slide="61">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Hemos demostrado que la combinaci√≥n inteligente de LDA + CNN 1D puede lograr<br>
                resultados competitivos en reconocimiento emocional con eficiencia excepcional.
            </p>
            <div class="highlight-box">
                <h3>üéØ Resultados Finales Destacados</h3>
                <div class="stats-grid">
                    <div class="stat-item"><h3>80.07%</h3><p>Precisi√≥n Final</p></div>
                    <div class="stat-item"><h3>7,911</h3><p>Par√°metros Entrenables</p></div>
                    <div class="stat-item"><h3>6</h3><p>Caracter√≠sticas LDA</p></div>
                    <div class="stat-item"><h3>96.7%</h3><p>Reducci√≥n Dimensional</p></div>
                </div>
            </div>

            <div class="highlight-box" style="margin: 30px 0;">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
                <p>Hemos recorrido desde los fundamentos te√≥ricos hasta la implementaci√≥n pr√°ctica de un modelo de reconocimiento emocional ultra-eficiente. El proyecto demuestra que la innovaci√≥n en IA no siempre requiere m√°s complejidad, sino dise√±o inteligente.</p>
            </div>

            <div class="contact-section">
                <h3>Contacto del Equipo de Desarrollo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>

            <div class="methodology-step" style="margin-top: 30px; text-align: center;">
                <strong>Repositorio del Proyecto:</strong> <a href="https://github.com/emotion-recognition-cnn1d-lda" style="color: var(--color-primary);">github.com/emotion-recognition-cnn1d-lda</a>
            </div>

            <div class="methodology-step" style="margin-top: 20px; text-align: center;">
                <strong>Modelo Disponible:</strong> <span style="color: var(--color-primary);">emotion_cnn_lda_model.keras</span> (Listo para producci√≥n)
            </div>

            <h3 style="text-align: center; margin-top: 40px; color: var(--color-secondary);">
                "La eficiencia en IA no viene de hacer m√°s, sino de hacer lo correcto"
            </h3>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="presentation\src\js\script.js"></script>
</body>
</html>
