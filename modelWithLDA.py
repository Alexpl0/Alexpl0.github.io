# =============================================================================
# ENTRENAMIENTO AVANZADO DE CNN 1D CON LDA PARA RECONOCIMIENTO DE EMOCIONES EN VOZ
# Versi√≥n corregida y optimizada para Google Colab con an√°lisis LDA
# =============================================================================

print("üöÄ Iniciando entrenamiento avanzado de CNN 1D con LDA para reconocimiento emocional")
print("=" * 80)

# =============================================================================
# PARTE 1: INSTALACI√ìN Y CONFIGURACI√ìN AVANZADA (CORREGIDA)
# =============================================================================

print("üì¶ Instalando dependencias optimizadas para GPU...")
# CORRECCI√ìN: Agregar resampy y otras dependencias faltantes
!pip install -q kaggle librosa resampy soundfile numpy pandas scikit-learn tensorflow plotly seaborn tqdm
!pip install -q tensorboard numba==0.56.4

# Verificar GPU
import tensorflow as tf
print(f"üîß TensorFlow versi√≥n: {tf.__version__}")
print(f"üéÆ GPU disponible: {tf.config.list_physical_devices('GPU')}")

# Configurar GPU para crecimiento din√°mico de memoria
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("‚úÖ Configuraci√≥n de GPU optimizada")
        print(f"üíæ GPU: {tf.test.gpu_device_name()}")
    except RuntimeError as e:
        print(f"‚ö†Ô∏è Error configurando GPU: {e}")
else:
    print("‚ö†Ô∏è No se detect√≥ GPU, usando CPU")

# Importaciones principales
import os
import json
import numpy as np
import pandas as pd
import librosa
import soundfile as sf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Conv1D, MaxPooling1D, Dropout, Dense,
                                   BatchNormalization, GlobalAveragePooling1D, Input)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau,
                                       ModelCheckpoint, TensorBoard)
from tensorflow.keras.utils import to_categorical
from google.colab import files
from tqdm.notebook import tqdm
import warnings
warnings.filterwarnings('ignore')

# Verificar instalaci√≥n de resampy
try:
    import resampy
    print("‚úÖ resampy instalado correctamente")
except ImportError:
    print("‚ùå Error: resampy no est√° instalado")
    !pip install resampy
    import resampy
    print("‚úÖ resampy instalado y importado")

# =============================================================================
# PARTE 2: CONFIGURACI√ìN DE KAGGLE Y DESCARGA DE DATASETS
# =============================================================================

print("\nüìÇ Configurando acceso a Kaggle...")

def setup_kaggle_credentials():
    """Configura las credenciales de Kaggle para acceso a datasets"""
    kaggle_dir = "/root/.kaggle"
    kaggle_file = os.path.join(kaggle_dir, "kaggle.json")

    if not os.path.exists(kaggle_file):
        print("üîë Por favor, sube tu archivo kaggle.json:")
        uploaded = files.upload()

        if "kaggle.json" in uploaded:
            os.makedirs(kaggle_dir, exist_ok=True)
            with open(kaggle_file, "w") as f:
                f.write(uploaded["kaggle.json"].decode())
            os.chmod(kaggle_file, 0o600)
            print("‚úÖ Credenciales de Kaggle configuradas")
        else:
            raise Exception("‚ùå Archivo kaggle.json no encontrado")
    else:
        print("‚úÖ Credenciales de Kaggle ya configuradas")

def download_datasets():
    """Descarga todos los datasets necesarios"""
    datasets = {
        "RAVDESS": "uwrfkaggler/ravdess-emotional-speech-audio",
        "TESS": "ejlok1/toronto-emotional-speech-set-tess",
        "MESD": "ejlok1/mexican-emotional-speech-database"
    }

    downloaded_paths = {}

    for name, dataset_id in datasets.items():
        path = f"/content/{name}_data"

        if not os.path.exists(path):
            print(f"‚¨áÔ∏è Descargando {name}...")
            try:
                os.system(f"kaggle datasets download -d {dataset_id} -p {path} --unzip -q")
                print(f"‚úÖ {name} descargado exitosamente")
                downloaded_paths[name] = path
            except Exception as e:
                print(f"‚ö†Ô∏è Error descargando {name}: {e}")
                downloaded_paths[name] = None
        else:
            print(f"‚úÖ {name} ya existe")
            downloaded_paths[name] = path

    return downloaded_paths

# Configurar y descargar datasets
setup_kaggle_credentials()
dataset_paths = download_datasets()

# =============================================================================
# PARTE 3: EXTRACCI√ìN AVANZADA DE CARACTER√çSTICAS (CORREGIDA)
# =============================================================================

print("\nüéµ Configurando extracci√≥n avanzada de caracter√≠sticas...")

class AdvancedFeatureExtractor:
    """Extractor avanzado de caracter√≠sticas de audio para reconocimiento emocional"""

    def __init__(self,
                 sr=22050,
                 n_mfcc=40,
                 n_chroma=12,
                 n_mel=128,
                 hop_length=512,
                 win_length=2048):

        self.sr = sr
        self.n_mfcc = n_mfcc
        self.n_chroma = n_chroma
        self.n_mel = n_mel
        self.hop_length = hop_length
        self.win_length = win_length

        print(f"üîß Configuraci√≥n del extractor:")
        print(f"   - Frecuencia de muestreo: {sr} Hz")
        print(f"   - MFCCs: {n_mfcc}")
        print(f"   - Caracter√≠sticas Chroma: {n_chroma}")
        print(f"   - Mel-spectrogram bins: {n_mel}")

    def extract_features(self, audio_path):
        """Extrae caracter√≠sticas completas de un archivo de audio"""
        try:
            # CORRECCI√ìN: Usar res_type='soxr_hq' para mejor compatibilidad
            # o 'polyphase' si 'soxr_hq' no est√° disponible
            try:
                # Intentar con soxr_hq primero
                audio, sr = librosa.load(audio_path, sr=self.sr, res_type='soxr_hq')
            except:
                try:
                    # Fallback a polyphase
                    audio, sr = librosa.load(audio_path, sr=self.sr, res_type='polyphase')
                except:
                    # √öltimo recurso: usar scipy
                    audio, sr = librosa.load(audio_path, sr=self.sr, res_type='scipy')

            # Verificar que el audio no est√© vac√≠o
            if len(audio) == 0:
                print(f"‚ö†Ô∏è Archivo de audio vac√≠o: {os.path.basename(audio_path)}")
                return None

            # Normalizar audio
            audio = librosa.util.normalize(audio)

            # Asegurar longitud m√≠nima
            min_length = self.hop_length * 2
            if len(audio) < min_length:
                audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')

            # 1. MFCCs (40 caracter√≠sticas)
            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=self.n_mfcc,
                                       hop_length=self.hop_length,
                                       win_length=self.win_length)
            mfccs_mean = np.mean(mfccs.T, axis=0)

            # 2. Caracter√≠sticas Chroma (12 caracter√≠sticas)
            stft = np.abs(librosa.stft(audio, hop_length=self.hop_length,
                                     win_length=self.win_length))
            chroma = librosa.feature.chroma_stft(S=stft, sr=sr)
            chroma_mean = np.mean(chroma.T, axis=0)

            # 3. Mel-spectrogram (128 caracter√≠sticas)
            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=self.n_mel,
                                                    hop_length=self.hop_length,
                                                    win_length=self.win_length)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            mel_mean = np.mean(mel_spec_db.T, axis=0)

            # Verificar que no haya NaN o valores infinitos
            mfccs_mean = np.nan_to_num(mfccs_mean, nan=0.0, posinf=0.0, neginf=0.0)
            chroma_mean = np.nan_to_num(chroma_mean, nan=0.0, posinf=0.0, neginf=0.0)
            mel_mean = np.nan_to_num(mel_mean, nan=0.0, posinf=0.0, neginf=0.0)

            # Concatenar todas las caracter√≠sticas (180 total)
            features = np.hstack([mfccs_mean, chroma_mean, mel_mean])

            assert len(features) == 180, f"Error: {len(features)} caracter√≠sticas esperadas 180"

            return features

        except Exception as e:
            print(f"‚ùå Error procesando {os.path.basename(audio_path)}: {e}")
            return None

    def batch_extract(self, audio_files, emotions, dataset_name):
        """Extrae caracter√≠sticas de m√∫ltiples archivos con barra de progreso"""
        print(f"\nüéØ Procesando {len(audio_files)} archivos de {dataset_name}...")

        features_list = []
        emotions_list = []
        failed_files = []

        for audio_file, emotion in tqdm(zip(audio_files, emotions),
                                      total=len(audio_files),
                                      desc=f"Procesando {dataset_name}"):
            features = self.extract_features(audio_file)
            if features is not None:
                features_list.append(features)
                emotions_list.append(emotion)
            else:
                failed_files.append(audio_file)

        print(f"‚úÖ Procesados {len(features_list)}/{len(audio_files)} archivos exitosamente")
        if failed_files:
            print(f"‚ö†Ô∏è {len(failed_files)} archivos fallaron")

        return np.array(features_list), np.array(emotions_list)

# =============================================================================
# PARTE 4: CARGA Y PROCESAMIENTO DE DATASETS (MEJORADA)
# =============================================================================

def load_dataset_files(dataset_path, dataset_name):
    """Carga archivos de audio y extrae etiquetas de emociones"""
    if dataset_path is None or not os.path.exists(dataset_path):
        print(f"‚ö†Ô∏è {dataset_name} no disponible")
        return [], []

    audio_files = []
    emotions = []

    # Buscar archivos de audio recursivamente
    valid_extensions = ['.wav', '.mp3', '.flac', '.ogg']

    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in valid_extensions):
                file_path = os.path.join(root, file)

                # Verificar que el archivo existe y no est√° corrupto
                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    emotion = extract_emotion_from_filename(file, dataset_name)
                    if emotion:
                        audio_files.append(file_path)
                        emotions.append(emotion)

    print(f"üìä {dataset_name}: {len(audio_files)} archivos encontrados")
    if emotions:
        emotion_counts = pd.Series(emotions).value_counts()
        print(f"   Distribuci√≥n: {dict(emotion_counts)}")

    return audio_files, emotions

def extract_emotion_from_filename(filename, dataset_name):
    """Extrae la emoci√≥n del nombre del archivo seg√∫n el dataset"""
    filename_lower = filename.lower()

    if dataset_name == "RAVDESS":
        try:
            # Formato: 03-01-06-01-02-01-12.wav
            parts = filename.split('-')
            if len(parts) >= 3:
                emotion_code = int(parts[2])
                emotion_map = {
                    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',
                    5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'
                }
                emotion = emotion_map.get(emotion_code)
                # Excluir 'calm' para simplificar
                return emotion if emotion and emotion != 'calm' else None
        except:
            return None

    elif dataset_name == "TESS":
        # Buscar emoci√≥n en el nombre del archivo
        emotion_keywords = {
            'angry': 'angry',
            'disgust': 'disgust',
            'fear': 'fearful',
            'happy': 'happy',
            'neutral': 'neutral',
            'sad': 'sad',
            'surprise': 'surprised'
        }

        for keyword, emotion in emotion_keywords.items():
            if keyword in filename_lower:
                return emotion

    elif dataset_name == "MESD":
        # Para MESD, buscar patrones comunes
        emotion_patterns = {
            'angry': 'angry',
            'enojo': 'angry',
            'feliz': 'happy',
            'happy': 'happy',
            'triste': 'sad',
            'sad': 'sad',
            'neutral': 'neutral',
            'miedo': 'fearful',
            'fear': 'fearful',
            'sorpresa': 'surprised',
            'surprise': 'surprised'
        }

        for pattern, emotion in emotion_patterns.items():
            if pattern in filename_lower:
                return emotion

    return None

# Cargar todos los datasets
print("\nüìÅ Cargando archivos de datasets...")
all_audio_files = []
all_emotions = []

for dataset_name, path in dataset_paths.items():
    files, emotions = load_dataset_files(path, dataset_name)
    all_audio_files.extend(files)
    all_emotions.extend(emotions)

print(f"\nüìà Total de archivos cargados: {len(all_audio_files)}")
if all_emotions:
    print(f"üé≠ Distribuci√≥n final de emociones:")
    emotion_distribution = pd.Series(all_emotions).value_counts()
    print(emotion_distribution)
else:
    print("‚ö†Ô∏è No se encontraron archivos de audio v√°lidos")
    print("Verificar que los datasets est√©n descargados correctamente")

# =============================================================================
# PARTE 5: EXTRACCI√ìN MASIVA DE CARACTER√çSTICAS (ROBUSTA)
# =============================================================================

if len(all_audio_files) > 0:
    print("\nüî¨ Iniciando extracci√≥n masiva de caracter√≠sticas...")

    # Inicializar extractor
    extractor = AdvancedFeatureExtractor()

    # Extraer caracter√≠sticas de todos los archivos
    X, y = extractor.batch_extract(all_audio_files, all_emotions, "Todos los datasets")

    print(f"\nüìä Caracter√≠sticas extra√≠das:")
    print(f"   - Forma de X: {X.shape}")
    print(f"   - Forma de y: {y.shape}")
    print(f"   - Emociones √∫nicas: {np.unique(y)}")

    # Verificar que tenemos suficientes datos
    if len(X) < 50:
        print("‚ö†Ô∏è Pocos datos disponibles. Considerar:")
        print("   - Verificar que los datasets est√©n descargados")
        print("   - Ajustar la funci√≥n de extracci√≥n de emociones")
        print("   - Usar datasets locales si est√°n disponibles")
else:
    print("‚ùå No se encontraron archivos de audio para procesar")
    print("Por favor, verificar la configuraci√≥n de datasets")

# =============================================================================
# PARTE 6: PREPROCESAMIENTO AVANZADO
# =============================================================================

# Solo continuar si tenemos datos suficientes
if 'X' in locals() and len(X) > 50:

    print("\n‚öôÔ∏è Aplicando preprocesamiento avanzado...")

    # Codificar etiquetas
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    emotion_classes = label_encoder.classes_
    n_classes = len(emotion_classes)

    print(f"üè∑Ô∏è Clases codificadas: {emotion_classes}")
    print(f"üìù N√∫mero de clases: {n_classes}")

    # Verificar que tenemos suficientes clases
    if n_classes < 2:
        print("‚ùå Error: Se necesitan al menos 2 clases para entrenar")
        exit()

    # Divisi√≥n estratificada de datos ANTES de la estandarizaci√≥n
    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(
        X, y_encoded,
        test_size=0.2,
        random_state=42,
        stratify=y_encoded
    )

    X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(
        X_train, y_train_encoded,
        test_size=0.2,
        random_state=42,
        stratify=y_train_encoded
    )

    print(f"üî¢ Divisi√≥n de datos:")
    print(f"   - Entrenamiento: {X_train.shape[0]} muestras")
    print(f"   - Validaci√≥n: {X_val.shape[0]} muestras")
    print(f"   - Prueba: {X_test.shape[0]} muestras")

    # Estandarizaci√≥n
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    print("‚úÖ Estandarizaci√≥n completada")

    # =============================================================================
    # PARTE 6.5: AN√ÅLISIS LDA (NUEVO)
    # =============================================================================

    print("\nüéØ Aplicando An√°lisis Discriminante Lineal (LDA)...")

    # Calcular el n√∫mero m√°ximo de componentes LDA
    max_lda_components = min(n_classes - 1, X_train_scaled.shape[1])
    print(f"üìê Componentes LDA m√°ximos posibles: {max_lda_components}")

    # Crear y ajustar LDA
    lda = LinearDiscriminantAnalysis(n_components=max_lda_components)
    X_train_lda = lda.fit_transform(X_train_scaled, y_train_encoded)
    X_val_lda = lda.transform(X_val_scaled)
    X_test_lda = lda.transform(X_test_scaled)

    print(f"‚úÖ LDA aplicado exitosamente")
    print(f"   - Forma original: {X_train_scaled.shape}")
    print(f"   - Forma LDA: {X_train_lda.shape}")
    print(f"   - Varianza explicada por componente: {lda.explained_variance_ratio_}")
    print(f"   - Varianza total explicada: {np.sum(lda.explained_variance_ratio_):.4f}")

    # =============================================================================
    # CONFIGURACI√ìN DE CARPETAS DE RESULTADOS
    # =============================================================================

    print("\nüìÅ Configurando carpetas de resultados...")

    # Crear estructura de carpetas
    results_dir = "/content/Results"
    model_dir = os.path.join(results_dir, "Model")
    lda_dir = os.path.join(results_dir, "LDA") 
    plots_dir = os.path.join(results_dir, "Plots")

    for directory in [results_dir, model_dir, lda_dir, plots_dir]:
        os.makedirs(directory, exist_ok=True)
        print(f"‚úÖ Carpeta creada: {directory}")

    # =============================================================================
    # VISUALIZACI√ìN AVANZADA DEL LDA
    # =============================================================================

    print("\nüìä Creando visualizaciones LDA...")

    # Configurar estilo de matplotlib
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")

    # Crear colores √∫nicos para cada emoci√≥n
    colors = plt.cm.Set3(np.linspace(0, 1, n_classes))
    emotion_colors = {emotion: colors[i] for i, emotion in enumerate(emotion_classes)}

    # 1. Gr√°fico 2D de las primeras dos componentes LDA
    if X_train_lda.shape[1] >= 2:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('An√°lisis Discriminante Lineal (LDA) - Visualizaciones', fontsize=16, fontweight='bold')

        # Subplot 1: Scatter plot 2D
        ax1 = axes[0, 0]
        for i, emotion in enumerate(emotion_classes):
            mask = y_train_encoded == i
            ax1.scatter(X_train_lda[mask, 0], X_train_lda[mask, 1], 
                       c=[emotion_colors[emotion]], label=emotion, alpha=0.7, s=50)

        ax1.set_xlabel('Primera Componente LDA', fontsize=12)
        ax1.set_ylabel('Segunda Componente LDA', fontsize=12)
        ax1.set_title('Distribuci√≥n de Emociones en Espacio LDA (2D)', fontweight='bold')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(True, alpha=0.3)

        # Subplot 2: Histograma de la primera componente
        ax2 = axes[0, 1]
        for i, emotion in enumerate(emotion_classes):
            mask = y_train_encoded == i
            ax2.hist(X_train_lda[mask, 0], alpha=0.6, label=emotion, bins=20,
                    color=emotion_colors[emotion])

        ax2.set_xlabel('Primera Componente LDA', fontsize=12)
        ax2.set_ylabel('Frecuencia', fontsize=12)
        ax2.set_title('Distribuci√≥n de la Primera Componente LDA', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Subplot 3: Boxplot de componentes LDA
        ax3 = axes[1, 0]
        lda_df = pd.DataFrame(X_train_lda[:, :min(4, X_train_lda.shape[1])], 
                             columns=[f'LDA_{i+1}' for i in range(min(4, X_train_lda.shape[1]))])
        lda_df['Emotion'] = [emotion_classes[i] for i in y_train_encoded]

        # Crear boxplot
        lda_melted = lda_df.melt(id_vars=['Emotion'], var_name='Componente', value_name='Valor')
        sns.boxplot(data=lda_melted, x='Componente', y='Valor', hue='Emotion', ax=ax3)
        ax3.set_title('Distribuci√≥n de Componentes LDA por Emoci√≥n', fontweight='bold')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # Subplot 4: Varianza explicada
        ax4 = axes[1, 1]
        component_range = range(1, len(lda.explained_variance_ratio_) + 1)
        ax4.bar(component_range, lda.explained_variance_ratio_, alpha=0.7, color='skyblue')
        ax4.plot(component_range, np.cumsum(lda.explained_variance_ratio_), 
                color='red', marker='o', linewidth=2, markersize=6)
        ax4.set_xlabel('Componente LDA', fontsize=12)
        ax4.set_ylabel('Varianza Explicada', fontsize=12)
        ax4.set_title('Varianza Explicada por Componente LDA', fontweight='bold')
        ax4.set_xticks(component_range)
        ax4.grid(True, alpha=0.3)

        # Agregar texto con varianza total
        total_variance = np.sum(lda.explained_variance_ratio_)
        ax4.text(0.7, 0.95, f'Varianza Total: {total_variance:.3f}', 
                transform=ax4.transAxes, fontsize=10, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="wheat", alpha=0.8))

        plt.tight_layout()
        
        # Guardar figura LDA principal
        lda_main_path = os.path.join(lda_dir, "lda_analysis_main.png")
        plt.savefig(lda_main_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Guardado: {lda_main_path}")
        plt.show()

    # 2. Visualizaci√≥n 3D (si hay al menos 3 componentes)
    if X_train_lda.shape[1] >= 3:
        print("üìä Creando visualizaci√≥n 3D con Plotly...")

        # Crear DataFrame para Plotly
        lda_3d_df = pd.DataFrame({
            'LDA1': X_train_lda[:, 0],
            'LDA2': X_train_lda[:, 1],
            'LDA3': X_train_lda[:, 2],
            'Emotion': [emotion_classes[i] for i in y_train_encoded]
        })

        # Crear gr√°fico 3D interactivo
        fig_3d = px.scatter_3d(lda_3d_df, x='LDA1', y='LDA2', z='LDA3', 
                              color='Emotion', 
                              title='An√°lisis LDA - Visualizaci√≥n 3D Interactiva',
                              labels={'LDA1': 'Primera Componente LDA',
                                     'LDA2': 'Segunda Componente LDA', 
                                     'LDA3': 'Tercera Componente LDA'})

        fig_3d.update_layout(
            scene=dict(
                xaxis_title='Primera Componente LDA',
                yaxis_title='Segunda Componente LDA',
                zaxis_title='Tercera Componente LDA'
            ),
            font=dict(size=12),
            width=800,
            height=600
        )

        # Guardar gr√°fico 3D
        lda_3d_path = os.path.join(lda_dir, "lda_3d_visualization.html")
        fig_3d.write_html(lda_3d_path)
        print(f"üíæ Guardado: {lda_3d_path}")
        
        fig_3d.show()

    # 3. Matriz de correlaci√≥n de componentes LDA
    print("üìä Analizando correlaciones entre componentes LDA...")

    if X_train_lda.shape[1] > 1:
        plt.figure(figsize=(10, 8))
        
        # Crear DataFrame con componentes LDA
        lda_components_df = pd.DataFrame(
            X_train_lda, 
            columns=[f'LDA_Comp_{i+1}' for i in range(X_train_lda.shape[1])]
        )
        
        # Calcular matriz de correlaci√≥n
        correlation_matrix = lda_components_df.corr()
        
        # Crear heatmap
        mask = np.triu(np.ones_like(correlation_matrix))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        
        plt.title('Matriz de Correlaci√≥n - Componentes LDA', fontsize=16, fontweight='bold')
        
        # Guardar matriz de correlaci√≥n
        correlation_path = os.path.join(lda_dir, "lda_correlation_matrix.png")
        plt.savefig(correlation_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Guardado: {correlation_path}")
        
        plt.tight_layout()
        plt.show()

    # 4. An√°lisis de separabilidad por pares de emociones
    print("üìä Analizando separabilidad entre emociones...")

    # Calcular distancias entre centroides de clases en espacio LDA
    centroids = []
    for i in range(n_classes):
        mask = y_train_encoded == i
        centroid = np.mean(X_train_lda[mask], axis=0)
        centroids.append(centroid)

    centroids = np.array(centroids)

    # Calcular matriz de distancias entre centroides
    from scipy.spatial.distance import pdist, squareform
    distances = squareform(pdist(centroids))

    plt.figure(figsize=(10, 8))
    sns.heatmap(distances, annot=True, xticklabels=emotion_classes, 
               yticklabels=emotion_classes, cmap='viridis', fmt='.2f')
    plt.title('Matriz de Distancias entre Centroides de Emociones (Espacio LDA)', 
              fontsize=14, fontweight='bold')
    plt.xlabel('Emociones', fontsize=12)
    plt.ylabel('Emociones', fontsize=12)
    
    # Guardar matriz de distancias
    distances_path = os.path.join(lda_dir, "emotion_centroids_distances.png")
    plt.savefig(distances_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {distances_path}")
    
    plt.tight_layout()
    
    # Guardar historial de entrenamiento
    training_history_path = os.path.join(model_dir, "training_history.png")
    plt.savefig(training_history_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {training_history_path}")
    
    plt.show()

    # Reporte de LDA
    print(f"\nüìã Reporte de LDA:")
    print(f"   - Dimensiones originales: {X_train_scaled.shape[1]}")
    print(f"   - Dimensiones despu√©s de LDA: {X_train_lda.shape[1]}")
    print(f"   - Reducci√≥n de dimensionalidad: {((X_train_scaled.shape[1] - X_train_lda.shape[1]) / X_train_scaled.shape[1] * 100):.1f}%")
    print(f"   - Varianza retenida: {np.sum(lda.explained_variance_ratio_) * 100:.1f}%")

    # =============================================================================
    # GR√ÅFICOS ADICIONALES DE AN√ÅLISIS
    # =============================================================================

    print("\nüìä Creando gr√°ficos adicionales de an√°lisis...")

    # 1. Distribuci√≥n de emociones en el dataset
    plt.figure(figsize=(12, 6))
    emotion_counts = pd.Series(y).value_counts()
    
    plt.subplot(1, 2, 1)
    bars = plt.bar(emotion_counts.index, emotion_counts.values, 
                   color=[emotion_colors[emotion] for emotion in emotion_counts.index])
    plt.title('Distribuci√≥n de Emociones en el Dataset', fontweight='bold')
    plt.xlabel('Emociones')
    plt.ylabel('N√∫mero de Muestras')
    plt.xticks(rotation=45)
    
    # Agregar valores en las barras
    for bar, value in zip(bars, emotion_counts.values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                str(value), ha='center', va='bottom')
    
    plt.subplot(1, 2, 2)
    plt.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%',
            colors=[emotion_colors[emotion] for emotion in emotion_counts.index])
    plt.title('Distribuci√≥n Porcentual de Emociones', fontweight='bold')
    
    plt.tight_layout()
    
    # Guardar distribuci√≥n de emociones
    emotion_dist_path = os.path.join(plots_dir, "emotion_distribution.png")
    plt.savefig(emotion_dist_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {emotion_dist_path}")
    plt.show()

    # 2. Comparaci√≥n de caracter√≠sticas antes y despu√©s de LDA
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Comparaci√≥n: Caracter√≠sticas Originales vs LDA', fontsize=16, fontweight='bold')

    # Antes de LDA - Primera componente principal
    pca_temp = PCA(n_components=2)
    X_pca = pca_temp.fit_transform(X_train_scaled)
    
    ax1 = axes[0, 0]
    for i, emotion in enumerate(emotion_classes):
        mask = y_train_encoded == i
        ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=[emotion_colors[emotion]], label=emotion, alpha=0.6, s=30)
    ax1.set_title('Caracter√≠sticas Originales (PCA)', fontweight='bold')
    ax1.set_xlabel('Primera Componente PCA')
    ax1.set_ylabel('Segunda Componente PCA')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    ax1.grid(True, alpha=0.3)

    # Despu√©s de LDA
    ax2 = axes[0, 1]
    for i, emotion in enumerate(emotion_classes):
        mask = y_train_encoded == i
        if X_train_lda.shape[1] >= 2:
            ax2.scatter(X_train_lda[mask, 0], X_train_lda[mask, 1], 
                       c=[emotion_colors[emotion]], label=emotion, alpha=0.6, s=30)
        else:
            ax2.scatter(X_train_lda[mask, 0], np.zeros(np.sum(mask)), 
                       c=[emotion_colors[emotion]], label=emotion, alpha=0.6, s=30)
    ax2.set_title('Caracter√≠sticas LDA', fontweight='bold')
    ax2.set_xlabel('Primera Componente LDA')
    ax2.set_ylabel('Segunda Componente LDA' if X_train_lda.shape[1] >= 2 else 'Valor Constante')
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    ax2.grid(True, alpha=0.3)

    # Comparaci√≥n de varianza explicada
    ax3 = axes[1, 0]
    pca_variance = pca_temp.explained_variance_ratio_
    lda_variance = lda.explained_variance_ratio_
    
    x_pos = np.arange(max(len(pca_variance), len(lda_variance)))
    width = 0.35
    
    ax3.bar(x_pos[:len(pca_variance)] - width/2, pca_variance, width, 
           label='PCA', alpha=0.7, color='lightblue')
    ax3.bar(x_pos[:len(lda_variance)] + width/2, lda_variance, width, 
           label='LDA', alpha=0.7, color='lightcoral')
    
    ax3.set_title('Comparaci√≥n: Varianza Explicada PCA vs LDA', fontweight='bold')
    ax3.set_xlabel('Componente')
    ax3.set_ylabel('Varianza Explicada')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # M√©tricas de separabilidad
    ax4 = axes[1, 1]
    
    # Calcular m√©tricas de separabilidad
    from sklearn.metrics import silhouette_score
    
    try:
        silhouette_original = silhouette_score(X_train_scaled[:1000], y_train_encoded[:1000])  # Muestra para velocidad
        silhouette_lda = silhouette_score(X_train_lda[:1000], y_train_encoded[:1000])
        
        metrics = ['Silhouette Score']
        original_scores = [silhouette_original]
        lda_scores = [silhouette_lda]
        
        x_pos = np.arange(len(metrics))
        ax4.bar(x_pos - width/2, original_scores, width, label='Original', alpha=0.7, color='lightblue')
        ax4.bar(x_pos + width/2, lda_scores, width, label='LDA', alpha=0.7, color='lightcoral')
        
        ax4.set_title('M√©tricas de Separabilidad', fontweight='bold')
        ax4.set_ylabel('Score')
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(metrics)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        print(f"üìä M√©tricas de separabilidad:")
        print(f"   - Silhouette Score (Original): {silhouette_original:.3f}")
        print(f"   - Silhouette Score (LDA): {silhouette_lda:.3f}")
        print(f"   - Mejora: {((silhouette_lda - silhouette_original) / silhouette_original * 100):.1f}%")
        
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudieron calcular m√©tricas de separabilidad: {e}")
        ax4.text(0.5, 0.5, 'M√©tricas no disponibles', ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('M√©tricas de Separabilidad', fontweight='bold')

    plt.tight_layout()
    
    # Guardar comparaci√≥n
    comparison_path = os.path.join(lda_dir, "pca_vs_lda_comparison.png")
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {comparison_path}")
    plt.show()

    # =============================================================================
    # PARTE 7: ARQUITECTURA AVANZADA DE CNN 1D (MODIFICADA PARA LDA)
    # =============================================================================

    print("\nüèóÔ∏è Construyendo arquitectura avanzada de CNN 1D con caracter√≠sticas LDA...")

    # Convertir etiquetas a categorical
    y_train_categorical = to_categorical(y_train_encoded, num_classes=n_classes)
    y_val_categorical = to_categorical(y_val_encoded, num_classes=n_classes)
    y_test_categorical = to_categorical(y_test_encoded, num_classes=n_classes)

    # Reformatear datos LDA para CNN 1D
    X_train_cnn = np.expand_dims(X_train_lda, axis=2)
    X_val_cnn = np.expand_dims(X_val_lda, axis=2)
    X_test_cnn = np.expand_dims(X_test_lda, axis=2)

    print(f"üéØ Datos preparados para CNN 1D con LDA:")
    print(f"   - Forma de entrada: {X_train_cnn.shape}")

    def create_advanced_cnn_model_lda(input_shape, num_classes,
                                    conv1_filters=64, conv2_filters=32,
                                    kernel_size=3, dropout_rate=0.3,
                                    use_batch_norm=True):
        """Crea un modelo CNN 1D optimizado para caracter√≠sticas LDA"""

        model = Sequential([
            # Capa de entrada
            Input(shape=input_shape, name='input_lda_features'),

            # Primer bloque convolucional (ajustado para LDA)
            Conv1D(filters=conv1_filters,
                   kernel_size=kernel_size,
                   padding='same',
                   activation='relu',
                   name='conv1d_1'),

            BatchNormalization(name='batch_norm_1') if use_batch_norm else tf.keras.layers.Lambda(lambda x: x),

            MaxPooling1D(pool_size=2, name='maxpool_1'),
            Dropout(dropout_rate, name='dropout_1'),

            # Segundo bloque convolucional
            Conv1D(filters=conv2_filters,
                   kernel_size=kernel_size,
                   padding='same',
                   activation='relu',
                   name='conv1d_2'),

            BatchNormalization(name='batch_norm_2') if use_batch_norm else tf.keras.layers.Lambda(lambda x: x),

            MaxPooling1D(pool_size=2, name='maxpool_2'),
            Dropout(dropout_rate, name='dropout_2'),

            # Global Average Pooling
            GlobalAveragePooling1D(name='global_avg_pool'),

            # Capa densa intermedia (reducida para LDA)
            Dense(32, activation='relu', name='dense_intermediate'),
            Dropout(dropout_rate * 0.5, name='dropout_final'),

            # Capa de salida
            Dense(num_classes, activation='softmax', name='output_emotions')
        ])

        return model

    # Crear modelo optimizado para LDA
    model = create_advanced_cnn_model_lda(
        input_shape=(X_train_lda.shape[1], 1),
        num_classes=n_classes,
        conv1_filters=64,
        conv2_filters=32,
        kernel_size=3,
        dropout_rate=0.3,
        use_batch_norm=True
    )

    print("üîç Resumen del modelo CNN 1D con LDA:")
    model.summary()

    # Calcular par√°metros
    total_params = model.count_params()
    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])

    print(f"\nüìä An√°lisis de par√°metros:")
    print(f"   - Par√°metros totales: {total_params:,}")
    print(f"   - Par√°metros entrenables: {trainable_params:,}")
    print(f"   - Reducci√≥n vs modelo original: ~{((128*180 - total_params) / (128*180) * 100):.1f}%")

    # =============================================================================
    # PARTE 8: CONFIGURACI√ìN DEL OPTIMIZADOR
    # =============================================================================

    print("\n‚ö° Configurando optimizador ADAM...")

    adam_optimizer = Adam(
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )

    # Compilar modelo
    model.compile(
        optimizer=adam_optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    print("‚úÖ Modelo compilado exitosamente")

    # =============================================================================
    # PARTE 9: CALLBACKS
    # =============================================================================

    print("\nüìã Configurando callbacks...")

    callbacks_list = [
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)
    ]

    # =============================================================================
    # PARTE 10: ENTRENAMIENTO
    # =============================================================================

    print("\nüöÄ Iniciando entrenamiento con caracter√≠sticas LDA...")

    EPOCHS = 100
    BATCH_SIZE = 32

    print(f"‚öôÔ∏è Configuraci√≥n de entrenamiento:")
    print(f"   - √âpocas: {EPOCHS}")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - Caracter√≠sticas de entrada: {X_train_lda.shape[1]} (LDA)")

    history = model.fit(
        X_train_cnn, y_train_categorical,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_val_cnn, y_val_categorical),
        callbacks=callbacks_list,
        verbose=1
    )

    # =============================================================================
    # PARTE 11: EVALUACI√ìN COMPLETA
    # =============================================================================

    print("\nüìà Evaluando modelo...")

    # Evaluaci√≥n en conjunto de prueba
    test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test_categorical, verbose=0)
    print(f"üìä Resultados finales:")
    print(f"   - P√©rdida en prueba: {test_loss:.4f}")
    print(f"   - Precisi√≥n en prueba: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

    # Predicciones detalladas
    y_pred_proba = model.predict(X_test_cnn, verbose=0)
    y_pred = np.argmax(y_pred_proba, axis=1)
    y_true = y_test_encoded

    # Reporte de clasificaci√≥n
    print(f"\nüìã Reporte de clasificaci√≥n detallado:")
    print(classification_report(y_true, y_pred, target_names=emotion_classes))

    # Matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=emotion_classes, yticklabels=emotion_classes)
    plt.title('Matriz de Confusi√≥n - Modelo CNN 1D con LDA', fontsize=16, fontweight='bold')
    plt.xlabel('Predicci√≥n', fontsize=12)
    plt.ylabel('Valor Real', fontsize=12)
    
    # Guardar matriz de confusi√≥n
    confusion_matrix_path = os.path.join(model_dir, "confusion_matrix.png")
    plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {confusion_matrix_path}")
    
    plt.tight_layout()
    plt.show()

    # Visualizar historial de entrenamiento
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Precisi√≥n
    axes[0].plot(history.history['accuracy'], label='Entrenamiento', linewidth=2)
    axes[0].plot(history.history['val_accuracy'], label='Validaci√≥n', linewidth=2)
    axes[0].set_title('Precisi√≥n del Modelo', fontweight='bold')
    axes[0].set_xlabel('√âpoca')
    axes[0].set_ylabel('Precisi√≥n')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # P√©rdida
    axes[1].plot(history.history['loss'], label='Entrenamiento', linewidth=2)
    axes[1].plot(history.history['val_loss'], label='Validaci√≥n', linewidth=2)
    axes[1].set_title('P√©rdida del Modelo', fontweight='bold')
    axes[1].set_xlabel('√âpoca')
    axes[1].set_ylabel('P√©rdida')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # =============================================================================
    # PARTE 12: GUARDADO DE MODELOS Y RESULTADOS
    # =============================================================================

    print("\nüíæ Guardando modelos y resultados...")

    # Guardar modelo CNN en carpeta Model (formato nativo Keras)
    model_path = os.path.join(model_dir, "emotion_cnn_lda_model.keras")
    model.save(model_path)
    print(f"‚úÖ Modelo CNN guardado: {model_path}")
    
    # Tambi√©n guardar en formato H5 para compatibilidad
    model_h5_path = os.path.join(model_dir, "emotion_cnn_lda_model.h5")
    model.save(model_h5_path)
    print(f"‚úÖ Modelo CNN (H5) guardado: {model_h5_path}")

    # Guardar objetos de preprocesamiento en carpeta Model
    import pickle

    preprocessing_objects = {
        'scaler': scaler,
        'lda': lda,
        'label_encoder': label_encoder,
        'emotion_classes': emotion_classes,
        'feature_extractor_config': {
            'sr': extractor.sr,
            'n_mfcc': extractor.n_mfcc,
            'n_chroma': extractor.n_chroma,
            'n_mel': extractor.n_mel,
            'hop_length': extractor.hop_length,
            'win_length': extractor.win_length
        }
    }

    preprocessing_path = os.path.join(model_dir, "preprocessing_objects.pkl")
    with open(preprocessing_path, 'wb') as f:
        pickle.dump(preprocessing_objects, f)
    print(f"‚úÖ Objetos de preprocesamiento guardados: {preprocessing_path}")

    # Funci√≥n helper para convertir numpy types a tipos nativos de Python
    def convert_numpy_types(obj):
        """Convierte recursivamente tipos numpy a tipos nativos de Python para JSON"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(convert_numpy_types(item) for item in obj)
        else:
            return obj

    # Guardar resultados de evaluaci√≥n en carpeta Model (con conversi√≥n numpy)
    results = {
        'test_accuracy': float(test_accuracy),  # Convertir expl√≠citamente
        'test_loss': float(test_loss),
        'classification_report': classification_report(y_true, y_pred, target_names=emotion_classes, output_dict=True),
        'confusion_matrix': cm.tolist(),  # Convertir array numpy a lista
        'lda_explained_variance': lda.explained_variance_ratio_.tolist(),
        'lda_total_variance': float(np.sum(lda.explained_variance_ratio_)),
        'training_history': {
            'loss': [float(x) for x in history.history['loss']],
            'accuracy': [float(x) for x in history.history['accuracy']],
            'val_loss': [float(x) for x in history.history['val_loss']],
            'val_accuracy': [float(x) for x in history.history['val_accuracy']]
        },
        'model_summary': {
            'total_params': int(total_params),
            'trainable_params': int(trainable_params),
            'input_shape': int(X_train_lda.shape[1]),
            'output_classes': int(n_classes),
            'epochs_trained': len(history.history['loss']),
            'best_val_accuracy': float(max(history.history['val_accuracy']))
        }
    }

    # Aplicar conversi√≥n recursiva para asegurar compatibilidad JSON
    results = convert_numpy_types(results)

    results_path = os.path.join(model_dir, "model_results.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Resultados guardados: {results_path}")

    # Guardar informaci√≥n detallada de LDA en carpeta LDA (con conversi√≥n numpy)
    lda_results = {
        'original_dimensions': int(X_train_scaled.shape[1]),
        'lda_dimensions': int(X_train_lda.shape[1]),
        'dimension_reduction_percentage': float((X_train_scaled.shape[1] - X_train_lda.shape[1]) / X_train_scaled.shape[1] * 100),
        'explained_variance_ratio': lda.explained_variance_ratio_.tolist(),
        'total_variance_explained': float(np.sum(lda.explained_variance_ratio_)),
        'emotion_classes': emotion_classes.tolist(),
        'centroids_distances': distances.tolist()
    }

    # Aplicar conversi√≥n recursiva
    lda_results = convert_numpy_types(lda_results)

    lda_results_path = os.path.join(lda_dir, "lda_analysis_results.json")
    with open(lda_results_path, 'w') as f:
        json.dump(lda_results, f, indent=2)
    print(f"‚úÖ Resultados LDA guardados: {lda_results_path}")

    # =============================================================================
    # VISUALIZACIONES ADICIONALES DEL MODELO CNN
    # =============================================================================

    print("\nüîç Creando visualizaciones del modelo CNN...")

    # 1. Visualizaci√≥n de filtros aprendidos
    def plot_learned_filters(model, layer_name, save_path):
        """Visualiza los filtros aprendidos de una capa convolucional"""
        try:
            # Obtener pesos de la capa
            for layer in model.layers:
                if layer.name == layer_name and 'conv' in layer.name.lower():
                    weights = layer.get_weights()[0]  # [kernel_size, input_channels, filters]
                    
                    # Configurar subplot
                    n_filters = min(weights.shape[-1], 16)  # M√°ximo 16 filtros
                    cols = 4
                    rows = (n_filters + cols - 1) // cols
                    
                    fig, axes = plt.subplots(rows, cols, figsize=(12, 3*rows))
                    fig.suptitle(f'Filtros Aprendidos - {layer_name}', fontsize=14, fontweight='bold')
                    
                    if rows == 1:
                        axes = axes.reshape(1, -1)
                    
                    for i in range(n_filters):
                        row = i // cols
                        col = i % cols
                        
                        # Visualizar filtro
                        filter_weights = weights[:, 0, i]  # [kernel_size]
                        axes[row, col].plot(filter_weights, linewidth=2)
                        axes[row, col].set_title(f'Filtro {i+1}')
                        axes[row, col].grid(True, alpha=0.3)
                        axes[row, col].set_xlabel('Posici√≥n')
                        axes[row, col].set_ylabel('Peso')
                    
                    # Ocultar subplots vac√≠os
                    for i in range(n_filters, rows * cols):
                        row = i // cols
                        col = i % cols
                        axes[row, col].axis('off')
                    
                    plt.tight_layout()
                    plt.savefig(save_path, dpi=300, bbox_inches='tight')
                    print(f"üíæ Guardado: {save_path}")
                    plt.show()
                    break
        except Exception as e:
            print(f"‚ö†Ô∏è Error visualizando filtros de {layer_name}: {e}")

    # Visualizar filtros de ambas capas convolucionales
    conv1_filters_path = os.path.join(model_dir, "conv1d_filters_layer1.png")
    plot_learned_filters(model, 'conv1d_1', conv1_filters_path)
    
    conv2_filters_path = os.path.join(model_dir, "conv1d_filters_layer2.png")
    plot_learned_filters(model, 'conv1d_2', conv2_filters_path)

    # 2. An√°lisis de activaciones
    def analyze_model_activations(model, X_sample, y_sample, save_dir):
        """Analiza las activaciones del modelo para muestras de ejemplo"""
        try:
            # Crear modelo para extraer activaciones intermedias
            layer_outputs = [layer.output for layer in model.layers if 'conv1d' in layer.name or 'dense' in layer.name]
            if layer_outputs:
                activation_model = Model(inputs=model.input, outputs=layer_outputs)
                
                # Seleccionar una muestra de cada clase
                sample_indices = []
                for class_idx in range(n_classes):
                    class_samples = np.where(y_sample == class_idx)[0]
                    if len(class_samples) > 0:
                        sample_indices.append(class_samples[0])
                
                if sample_indices:
                    X_samples = X_sample[sample_indices]
                    y_samples = y_sample[sample_indices]
                    
                    # Obtener activaciones
                    activations = activation_model.predict(X_samples, verbose=0)
                    
                    # Visualizar activaciones de capas convolucionales
                    for layer_idx, activation in enumerate(activations):
                        if len(activation.shape) == 3:  # Conv1D layers
                            fig, axes = plt.subplots(len(sample_indices), 1, figsize=(15, 2*len(sample_indices)))
                            if len(sample_indices) == 1:
                                axes = [axes]
                            
                            fig.suptitle(f'Activaciones - Capa {layer_idx+1}', fontsize=14, fontweight='bold')
                            
                            for sample_idx, (sample_activation, sample_class) in enumerate(zip(activation, y_samples)):
                                # Visualizar activaci√≥n como heatmap
                                axes[sample_idx].imshow(sample_activation.T, cmap='viridis', aspect='auto')
                                axes[sample_idx].set_title(f'Clase: {emotion_classes[sample_class]}')
                                axes[sample_idx].set_xlabel('Posici√≥n Temporal')
                                axes[sample_idx].set_ylabel('Canal de Activaci√≥n')
                            
                            plt.tight_layout()
                            activation_path = os.path.join(save_dir, f"activations_layer_{layer_idx+1}.png")
                            plt.savefig(activation_path, dpi=300, bbox_inches='tight')
                            print(f"üíæ Guardado: {activation_path}")
                            plt.show()
                            
        except Exception as e:
            print(f"‚ö†Ô∏è Error analizando activaciones: {e}")

    # Analizar activaciones con muestras de prueba
    analyze_model_activations(model, X_test_cnn[:min(len(X_test_cnn), 20)], 
                            y_test_encoded[:min(len(y_test_encoded), 20)], model_dir)

    # 3. Matriz de predicciones con confianza
    plt.figure(figsize=(12, 8))
    
    # Calcular m√©tricas por clase
    from sklearn.metrics import precision_recall_fscore_support
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    
    # Crear subplot con m√©tricas
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('An√°lisis Detallado del Rendimiento del Modelo', fontsize=16, fontweight='bold')
    
    # Subplot 1: M√©tricas por clase
    ax1 = axes[0, 0]
    x_pos = np.arange(len(emotion_classes))
    width = 0.25
    
    ax1.bar(x_pos - width, precision, width, label='Precisi√≥n', alpha=0.8)
    ax1.bar(x_pos, recall, width, label='Recall', alpha=0.8)
    ax1.bar(x_pos + width, f1, width, label='F1-Score', alpha=0.8)
    
    ax1.set_xlabel('Emociones')
    ax1.set_ylabel('Score')
    ax1.set_title('M√©tricas por Clase')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(emotion_classes, rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Subplot 2: Distribuci√≥n de confianza de predicciones
    ax2 = axes[0, 1]
    confidence_scores = np.max(y_pred_proba, axis=1)
    correct_predictions = (y_pred == y_true)
    
    ax2.hist(confidence_scores[correct_predictions], bins=20, alpha=0.7, 
            label='Predicciones Correctas', color='green')
    ax2.hist(confidence_scores[~correct_predictions], bins=20, alpha=0.7, 
            label='Predicciones Incorrectas', color='red')
    ax2.set_xlabel('Confianza de Predicci√≥n')
    ax2.set_ylabel('Frecuencia')
    ax2.set_title('Distribuci√≥n de Confianza')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Subplot 3: Curva de precisi√≥n vs recall
    ax3 = axes[1, 0]
    for i, emotion in enumerate(emotion_classes):
        y_true_binary = (y_true == i).astype(int)
        y_score = y_pred_proba[:, i]
        
        from sklearn.metrics import precision_recall_curve, auc
        precision_curve, recall_curve, _ = precision_recall_curve(y_true_binary, y_score)
        auc_score = auc(recall_curve, precision_curve)
        
        ax3.plot(recall_curve, precision_curve, 
                label=f'{emotion} (AUC={auc_score:.2f})', alpha=0.8)
    
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precisi√≥n')
    ax3.set_title('Curvas Precisi√≥n-Recall por Clase')
    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax3.grid(True, alpha=0.3)
    
    # Subplot 4: Errores por emoci√≥n
    ax4 = axes[1, 1]
    error_counts = []
    for i in range(n_classes):
        errors = np.sum((y_true == i) & (y_pred != i))
        error_counts.append(errors)
    
    bars = ax4.bar(emotion_classes, error_counts, color='lightcoral', alpha=0.7)
    ax4.set_xlabel('Emociones')
    ax4.set_ylabel('N√∫mero de Errores')
    ax4.set_title('Errores de Clasificaci√≥n por Emoci√≥n')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    # Agregar valores en las barras
    for bar, value in zip(bars, error_counts):
        if value > 0:
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(value), ha='center', va='bottom')
    
    plt.tight_layout()
    
    # Guardar an√°lisis de rendimiento
    performance_path = os.path.join(model_dir, "detailed_performance_analysis.png")
    plt.savefig(performance_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Guardado: {performance_path}")
    plt.show()

    # Crear reporte de texto completo
    report_content = f"""
    REPORTE COMPLETO - RECONOCIMIENTO DE EMOCIONES CNN 1D + LDA
    ===========================================================
    
    CONFIGURACI√ìN DEL MODELO:
    - Arquitectura: CNN 1D con caracter√≠sticas LDA
    - Caracter√≠sticas de entrada: {X_train_lda.shape[1]} (despu√©s de LDA)
    - Caracter√≠sticas originales: {X_train_scaled.shape[1]}
    - Reducci√≥n dimensional: {((X_train_scaled.shape[1] - X_train_lda.shape[1]) / X_train_scaled.shape[1] * 100):.1f}%
    - N√∫mero de clases: {n_classes}
    - Clases: {', '.join(emotion_classes)}
    
    AN√ÅLISIS LDA:
    - Componentes LDA: {X_train_lda.shape[1]}
    - Varianza explicada total: {np.sum(lda.explained_variance_ratio_)*100:.2f}%
    - Varianza por componente: {[f'{var:.3f}' for var in lda.explained_variance_ratio_]}
    
    ARQUITECTURA DEL MODELO:
    - Par√°metros totales: {total_params:,}
    - Par√°metros entrenables: {trainable_params:,}
    - Capas convolucionales: 2 (64 y 32 filtros)
    - Kernel size: 3
    - Dropout rate: 0.3
    - Global Average Pooling + Dense(32) + Dense({n_classes})
    
    ENTRENAMIENTO:
    - √âpocas totales configuradas: {EPOCHS}
    - √âpocas realmente entrenadas: {len(history.history['loss'])}
    - Batch size: {BATCH_SIZE}
    - Optimizador: Adam (lr=0.001)
    - Early stopping: Activado (patience=15)
    - Reduce LR: Activado (patience=7)
    
    RESULTADOS FINALES:
    - Precisi√≥n en prueba: {test_accuracy*100:.2f}%
    - P√©rdida en prueba: {test_loss:.4f}
    - Mejor precisi√≥n en validaci√≥n: {max(history.history['val_accuracy'])*100:.2f}%
    
    M√âTRICAS POR CLASE:
    {classification_report(y_true, y_pred, target_names=emotion_classes)}
    
    DISTRIBUCI√ìN DE DATOS:
    - Total de muestras procesadas: {len(X)}
    - Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)
    - Validaci√≥n: {X_val.shape[0]} muestras ({X_val.shape[0]/len(X)*100:.1f}%)
    - Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)
    
    DATASETS UTILIZADOS:
    {chr(10).join([f"    - {name}: {('‚úÖ Cargado' if path else '‚ùå No disponible')}" for name, path in dataset_paths.items()])}
    
    ARCHIVOS GENERADOS:
    ==================
    
    üìÅ Results/
    ‚îú‚îÄ‚îÄ üìÅ Model/
    ‚îÇ   ‚îú‚îÄ‚îÄ emotion_cnn_lda_model.keras (Modelo entrenado - formato nativo - {os.path.getsize(model_path)/(1024*1024):.1f} MB)
    ‚îÇ   ‚îú‚îÄ‚îÄ emotion_cnn_lda_model.h5 (Modelo entrenado - formato legacy - {os.path.getsize(model_h5_path)/(1024*1024):.1f} MB)
    ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing_objects.pkl (Objetos de preprocesamiento)
    ‚îÇ   ‚îú‚îÄ‚îÄ model_results.json (Resultados detallados)
    ‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png (Matriz de confusi√≥n)
    ‚îÇ   ‚îú‚îÄ‚îÄ training_history.png (Historial de entrenamiento)
    ‚îÇ   ‚îú‚îÄ‚îÄ conv1d_filters_layer1.png (Filtros primera capa)
    ‚îÇ   ‚îú‚îÄ‚îÄ conv1d_filters_layer2.png (Filtros segunda capa)
    ‚îÇ   ‚îú‚îÄ‚îÄ activations_layer_*.png (Activaciones por capa)
    ‚îÇ   ‚îî‚îÄ‚îÄ detailed_performance_analysis.png (An√°lisis detallado)
    ‚îÇ
    ‚îú‚îÄ‚îÄ üìÅ LDA/
    ‚îÇ   ‚îú‚îÄ‚îÄ lda_analysis_main.png (An√°lisis principal LDA)
    ‚îÇ   ‚îú‚îÄ‚îÄ lda_3d_visualization.html (Visualizaci√≥n 3D interactiva)
    ‚îÇ   ‚îú‚îÄ‚îÄ lda_correlation_matrix.png (Matriz de correlaci√≥n)
    ‚îÇ   ‚îú‚îÄ‚îÄ emotion_centroids_distances.png (Distancias entre centroides)
    ‚îÇ   ‚îú‚îÄ‚îÄ pca_vs_lda_comparison.png (Comparaci√≥n PCA vs LDA)
    ‚îÇ   ‚îî‚îÄ‚îÄ lda_analysis_results.json (Resultados LDA detallados)
    ‚îÇ
    ‚îú‚îÄ‚îÄ üìÅ Plots/
    ‚îÇ   ‚îî‚îÄ‚îÄ emotion_distribution.png (Distribuci√≥n de emociones)
    ‚îÇ
    ‚îú‚îÄ‚îÄ complete_report.txt (Este reporte)
    ‚îî‚îÄ‚îÄ emotion_recognition_results.zip (Archivo comprimido con todo)
    
    INSTRUCCIONES DE USO DEL MODELO:
    ================================
    
    Para usar el modelo entrenado:
    
    1. Cargar objetos de preprocesamiento:
       import pickle
       with open('preprocessing_objects.pkl', 'rb') as f:
           prep_objects = pickle.load(f)
       
    2. Cargar modelo (formato nativo recomendado):
       from tensorflow.keras.models import load_model
       model = load_model('emotion_cnn_lda_model.keras')
       
       # O usar formato legacy si es necesario:
       model = load_model('emotion_cnn_lda_model.h5')
       
    3. Procesar nuevo audio:
       # Extraer caracter√≠sticas usando la misma configuraci√≥n
       # Aplicar StandardScaler y LDA en el mismo orden
       # Hacer predicci√≥n con el modelo
    
    NOTAS T√âCNICAS:
    ==============
    - El modelo espera caracter√≠sticas LDA de dimensi√≥n {X_train_lda.shape[1]}
    - Las caracter√≠sticas originales deben ser estandarizadas antes de aplicar LDA
    - El LDA debe ser aplicado con los mismos par√°metros guardados
    - La extracci√≥n de caracter√≠sticas debe usar la configuraci√≥n guardada
    
    FECHA DE GENERACI√ìN: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
    """

    report_path = os.path.join(results_dir, "complete_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    print(f"‚úÖ Reporte completo guardado: {report_path}")

    # =============================================================================
    # PARTE 13: DESCARGA AUTOM√ÅTICA DE ARCHIVOS
    # =============================================================================

    print("\nüì• Preparando descarga autom√°tica de archivos...")

    # Crear archivo ZIP con todos los resultados
    import zipfile
    import glob

    zip_path = "/content/emotion_recognition_results.zip"
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Agregar todos los archivos de la carpeta Results recursivamente
        for root, dirs, files in os.walk(results_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # Crear ruta relativa desde Results para mantener estructura
                archive_name = os.path.relpath(file_path, "/content")
                zipf.write(file_path, archive_name)
    
    print(f"‚úÖ Archivo ZIP creado: {zip_path}")

    # Mostrar estructura de archivos creados
    print(f"\nüìÇ Estructura de archivos generados:")
    for root, dirs, files in os.walk(results_dir):
        level = root.replace(results_dir, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = ' ' * 2 * (level + 1)
        for file in files:
            file_size = os.path.getsize(os.path.join(root, file))
            if file_size > 1024*1024:  # MB
                size_str = f"({file_size/(1024*1024):.1f} MB)"
            elif file_size > 1024:  # KB
                size_str = f"({file_size/1024:.1f} KB)"
            else:
                size_str = f"({file_size} B)"
            print(f"{subindent}{file} {size_str}")

    # Funci√≥n de descarga autom√°tica
    def download_all_results():
        """Descarga autom√°ticamente todos los archivos generados"""
        print("\nüöÄ Iniciando descarga autom√°tica...")
        print("=" * 50)
        
        try:
            # Descargar ZIP principal
            print("üì¶ Descargando archivo comprimido con todos los resultados...")
            files.download(zip_path)
            print("‚úÖ Descarga del ZIP completada")
            
            # Descargar archivos individuales importantes
            important_files = [
                (model_path, "Modelo CNN entrenado (.keras - formato nativo)"),
                (model_h5_path, "Modelo CNN entrenado (.h5 - formato legacy)"),
                (os.path.join(model_dir, "model_results.json"), "Resultados del modelo (.json)"),
                (os.path.join(lda_dir, "lda_analysis_results.json"), "Resultados LDA (.json)"),
                (report_path, "Reporte completo (.txt)")
            ]
            
            print("\nüìÑ Descargando archivos individuales importantes...")
            for file_path, description in important_files:
                if os.path.exists(file_path):
                    print(f"‚¨áÔ∏è {description}")
                    files.download(file_path)
                else:
                    print(f"‚ö†Ô∏è No encontrado: {description}")
                    
            print("\n‚úÖ ¬°Descarga autom√°tica completada!")
            print("üìÅ Revisa tu carpeta de descargas para encontrar todos los archivos")
            
        except Exception as e:
            print(f"‚ùå Error en descarga autom√°tica: {e}")
            print("üí° Puedes descargar manualmente usando los comandos mostrados arriba")

    # Funci√≥n de descarga por categor√≠as
    def download_model_files():
        """Descarga solo archivos relacionados con el modelo"""
        print("üì¶ Descargando archivos del modelo...")
        for file in os.listdir(model_dir):
            file_path = os.path.join(model_dir, file)
            if os.path.isfile(file_path):
                files.download(file_path)
        print("‚úÖ Archivos del modelo descargados")

    def download_lda_files():
        """Descarga solo archivos relacionados con LDA"""
        print("üìä Descargando archivos de LDA...")
        for file in os.listdir(lda_dir):
            file_path = os.path.join(lda_dir, file)
            if os.path.isfile(file_path):
                files.download(file_path)
        print("‚úÖ Archivos de LDA descargados")

    def download_plots():
        """Descarga solo gr√°ficos y visualizaciones"""
        print("üìà Descargando gr√°ficos...")
        for file in os.listdir(plots_dir):
            file_path = os.path.join(plots_dir, file)
            if os.path.isfile(file_path):
                files.download(file_path)
        print("‚úÖ Gr√°ficos descargados")

    # Mostrar opciones de descarga
    print(f"\nüì• OPCIONES DE DESCARGA:")
    print(f"=" * 60)
    print(f"")
    print(f"üéØ OPCI√ìN RECOMENDADA - Descarga autom√°tica completa:")
    print(f"   download_all_results()")
    print(f"")
    print(f"üì¶ DESCARGAS POR CATEGOR√çA:")
    print(f"   download_model_files()     # Solo archivos del modelo")
    print(f"   download_lda_files()       # Solo archivos de LDA")
    print(f"   download_plots()           # Solo gr√°ficos adicionales")
    print(f"")
    print(f"üìÑ DESCARGAS INDIVIDUALES:")
    print(f"   # ZIP completo:")
    print(f"   files.download('{zip_path}')")
    print(f"")
    print(f"   # Modelo entrenado (formato nativo):")
    print(f"   files.download('{model_path}')")
    print(f"")
    print(f"   # Modelo entrenado (formato legacy):")
    print(f"   files.download('{model_h5_path}')")
    print(f"")
    print(f"   # Reporte completo:")
    print(f"   files.download('{report_path}')")
    print(f"")
    print(f"üîß COMANDOS MANUALES COMPLETOS:")
    print(f"   import glob")
    print(f"   # Todos los archivos:")
    print(f"   for file in glob.glob('{results_dir}/**/*', recursive=True):")
    print(f"       if os.path.isfile(file): files.download(file)")

    # Crear script de descarga
    download_script = f'''# Script de descarga autom√°tica - Reconocimiento de Emociones CNN+LDA
from google.colab import files
import os
import glob

print("üöÄ Iniciando descarga autom√°tica...")

# Descargar ZIP completo
print("üì¶ Descargando archivo ZIP completo...")
files.download('{zip_path}')

# Descargar archivos importantes individualmente
print("üìÑ Descargando archivos importantes...")
important_files = [
    '{model_path}',  # Modelo formato nativo (.keras)
    '{model_h5_path}',  # Modelo formato legacy (.h5)
    '{os.path.join(model_dir, "model_results.json")}',
    '{os.path.join(lda_dir, "lda_analysis_results.json")}',
    '{report_path}'
]

for file_path in important_files:
    if os.path.exists(file_path):
        print(f"‚¨áÔ∏è Descargando: {{os.path.basename(file_path)}}")
        files.download(file_path)
    else:
        print(f"‚ö†Ô∏è No encontrado: {{file_path}}")

print("‚úÖ Descarga completada!")
print("üìÅ Revisa tu carpeta de descargas")
'''

    script_path = os.path.join(results_dir, "download_script.py")
    with open(script_path, 'w') as f:
        f.write(download_script)
    print(f"üìù Script de descarga creado: {script_path}")

    # Hacer las funciones disponibles globalmente
    globals()['download_all_results'] = download_all_results
    globals()['download_model_files'] = download_model_files
    globals()['download_lda_files'] = download_lda_files
    globals()['download_plots'] = download_plots

    # Resumen final
    print(f"\nüéâ ¬°Entrenamiento completado exitosamente!")
    print(f"{'='*80}")
    print(f"üìä RESUMEN FINAL:")
    print(f"   - Precisi√≥n final: {test_accuracy*100:.2f}%")
    print(f"   - Reducci√≥n dimensional LDA: {X_train_scaled.shape[1]} ‚Üí {X_train_lda.shape[1]} caracter√≠sticas")
    print(f"   - Varianza retenida por LDA: {np.sum(lda.explained_variance_ratio_)*100:.1f}%")
    print(f"   - N√∫mero de par√°metros del modelo: {total_params:,}")
    print(f"   - Clases reconocidas: {', '.join(emotion_classes)}")
    print(f"{'='*80}")

else:
    print("‚ùå No hay suficientes datos para entrenar el modelo")
    print("Por favor, verificar la configuraci√≥n de los datasets")