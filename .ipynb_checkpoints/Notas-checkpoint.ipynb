{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db08358c",
   "metadata": {},
   "source": [
    "# Análisis de Métodos de Regresión en Inteligencia Artificial\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este documento de investigación aborda tres aspectos fundamentales en el análisis de datos y la creación de modelos predictivos en inteligencia artificial:\n",
    "1. Tipos de ajustes de curvas en conjuntos de datos: lineal, polinomial y el problema del sobreajuste (overfitting)\n",
    "2. La regresión lineal y el método de mínimos cuadrados\n",
    "3. El algoritmo de descenso de gradiente (Gradient Descent)\n",
    "\n",
    "Estos conceptos son pilares en el aprendizaje automático y el análisis de datos, siendo herramientas esenciales para modelar relaciones entre variables y hacer predicciones basadas en datos.\n",
    "\n",
    "## Configuración del entorno\n",
    "\n",
    "Primero, importaremos las bibliotecas necesarias para nuestro análisis:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración para mejorar la visualización de gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 14\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "```\n",
    "\n",
    "## 1. Tipos de \"rectas\" en una gráfica de conjunto de datos\n",
    "\n",
    "Cuando se ajustan modelos a datos, existen diferentes enfoques para modelar la relación entre variables. Veremos tres tipos principales:\n",
    "\n",
    "### 1.1 Regresión Lineal\n",
    "\n",
    "La regresión lineal es el método más simple que intenta ajustar una línea recta a los datos:\n",
    "\n",
    "```python\n",
    "# Generamos datos con una relación lineal y algo de ruido\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X.ravel() + 1 + np.random.randn(100) * 1.5\n",
    "\n",
    "# Creamos el modelo y lo ajustamos\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X, y)\n",
    "\n",
    "# Predicciones del modelo\n",
    "y_pred_linear = model_linear.predict(X)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', alpha=0.6, label='Datos')\n",
    "plt.plot(X, y_pred_linear, color='red', linewidth=2, label=f'Modelo Lineal: y = {model_linear.coef_[0]:.2f}x + {model_linear.intercept_:.2f}')\n",
    "plt.title('Regresión Lineal')\n",
    "plt.xlabel('Variable independiente (X)')\n",
    "plt.ylabel('Variable dependiente (y)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "La regresión lineal modela relaciones de la forma:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Donde:\n",
    "- $\\beta_0$ es la intersección con el eje y (ordenada al origen)\n",
    "- $\\beta_1$ es la pendiente de la recta\n",
    "- $\\epsilon$ representa el error o ruido aleatorio\n",
    "\n",
    "### 1.2 Regresión Polinomial\n",
    "\n",
    "Cuando los datos presentan una relación no lineal, podemos utilizar polinomios de mayor grado:\n",
    "\n",
    "```python\n",
    "# Generamos datos con una relación cuadrática\n",
    "X_poly_data = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_poly_data = 1 + 0.5 * X_poly_data.ravel() + 0.3 * X_poly_data.ravel()**2 + np.random.randn(100) * 2\n",
    "\n",
    "# Creamos características polinomiales\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X_poly_data)\n",
    "\n",
    "# Ajustamos el modelo lineal a las características polinomiales\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_poly_data)\n",
    "\n",
    "# Predicciones del modelo\n",
    "y_pred_poly = model_poly.predict(X_poly)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_poly_data, y_poly_data, color='blue', alpha=0.6, label='Datos')\n",
    "plt.plot(X_poly_data, y_pred_poly, color='green', linewidth=2, label='Modelo Polinomial (grado 2)')\n",
    "plt.title('Regresión Polinomial')\n",
    "plt.xlabel('Variable independiente (X)')\n",
    "plt.ylabel('Variable dependiente (y)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "La regresión polinomial permite modelar relaciones más complejas:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n + \\epsilon$$\n",
    "\n",
    "### 1.3 El problema del Sobreajuste (Overfitting)\n",
    "\n",
    "El sobreajuste ocurre cuando un modelo es demasiado complejo y se ajusta al ruido en los datos en lugar de capturar la tendencia subyacente:\n",
    "\n",
    "```python\n",
    "# Generamos pocos datos con una relación simple\n",
    "X_overfit = np.linspace(0, 10, 15).reshape(-1, 1)\n",
    "y_overfit = 3 + 0.5 * X_overfit.ravel() + np.random.randn(15) * 1.5\n",
    "\n",
    "# Creamos modelos de diferentes complejidades\n",
    "grados = [1, 3, 15]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, grado in enumerate(grados):\n",
    "    # Generamos características polinomiales\n",
    "    poly_features = PolynomialFeatures(degree=grado, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X_overfit)\n",
    "    \n",
    "    # Ajustamos el modelo\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_overfit)\n",
    "    \n",
    "    # Puntos para trazar la curva de predicción\n",
    "    X_test = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Cálculo del error cuadrático medio\n",
    "    X_overfit_poly = poly_features.transform(X_overfit)\n",
    "    y_train_pred = model.predict(X_overfit_poly)\n",
    "    mse = mean_squared_error(y_overfit, y_train_pred)\n",
    "    \n",
    "    # Visualización\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(X_overfit, y_overfit, color='blue', alpha=0.6, label='Datos de entrenamiento')\n",
    "    plt.plot(X_test, y_pred, color='red', linewidth=2, label=f'Polinomio grado {grado}')\n",
    "    plt.title(f'Grado {grado}, MSE: {mse:.2f}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.ylim(-5, 15)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Observaciones importantes sobre el sobreajuste:\n",
    "- Un modelo con grado bajo puede subajustar los datos (alto sesgo)\n",
    "- Un modelo con grado alto puede sobreajustar los datos (alta varianza)\n",
    "- El modelo óptimo encuentra un equilibrio entre sesgo y varianza\n",
    "\n",
    "## 2. Regresión Lineal y el Método de Mínimos Cuadrados\n",
    "\n",
    "El método de mínimos cuadrados busca encontrar los parámetros del modelo que minimicen la suma de los cuadrados de las diferencias entre las predicciones y los valores reales.\n",
    "\n",
    "### 2.1 Derivación matemática\n",
    "\n",
    "Para un modelo lineal $y = \\beta_0 + \\beta_1 x$, buscamos minimizar:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n",
    "\n",
    "La solución analítica es:\n",
    "\n",
    "$$\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$\n",
    "\n",
    "Donde:\n",
    "- $\\bar{x}$ y $\\bar{y}$ son las medias de $x$ e $y$\n",
    "\n",
    "Implementemos el método de mínimos cuadrados manualmente:\n",
    "\n",
    "```python\n",
    "# Generamos algunos datos de ejemplo\n",
    "X_simple = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_simple = 2 * X_simple.ravel() + 1 + np.random.randn(50) * 1.5\n",
    "\n",
    "# Implementación manual del método de mínimos cuadrados\n",
    "def minimos_cuadrados(x, y):\n",
    "    # Convertimos a arrays unidimensionales\n",
    "    x = x.ravel()\n",
    "    y = y.ravel()\n",
    "    \n",
    "    # Calculamos medias\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Calculamos la pendiente (beta_1)\n",
    "    numerador = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominador = np.sum((x - x_mean) ** 2)\n",
    "    beta_1 = numerador / denominador\n",
    "    \n",
    "    # Calculamos la intersección (beta_0)\n",
    "    beta_0 = y_mean - beta_1 * x_mean\n",
    "    \n",
    "    return beta_0, beta_1\n",
    "\n",
    "# Calculamos los coeficientes\n",
    "beta_0, beta_1 = minimos_cuadrados(X_simple, y_simple)\n",
    "print(f\"Coeficientes calculados manualmente: β₀ = {beta_0:.4f}, β₁ = {beta_1:.4f}\")\n",
    "\n",
    "# Comparamos con sklearn\n",
    "model = LinearRegression()\n",
    "model.fit(X_simple, y_simple)\n",
    "print(f\"Coeficientes con sklearn: β₀ = {model.intercept_:.4f}, β₁ = {model.coef_[0]:.4f}\")\n",
    "\n",
    "# Visualización de los resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, color='blue', alpha=0.6, label='Datos')\n",
    "plt.plot(X_simple, beta_0 + beta_1 * X_simple.ravel(), 'r-', linewidth=2, \n",
    "         label=f'Manual: y = {beta_1:.2f}x + {beta_0:.2f}')\n",
    "plt.plot(X_simple, model.predict(X_simple), 'g--', linewidth=2, \n",
    "         label=f'Sklearn: y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}')\n",
    "plt.title('Método de Mínimos Cuadrados')\n",
    "plt.xlabel('Variable independiente (X)')\n",
    "plt.ylabel('Variable dependiente (y)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2.2 Visualización de la función de costo\n",
    "\n",
    "Para entender mejor la optimización, visualizaremos la función de costo en el espacio de parámetros:\n",
    "\n",
    "```python\n",
    "# Función para calcular el error cuadrático medio\n",
    "def calcular_mse(X, y, beta_0, beta_1):\n",
    "    y_pred = beta_0 + beta_1 * X.ravel()\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "# Creamos una malla de valores para beta_0 y beta_1\n",
    "beta_0_range = np.linspace(beta_0 - 2, beta_0 + 2, 100)\n",
    "beta_1_range = np.linspace(beta_1 - 2, beta_1 + 2, 100)\n",
    "B0, B1 = np.meshgrid(beta_0_range, beta_1_range)\n",
    "\n",
    "# Calculamos el MSE para cada combinación de parámetros\n",
    "Z = np.zeros_like(B0)\n",
    "for i in range(len(beta_0_range)):\n",
    "    for j in range(len(beta_1_range)):\n",
    "        Z[j, i] = calcular_mse(X_simple, y_simple, B0[j, i], B1[j, i])\n",
    "\n",
    "# Visualización en 3D\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(B0, B1, Z, cmap=cm.coolwarm, alpha=0.8)\n",
    "ax1.set_xlabel('β₀ (Intersección)')\n",
    "ax1.set_ylabel('β₁ (Pendiente)')\n",
    "ax1.set_zlabel('Error Cuadrático Medio')\n",
    "ax1.set_title('Superficie de la función de costo')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10, ax=ax1)\n",
    "\n",
    "# Gráfico de contorno\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(B0, B1, Z, 20, cmap=cm.coolwarm)\n",
    "ax2.scatter(beta_0, beta_1, color='red', marker='*', s=200, label='Mínimo (Solución)')\n",
    "ax2.set_xlabel('β₀ (Intersección)')\n",
    "ax2.set_ylabel('β₁ (Pendiente)')\n",
    "ax2.set_title('Contorno de la función de costo')\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 3. Algoritmo de Descenso de Gradiente (Gradient Descent)\n",
    "\n",
    "El descenso de gradiente es un algoritmo de optimización que busca el mínimo de una función de costo ajustando iterativamente los parámetros en la dirección opuesta al gradiente.\n",
    "\n",
    "### 3.1 Implementación del algoritmo\n",
    "\n",
    "```python\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, tolerance=1e-6):\n",
    "    # Añadimos una columna de unos para el intercepto\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # Inicializamos los parámetros\n",
    "    theta = np.random.randn(2, 1)\n",
    "    \n",
    "    # Historial de parámetros y costos\n",
    "    theta_history = [theta.copy()]\n",
    "    cost_history = []\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Predicciones actuales\n",
    "        y_pred = X_b.dot(theta)\n",
    "        \n",
    "        # Cálculo del error\n",
    "        error = y_pred - y.reshape(-1, 1)\n",
    "        \n",
    "        # Cálculo del gradiente\n",
    "        gradients = 2/m * X_b.T.dot(error)\n",
    "        \n",
    "        # Actualización de parámetros\n",
    "        theta_new = theta - learning_rate * gradients\n",
    "        \n",
    "        # Calculamos el costo actual\n",
    "        cost = np.mean(error ** 2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Guardamos los parámetros\n",
    "        theta_history.append(theta_new.copy())\n",
    "        \n",
    "        # Verificamos la convergencia\n",
    "        if np.linalg.norm(theta_new - theta) < tolerance:\n",
    "            print(f\"Convergencia alcanzada en la iteración {iteration}\")\n",
    "            break\n",
    "            \n",
    "        theta = theta_new\n",
    "    \n",
    "    return theta, np.array(theta_history), np.array(cost_history)\n",
    "\n",
    "# Aplicamos el algoritmo a nuestros datos\n",
    "X_gd = X_simple.copy()\n",
    "y_gd = y_simple.copy()\n",
    "\n",
    "# Ejecutamos el descenso de gradiente\n",
    "theta_final, theta_history, cost_history = gradient_descent(X_gd, y_gd, learning_rate=0.01, n_iterations=2000)\n",
    "\n",
    "# Extracción de parámetros\n",
    "beta_0_gd = theta_final[0, 0]\n",
    "beta_1_gd = theta_final[1, 0]\n",
    "\n",
    "print(f\"Resultados del descenso de gradiente: β₀ = {beta_0_gd:.4f}, β₁ = {beta_1_gd:.4f}\")\n",
    "print(f\"Resultados con mínimos cuadrados: β₀ = {beta_0:.4f}, β₁ = {beta_1:.4f}\")\n",
    "```\n",
    "\n",
    "### 3.2 Visualización del proceso de convergencia\n",
    "\n",
    "```python\n",
    "# Graficamos la convergencia del costo\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, 'b-')\n",
    "plt.title('Evolución de la función de costo')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error Cuadrático Medio')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cost_history, 'b-')\n",
    "plt.title('Evolución de la función de costo (escala logarítmica)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error Cuadrático Medio')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualizamos la trayectoria de los parámetros en el contorno de la función de costo\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(B0, B1, Z, 30, cmap=cm.coolwarm)\n",
    "plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r.-', linewidth=1, markersize=3)\n",
    "plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=8, label='Inicio')\n",
    "plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'r*', markersize=12, label='Final')\n",
    "plt.plot(beta_0, beta_1, 'b*', markersize=12, label='Solución analítica')\n",
    "plt.xlabel('β₀ (Intersección)')\n",
    "plt.ylabel('β₁ (Pendiente)')\n",
    "plt.title('Trayectoria del descenso de gradiente')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3.3 Comparación con la solución analítica\n",
    "\n",
    "Veamos cómo el descenso de gradiente se compara con la solución analítica:\n",
    "\n",
    "```python\n",
    "# Comparamos visualmente las predicciones\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_gd, y_gd, color='blue', alpha=0.6, label='Datos')\n",
    "plt.plot(X_gd, beta_0_gd + beta_1_gd * X_gd.ravel(), 'r-', linewidth=2, \n",
    "         label=f'Desc. Gradiente: y = {beta_1_gd:.2f}x + {beta_0_gd:.2f}')\n",
    "plt.plot(X_gd, beta_0 + beta_1 * X_gd.ravel(), 'g--', linewidth=2, \n",
    "         label=f'Mín. Cuadrados: y = {beta_1:.2f}x + {beta_0:.2f}')\n",
    "plt.title('Comparación de métodos')\n",
    "plt.xlabel('Variable independiente (X)')\n",
    "plt.ylabel('Variable dependiente (y)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3.4 Efecto de la tasa de aprendizaje\n",
    "\n",
    "La tasa de aprendizaje (learning rate) es un hiperparámetro crucial en el descenso de gradiente:\n",
    "\n",
    "```python\n",
    "# Probamos diferentes tasas de aprendizaje\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    # Ejecutamos el algoritmo\n",
    "    theta, theta_history, cost_history = gradient_descent(X_gd, y_gd, learning_rate=lr, n_iterations=50)\n",
    "    \n",
    "    # Graficamos la trayectoria\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.contour(B0, B1, Z, 30, cmap=cm.coolwarm)\n",
    "    plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r.-', linewidth=1, markersize=3)\n",
    "    plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=8, label='Inicio')\n",
    "    plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'r*', markersize=12, label='Final')\n",
    "    plt.plot(beta_0, beta_1, 'b*', markersize=12, label='Solución analítica')\n",
    "    plt.title(f'Tasa de aprendizaje = {lr}')\n",
    "    plt.xlabel('β₀')\n",
    "    plt.ylabel('β₁')\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "En este documento hemos explorado:\n",
    "\n",
    "1. **Tipos de ajustes**:\n",
    "   - La regresión lineal es útil para modelar relaciones lineales simples\n",
    "   - La regresión polinomial puede capturar patrones no lineales\n",
    "   - El sobreajuste ocurre cuando el modelo es demasiado complejo para los datos disponibles\n",
    "\n",
    "2. **Método de mínimos cuadrados**:\n",
    "   - Proporciona una solución analítica para problemas de regresión lineal\n",
    "   - Minimiza la suma de errores cuadráticos entre predicciones y valores reales\n",
    "\n",
    "3. **Descenso de gradiente**:\n",
    "   - Es un algoritmo iterativo que busca el mínimo de la función de costo\n",
    "   - La tasa de aprendizaje influye significativamente en la convergencia\n",
    "   - Puede aplicarse a problemas donde no existe una solución analítica directa\n",
    "\n",
    "Estos conceptos son fundamentales en el aprendizaje automático y forman la base para algoritmos más complejos como las redes neuronales.\n",
    "\n",
    "## Referencias\n",
    "\n",
    "American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\n",
    "\n",
    "Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.\n",
    "\n",
    "Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O'Reilly Media, Inc.\n",
    "\n",
    "Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer Science & Business Media.\n",
    "\n",
    "James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer.\n",
    "\n",
    "Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
