<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 24</div>

    <div class="presentation-container">
        <!-- Diapositivas 1-7 se mantienen intactas -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
        </div>
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
        </div>
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, permitiendo la retroalimentaci√≥n y mejora continua en cada fase del proyecto.</p>
            <div class="methodology-diagram-iterative">
                <div class="step-container">
                    <div class="step">Adquisici√≥n de Datos</div>
                    <div class="arrow-down">‚Üì</div>
                    <div class="step">An√°lisis Exploratorio</div>
                    <div class="arrow-down">‚Üì</div>
                    <div class="step">Preprocesamiento</div>
                </div>
                <div class="step-container">
                    <div class="arrow-loop-right">‚§¥</div>
                    <div class="step">Evaluaci√≥n</div>
                    <div class="arrow-up">‚Üë</div>
                     <div class="step">Entrenamiento</div>
                    <div class="arrow-up">‚Üë</div>
                    <div class="step">Extracci√≥n de Caracter√≠sticas</div>
                    <div class="arrow-loop-left">‚§µ</div>
                </div>
            </div>
            <p style="text-align: center; margin-top: 20px;">Este modelo permite regresar a pasos anteriores si los resultados de la evaluaci√≥n no son satisfactorios, por ejemplo, volviendo al preprocesamiento o a la extracci√≥n de caracter√≠sticas para refinar el modelo.</p>
        </div>
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituy√≥ un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>

        <div class="slide hidden" data-slide="8">
            <h2>7. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribuci√≥n de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gr√°ficos de Viol√≠n de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>Nota:</strong> Las siguientes visualizaciones fueron generadas con un script de Python para analizar las caracter√≠sticas del dataset RAVDESS.
            </footer>
        </div>

        <!-- ***** INICIO DE DIAPOSITIVA ACTUALIZADA (9) ***** -->
        <div class="slide hidden" data-slide="9">
            <h2>8. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Pitch: Distribuci√≥n de Frecuencia Fundamental (F0) por Emoci√≥n</h3>
                <img src="graficas/eda/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica nos muestra c√≥mo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoci√≥n. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores m√°s altos significan un tono m√°s agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qu√© tan comunes son ciertos tonos para una emoci√≥n.
                </p>
                <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energ√≠a como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos m√°s agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos m√°s graves y mon√≥tonos.
                </footer>
            </div>
        </div>
        <!-- ***** FIN DE DIAPOSITIVA ACTUALIZADA (9) ***** -->

        <!-- ***** INICIO DE DIAPOSITIVA ACTUALIZADA (10) ***** -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n y An√°lisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>An√°lisis Espectral: Distribuci√≥n de los Primeros 13 MFCCs por Emoci√≥n</h3>
                    <img src="graficas/eda/distribucion_mfcc_por_emocion.png" alt="Gr√°ficos de Viol√≠n de MFCCs por Emoci√≥n" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualizaci√≥n nos permite comparar la "forma" del sonido para cada emoci√≥n a trav√©s de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "viol√≠n" muestra el rango y la concentraci√≥n de valores para un coeficiente (eje X) y una emoci√≥n (color).
                </p>
                 <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribuci√≥n para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posici√≥n de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoci√≥n de otra. Coeficientes con distribuciones muy distintas entre colores son altamente informativos para la clasificaci√≥n.
                </footer>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretaci√≥n General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energ√≠a total o sonoridad de la se√±al.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles m√°s finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <!-- ***** FIN DE DIAPOSITIVA ACTUALIZADA (10) ***** -->

        <div class="slide hidden" data-slide="11">
            <h2>10. Pipeline de Preprocesamiento de Datos</h2>
            <p>Para asegurar la calidad y consistencia de los datos antes del entrenamiento, se aplic√≥ un pipeline de preprocesamiento con los siguientes pasos clave:</p>
            <div class="preprocessing-grid">
                <!-- Tarjeta 1: Normalizaci√≥n -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M2 12h3m14 0h3M12 2v3m0 14v3"/><circle cx="12" cy="12" r="7"/></svg>
                    </div>
                    <h4>Normalizaci√≥n de Audio</h4>
                    <p>Unificar todos los archivos a una frecuencia de muestreo de 22.05 kHz y una resoluci√≥n de 16 bits para estandarizar la calidad.</p>
                </div>
                <!-- Tarjeta 2: Validaci√≥n -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"></polyline></svg>
                    </div>
                    <h4>Validaci√≥n de Datos</h4>
                    <p>Descartar archivos de audio corruptos, inv√°lidos o con una duraci√≥n demasiado corta (menor a 1 segundo) para el an√°lisis.</p>
                </div>
                <!-- Tarjeta 3: Filtrado -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polygon points="22 3 2 3 10 12.46 10 19 14 21 14 12.46 22 3"></polygon></svg>
                    </div>
                    <h4>Filtrado de Ruido</h4>
                    <p>Aplicar filtros digitales para eliminar el ruido de fondo y las frecuencias que no son relevantes para el habla humana.</p>
                </div>
                <!-- Tarjeta 4: Segmentaci√≥n -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.5 16.5L18 20l-1.5 1.5L13 18l-1.5 1.5L10 18l-1.5 1.5L7 18l-1.5 1.5L4 18l-1.5 1.5L1 18M3 6l1.5-1.5L6 6l1.5-1.5L9 6l1.5-1.5L12 6l1.5-1.5L15 6l1.5-1.5L18 6l1.5-1.5L21 6l1.5-1.5L24 6"/><path d="M3 10l1.5-1.5L6 10l1.5-1.5L9 10l1.5-1.5L12 10l1.5-1.5L15 10l1.5-1.5L18 10l1.5-1.5L21 10l1.5-1.5L24 10"/></svg>
                    </div>
                    <h4>Segmentaci√≥n Temporal</h4>
                    <p>Dividir los audios m√°s largos en ventanas de 3 segundos con solapamiento para aumentar la cantidad de muestras de entrenamiento.</p>
                </div>
                 <!-- Tarjeta 5: Balanceo -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 3L2 9l10 6 10-6-10-6zM2 15l10 6 10-6M2 9l10 6 10-6"/></svg>
                    </div>
                    <h4>Balanceo de Clases</h4>
                    <p>Utilizar t√©cnicas como SMOTE para sobremuestrear las clases minoritarias y asegurar que el modelo no se sesgue.</p>
                </div>
            </div>
            <footer class="footnote" style="margin-top: 20px;">
                <strong>SMOTE:</strong> T√©cnica para crear muestras sint√©ticas de las clases minoritarias y as√≠ balancear el dataset.
            </footer>
        </div>

        <div class="slide hidden" data-slide="12">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
        </div>
        <div class="slide hidden" data-slide="13">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>

        <div class="slide hidden" data-slide="14">
            <h2>13. Implementaci√≥n del Modelo CNN 1D</h2>
            <h3>1. Preparaci√≥n de Datos (C√≥digo Real del Proyecto)</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# A√±adir una dimensi√≥n para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            <p class="code-description">Dividimos los datos, asegurando que cada emoci√≥n est√© representada por igual en ambos conjuntos (`stratify=y`). Luego, escalamos los datos y a√±adimos una dimensi√≥n extra, que es el formato que espera la capa `Conv1D` de Keras.</p>
            
            <h3>2. Definici√≥n y Entrenamiento del Modelo</h3>
            <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense

model = Sequential([
    Conv1D(256, 5, padding='same', activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Conv1D(128, 5, padding='same', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Flatten(),
    Dense(y_encoded.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train_cnn, y_train, epochs=100, batch_size=64, validation_split=0.2
)</code></pre>
            <p class="code-description">Definimos la arquitectura de la CNN 1D, la compilamos con el optimizador 'adam' y la funci√≥n de p√©rdida para clasificaci√≥n multiclase. Finalmente, la entrenamos con los datos de entrenamiento, usando un 20% de estos para validaci√≥n interna en cada √©poca.</p>
        </div>

        <div class="slide hidden" data-slide="15">
            <h2>14. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo.
            </footer>
        </div>
        <div class="slide hidden" data-slide="16">
            <h2>15. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qu√© tan dispersos est√°n los datos. <strong>Dispersi√≥n:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>
        <div class="slide hidden" data-slide="17">
            <h2>16. Visualizaci√≥n 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separaci√≥n entre las emociones. El resultado es una separaci√≥n mucho m√°s clara y c√∫mulos m√°s compactos, lo que confirma visualmente que nuestras caracter√≠sticas son muy efectivas para la clasificaci√≥n.
            </p>
        </div>
        
        <div class="slide hidden" data-slide="18">
            <h2>17. Comparaci√≥n Final y Reflexi√≥n</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersi√≥n general de los datos</td>
                        <td>Muestra qu√© tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opini√≥n Reflexiva</h3>
            <p>Al comparar ambas t√©cnicas, <strong>LDA demuestra ser m√°s efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es √∫til para entender la estructura general de la varianza en los datos, LDA, al ser un m√©todo supervisado, logra crear proyecciones donde las emociones forman c√∫mulos m√°s definidos y distinguibles. Esto sugiere que las caracter√≠sticas extra√≠das, cuando se proyectan con un objetivo de clasificaci√≥n, son altamente discriminativas.</p>
        </div>

        <div class="slide hidden" data-slide="19">
            <h2>18. Fundamento Te√≥rico: Red Neuronal Convolucional 1D</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>¬øC√≥mo "Piensa" una CNN 1D?</h4>
                    <div class="code-block-container">
                        <div class="code-block-header">
                            <span class="code-title">Pseudoc√≥digo de Operaci√≥n</span>
                            <div class="window-controls">
                                <span class="control-btn minimize"></span>
                                <span class="control-btn maximize"></span>
                                <span class="control-btn close"></span>
                            </div>
                        </div>
                        <pre>
Entrada = Secuencia de Caracter√≠sticas (180 n√∫meros)

<span class="comment">// 1. Capas Convolucionales 1D (Detectores de Patrones)</span>
Para cada filtro en la capa:
    Desliza el filtro sobre la secuencia
    Calcula la suma ponderada (convoluci√≥n)
    Aplica funci√≥n de activaci√≥n (ReLU) -> Resalta patrones

<span class="comment">// 2. Capas de Pooling 1D (Compresores de Informaci√≥n)</span>
Reduce la longitud de la secuencia (ej. toma el valor m√°ximo)
-> Mantiene la informaci√≥n m√°s relevante y descarta el resto

<span class="comment">// 3. Capas Densas (Clasificador Final)</span>
Aplana la salida a un solo vector
Conecta todas las neuronas
Aplica Softmax para calcular probabilidades

<span class="comment">// 4. Salida</span>
Predicci√≥n = Emoci√≥n con la probabilidad m√°s alta
                        </pre>
                    </div>
                    <div class="activation-function-info">
                        <h4>Funciones de Activaci√≥n Clave</h4>
                        <p><strong>ReLU (Rectified Linear Unit):</strong> <span class="formula">ReLU(x) = max(0, x)</span>. Se usa en capas ocultas para introducir no-linealidad, permitiendo al modelo aprender relaciones complejas.</p>
                        <p><strong>Softmax:</strong> Se usa en la capa de salida para convertir las puntuaciones en una distribuci√≥n de probabilidad sobre las 7 emociones.</p>
                    </div>
                </div>
        
                <div class="cnn-side-panel">
                    <div class="advantages-disadvantages-1d">
                        <h4>Ventajas del Enfoque 1D</h4>
                        <ul>
                            <li>Analiza directamente la secuencia temporal de caracter√≠sticas.</li>
                            <li>Eficiente computacionalmente, menos par√°metros que una CNN 2D.</li>
                            <li>Arquitectura ideal para cualquier tipo de se√±al o serie de tiempo.</li>
                        </ul>
                        <h4>Desventajas</h4>
                        <ul>
                            <li>Puede perder informaci√≥n contextual que un espectrograma 2D s√≠ capturar√≠a.</li>
                            <li>Depende fuertemente de la calidad de las caracter√≠sticas extra√≠das manualmente.</li>
                        </ul>
                    </div>
                    <div class="cnn-audio-sample">
                        <h4>Muestra de Audio de Entrada (Disgusto)</h4>
                        <p>Este es un ejemplo del tipo de audio que procesa el modelo antes de la extracci√≥n de caracter√≠sticas.</p>
                        <audio controls src="presentation/src/audio/03-01-07-02-02-02-11.wav">
                            Tu navegador no soporta el elemento de audio.
                        </audio>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="20">
            <h2>19. Arquitectura del Modelo CNN 1D</h2>
            <p>Esta es la arquitectura de Keras que implementamos. Keras es una librer√≠a de alto nivel que facilita la construcci√≥n de redes neuronales. La red est√° dise√±ada para procesar la secuencia de 180 caracter√≠sticas extra√≠das de cada audio y aprender a clasificar la emoci√≥n.</p>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <p>Secuencia de Caracter√≠sticas<br><span class="layer-shape">(180 caracter√≠sticas, 1)</span></p>
                    <p class="layer-desc">Recibe el vector de 180 caracter√≠sticas como una secuencia temporal.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (256 filtros) + ReLU</h4>
                    <p class="layer-desc">La primera capa convolucional busca patrones temporales simples en la secuencia de entrada usando 256 filtros diferentes.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Reduce la longitud de la secuencia a una quinta parte, manteniendo solo las activaciones de patrones m√°s fuertes para la siguiente capa.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (128 filtros) + ReLU</h4>
                    <p class="layer-desc">Una segunda capa convolucional busca patrones m√°s complejos y de mayor duraci√≥n a partir de las caracter√≠sticas detectadas por la capa anterior.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Vuelve a reducir la secuencia para condensar a√∫n m√°s la informaci√≥n.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dense">
                    <h4>Flatten & Capa Densa</h4>
                    <p class="layer-desc">La capa Flatten convierte la secuencia final en un solo vector largo, que se conecta a una capa densa para la clasificaci√≥n.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box output">
                    <h4>Capa de Salida (Softmax)</h4>
                    <p class="layer-desc">La capa final tiene 7 neuronas, una por cada emoci√≥n, y usa Softmax para asignar una probabilidad a cada una.</p>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="21">
            <h2>20. Resumen de Neuronas y Par√°metros</h2>
            <div class="cnn-details">
                <p>
                    Esta tabla detalla la estructura de nuestro modelo. La **"Forma de Salida"** muestra c√≥mo cambian las dimensiones de los datos despu√©s de cada capa. Los **"Par√°metros"** son los pesos o "conocimientos" que el modelo aprende durante el entrenamiento. En total, nuestro modelo debe aprender y ajustar **171,783 par√°metros** para poder realizar la clasificaci√≥n.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa (Tipo)</th>
                            <th>Forma de Salida</th>
                            <th>Par√°metros</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>conv1d</td>
                            <td>(None, 180, 256)</td>
                            <td>1,536</td>
                        </tr>
                        <tr>
                            <td>max_pooling1d</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>conv1d_1</td>
                            <td>(None, 36, 128)</td>
                            <td>163,968</td>
                        </tr>
                         <tr>
                            <td>max_pooling1d_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>flatten</td>
                            <td>(None, 896)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dense</td>
                            <td>(None, 7)</td>
                            <td>6,279</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="2"><strong>Total de Par√°metros Entrenables</strong></td>
                            <td><strong>171,783</strong></td>
                        </tr>
                    </tbody>
                </table>
                 <footer class="cnn-table-footnote">
                    <strong>Par√°metros:</strong> Son los "conocimientos" internos de la red que se ajustan durante el entrenamiento.
                </footer>
            </div>
        </div>

        <!-- Diapositivas Finales -->
        <div class="slide hidden" data-slide="22">
            <h2>21. Recursos</h2>
            <div class="dataset-card">
                <h3>Lenguaje y Frameworks</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal</li>
                    <li><strong>TensorFlow/Keras:</strong> Desarrollo de CNN 1D</li>
                    <li><strong>Librosa:</strong> Extracci√≥n de caracter√≠sticas de audio</li>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y m√©tricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Hardware y Plataformas</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU.</li>
                    <li><strong>GitHub:</strong> Control de versiones.</li>
                </ul>
            </div>
        </div>
        <div class="slide hidden" data-slide="23">
            <h2>22. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo CNN 1D para clasificar 7 emociones.</li>
                        <li>Pipeline de extracci√≥n de 180 caracter√≠sticas.</li>
                        <li>Comparaci√≥n visual 3D de PCA y LDA.</li>
                        <li>M√©tricas de rendimiento del modelo.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>An√°lisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicaci√≥n de usuario final.</li>
                        <li>El modelo no opera en tiempo real.</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="slide hidden farewell-slide" data-slide="24">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de caracter√≠sticas y clasificar emociones complejas en la voz.
            </p>
            <div class="highlight-box">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="presentation/src/js/script.js"></script>
</body>
</html>
