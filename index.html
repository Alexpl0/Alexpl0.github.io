<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 24</div>

    <div class="presentation-container">
        <!-- Diapositivas 1-7 se mantienen intactas -->
        <div class="slide" data-slide="1">
            <h1>Análisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Acústicos para la Clasificación del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque técnico para la modelación de características prosódicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro Pérez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Gutiérrez</strong></p>
            </div>
        </div>
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificación Técnica</h2>
            <div class="highlight-box">
                <h3>¿Por qué Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma acústica compleja, rica en información latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Características Prosódicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribución de energía en las frecuencias (formantes) varía sistemáticamente con la emoción.</p></div>
                <div class="stat-item"><h3>Señal No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con técnicas de IA.</p></div>
            </div>
            <p><strong>¿Por qué Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracción de Patrones:</strong> Capacidad para identificar automáticamente características complejas en espectrogramas, indetectables para el análisis tradicional.</li>
                <li><strong>Análisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificación consistente y reproducible de las características vocales.</li>
                <li><strong>Modelado de Alta Dimensión:</strong> Habilidad para procesar miles de características extraídas de una sola señal de audio.</li>
            </ul>
        </div>
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripción del Problema</h2>
            <div class="highlight-box">
                <h3>Problema Técnico Principal</h3>
                <p>El desafío de clasificar estados emocionales a partir de la señal del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¿Qué reto técnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones acústicos para 7 emociones distintas: <strong>alegría, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabación.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la señal hasta la clasificación.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificación de emociones humanas a partir del análisis de características acústicas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la señal hasta la predicción del modelo.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Específicos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar múltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar características acústicas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Diseñar, entrenar y optimizar modelos especializados de aprendizaje automático</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando métricas estándar de clasificación</div>
            <div class="methodology-step"><strong>5.</strong> Implementar técnicas de reducción de dimensionalidad y visualización</div>
        </div>
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodología Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo cíclico, permitiendo la retroalimentación y mejora continua en cada fase del proyecto.</p>
            <div class="methodology-diagram-iterative">
                <div class="step-container">
                    <div class="step">Adquisición de Datos</div>
                    <div class="arrow-down">↓</div>
                    <div class="step">Análisis Exploratorio</div>
                    <div class="arrow-down">↓</div>
                    <div class="step">Preprocesamiento</div>
                </div>
                <div class="step-container">
                    <div class="arrow-loop-right">⤴</div>
                    <div class="step">Evaluación</div>
                    <div class="arrow-up">↑</div>
                     <div class="step">Entrenamiento</div>
                    <div class="arrow-up">↑</div>
                    <div class="step">Extracción de Características</div>
                    <div class="arrow-loop-left">⤵</div>
                </div>
            </div>
            <p style="text-align: center; margin-top: 20px;">Este modelo permite regresar a pasos anteriores si los resultados de la evaluación no son satisfactorios, por ejemplo, volviendo al preprocesamiento o a la extracción de características para refinar el modelo.</p>
        </div>
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisición de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Características:</strong> Español mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptación específica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Características:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Características:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituyó un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>

        <div class="slide hidden" data-slide="8">
            <h2>7. Análisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigación:</h3>
            <ul>
                <li>¿Existen diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¿Cómo varían las características prosódicas entre emociones?</li>
                <li>¿Qué nivel de variabilidad existe dentro de cada categoría?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribución de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gráficos de Violín de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gráficos de Dispersión 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>Nota:</strong> Las siguientes visualizaciones fueron generadas con un script de Python para analizar las características del dataset RAVDESS.
            </footer>
        </div>

        <!-- ***** INICIO DE DIAPOSITIVA ACTUALIZADA (9) ***** -->
        <div class="slide hidden" data-slide="9">
            <h2>8. Visualización: Distribución del Pitch</h2>
            <div class="single-chart-container">
                <h3>Análisis de Pitch: Distribución de Frecuencia Fundamental (F0) por Emoción</h3>
                <img src="graficas/eda/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoción">
                <p>
                    Esta gráfica nos muestra cómo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoción. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores más altos significan un tono más agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qué tan comunes son ciertos tonos para una emoción.
                </p>
                <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energía como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos más agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos más graves y monótonos.
                </footer>
            </div>
        </div>
        <!-- ***** FIN DE DIAPOSITIVA ACTUALIZADA (9) ***** -->

        <!-- ***** INICIO DE DIAPOSITIVA ACTUALIZADA (10) ***** -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualización y Análisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>Análisis Espectral: Distribución de los Primeros 13 MFCCs por Emoción</h3>
                    <img src="graficas/eda/distribucion_mfcc_por_emocion.png" alt="Gráficos de Violín de MFCCs por Emoción" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualización nos permite comparar la "forma" del sonido para cada emoción a través de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "violín" muestra el rango y la concentración de valores para un coeficiente (eje X) y una emoción (color).
                </p>
                 <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribución para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posición de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoción de otra. Coeficientes con distribuciones muy distintas entre colores son altamente informativos para la clasificación.
                </footer>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretación General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energía total o sonoridad de la señal.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles más finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <!-- ***** FIN DE DIAPOSITIVA ACTUALIZADA (10) ***** -->

        <div class="slide hidden" data-slide="11">
            <h2>10. Pipeline de Preprocesamiento de Datos</h2>
            <p>Para asegurar la calidad y consistencia de los datos antes del entrenamiento, se aplicó un pipeline de preprocesamiento con los siguientes pasos clave:</p>
            <div class="preprocessing-grid">
                <!-- Tarjeta 1: Normalización -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M2 12h3m14 0h3M12 2v3m0 14v3"/><circle cx="12" cy="12" r="7"/></svg>
                    </div>
                    <h4>Normalización de Audio</h4>
                    <p>Unificar todos los archivos a una frecuencia de muestreo de 22.05 kHz y una resolución de 16 bits para estandarizar la calidad.</p>
                </div>
                <!-- Tarjeta 2: Validación -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="20 6 9 17 4 12"></polyline></svg>
                    </div>
                    <h4>Validación de Datos</h4>
                    <p>Descartar archivos de audio corruptos, inválidos o con una duración demasiado corta (menor a 1 segundo) para el análisis.</p>
                </div>
                <!-- Tarjeta 3: Filtrado -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polygon points="22 3 2 3 10 12.46 10 19 14 21 14 12.46 22 3"></polygon></svg>
                    </div>
                    <h4>Filtrado de Ruido</h4>
                    <p>Aplicar filtros digitales para eliminar el ruido de fondo y las frecuencias que no son relevantes para el habla humana.</p>
                </div>
                <!-- Tarjeta 4: Segmentación -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.5 16.5L18 20l-1.5 1.5L13 18l-1.5 1.5L10 18l-1.5 1.5L7 18l-1.5 1.5L4 18l-1.5 1.5L1 18M3 6l1.5-1.5L6 6l1.5-1.5L9 6l1.5-1.5L12 6l1.5-1.5L15 6l1.5-1.5L18 6l1.5-1.5L21 6l1.5-1.5L24 6"/><path d="M3 10l1.5-1.5L6 10l1.5-1.5L9 10l1.5-1.5L12 10l1.5-1.5L15 10l1.5-1.5L18 10l1.5-1.5L21 10l1.5-1.5L24 10"/></svg>
                    </div>
                    <h4>Segmentación Temporal</h4>
                    <p>Dividir los audios más largos en ventanas de 3 segundos con solapamiento para aumentar la cantidad de muestras de entrenamiento.</p>
                </div>
                 <!-- Tarjeta 5: Balanceo -->
                <div class="preprocessing-step-card">
                    <div class="icon-container">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 3L2 9l10 6 10-6-10-6zM2 15l10 6 10-6M2 9l10 6 10-6"/></svg>
                    </div>
                    <h4>Balanceo de Clases</h4>
                    <p>Utilizar técnicas como SMOTE para sobremuestrear las clases minoritarias y asegurar que el modelo no se sesgue.</p>
                </div>
            </div>
            <footer class="footnote" style="margin-top: 20px;">
                <strong>SMOTE:</strong> Técnica para crear muestras sintéticas de las clases minoritarias y así balancear el dataset.
            </footer>
        </div>

        <div class="slide hidden" data-slide="12">
            <h2>11. Preprocesamiento para Reducción Dimensional</h2>
            <h3>¿Qué fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarización de Características (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    Técnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las características con rangos de valores más grandes (como el pitch) dominarían a las de rangos más pequeños (como los MFCCs), sesgando el análisis.
                </p>
            </div>
            <p>La estandarización asegura que todas las características contribuyan de manera equitativa al análisis, resultando en un modelo más justo y preciso.</p>
        </div>
        <div class="slide hidden" data-slide="13">
            <h2>12. ¿Cómo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. Cálculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las características (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">μ = (Σx) / n</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>2. Cálculo de la Desviación Estándar</h4>
                    <p>Luego, calcula la desviación estándar, que mide cuánta variación o dispersión existe respecto a la media.</p>
                     <p class="formula">σ = √[Σ(x-μ)² / n]</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>3. Transformación (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviación estándar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - μ) / σ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> Técnica de preprocesamiento que estandariza las características al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>

        <div class="slide hidden" data-slide="14">
            <h2>13. Implementación del Modelo CNN 1D</h2>
            <h3>1. Preparación de Datos (Código Real del Proyecto)</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las características
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Añadir una dimensión para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            <p class="code-description">Dividimos los datos, asegurando que cada emoción esté representada por igual en ambos conjuntos (`stratify=y`). Luego, escalamos los datos y añadimos una dimensión extra, que es el formato que espera la capa `Conv1D` de Keras.</p>
            
            <h3>2. Definición y Entrenamiento del Modelo</h3>
            <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense

model = Sequential([
    Conv1D(256, 5, padding='same', activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Conv1D(128, 5, padding='same', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Flatten(),
    Dense(y_encoded.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train_cnn, y_train, epochs=100, batch_size=64, validation_split=0.2
)</code></pre>
            <p class="code-description">Definimos la arquitectura de la CNN 1D, la compilamos con el optimizador 'adam' y la función de pérdida para clasificación multiclase. Finalmente, la entrenamos con los datos de entrenamiento, usando un 20% de estos para validación interna en cada época.</p>
        </div>

        <div class="slide hidden" data-slide="15">
            <h2>14. Reducción de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>Análisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersión de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>Análisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separación entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyección para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegría"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por sí mismo.
            </footer>
        </div>
        <div class="slide hidden" data-slide="16">
            <h2>15. Visualización 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gráfica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoción. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersión natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qué tan dispersos están los datos. <strong>Dispersión:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>
        <div class="slide hidden" data-slide="17">
            <h2>16. Visualización 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aquí, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separación entre las emociones. El resultado es una separación mucho más clara y cúmulos más compactos, lo que confirma visualmente que nuestras características son muy efectivas para la clasificación.
            </p>
        </div>
        
        <div class="slide hidden" data-slide="18">
            <h2>17. Comparación Final y Reflexión</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>Análisis de Componentes Principales (PCA)</th>
                        <th>Análisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersión general de los datos</td>
                        <td>Muestra qué tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opinión Reflexiva</h3>
            <p>Al comparar ambas técnicas, <strong>LDA demuestra ser más efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es útil para entender la estructura general de la varianza en los datos, LDA, al ser un método supervisado, logra crear proyecciones donde las emociones forman cúmulos más definidos y distinguibles. Esto sugiere que las características extraídas, cuando se proyectan con un objetivo de clasificación, son altamente discriminativas.</p>
        </div>

        <div class="slide hidden" data-slide="19">
            <h2>18. Fundamento Teórico: Red Neuronal Convolucional 1D</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>¿Cómo "Piensa" una CNN 1D?</h4>
                    <div class="code-block-container">
                        <div class="code-block-header">
                            <span class="code-title">Pseudocódigo de Operación</span>
                            <div class="window-controls">
                                <span class="control-btn minimize"></span>
                                <span class="control-btn maximize"></span>
                                <span class="control-btn close"></span>
                            </div>
                        </div>
                        <pre>
Entrada = Secuencia de Características (180 números)

<span class="comment">// 1. Capas Convolucionales 1D (Detectores de Patrones)</span>
Para cada filtro en la capa:
    Desliza el filtro sobre la secuencia
    Calcula la suma ponderada (convolución)
    Aplica función de activación (ReLU) -> Resalta patrones

<span class="comment">// 2. Capas de Pooling 1D (Compresores de Información)</span>
Reduce la longitud de la secuencia (ej. toma el valor máximo)
-> Mantiene la información más relevante y descarta el resto

<span class="comment">// 3. Capas Densas (Clasificador Final)</span>
Aplana la salida a un solo vector
Conecta todas las neuronas
Aplica Softmax para calcular probabilidades

<span class="comment">// 4. Salida</span>
Predicción = Emoción con la probabilidad más alta
                        </pre>
                    </div>
                    <div class="activation-function-info">
                        <h4>Funciones de Activación Clave</h4>
                        <p><strong>ReLU (Rectified Linear Unit):</strong> <span class="formula">ReLU(x) = max(0, x)</span>. Se usa en capas ocultas para introducir no-linealidad, permitiendo al modelo aprender relaciones complejas.</p>
                        <p><strong>Softmax:</strong> Se usa en la capa de salida para convertir las puntuaciones en una distribución de probabilidad sobre las 7 emociones.</p>
                    </div>
                </div>
        
                <div class="cnn-side-panel">
                    <div class="advantages-disadvantages-1d">
                        <h4>Ventajas del Enfoque 1D</h4>
                        <ul>
                            <li>Analiza directamente la secuencia temporal de características.</li>
                            <li>Eficiente computacionalmente, menos parámetros que una CNN 2D.</li>
                            <li>Arquitectura ideal para cualquier tipo de señal o serie de tiempo.</li>
                        </ul>
                        <h4>Desventajas</h4>
                        <ul>
                            <li>Puede perder información contextual que un espectrograma 2D sí capturaría.</li>
                            <li>Depende fuertemente de la calidad de las características extraídas manualmente.</li>
                        </ul>
                    </div>
                    <div class="cnn-audio-sample">
                        <h4>Muestra de Audio de Entrada (Disgusto)</h4>
                        <p>Este es un ejemplo del tipo de audio que procesa el modelo antes de la extracción de características.</p>
                        <audio controls src="presentation/src/audio/03-01-07-02-02-02-11.wav">
                            Tu navegador no soporta el elemento de audio.
                        </audio>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="20">
            <h2>19. Arquitectura del Modelo CNN 1D</h2>
            <p>Esta es la arquitectura de Keras que implementamos. Keras es una librería de alto nivel que facilita la construcción de redes neuronales. La red está diseñada para procesar la secuencia de 180 características extraídas de cada audio y aprender a clasificar la emoción.</p>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <p>Secuencia de Características<br><span class="layer-shape">(180 características, 1)</span></p>
                    <p class="layer-desc">Recibe el vector de 180 características como una secuencia temporal.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box conv">
                    <h4>Conv1D (256 filtros) + ReLU</h4>
                    <p class="layer-desc">La primera capa convolucional busca patrones temporales simples en la secuencia de entrada usando 256 filtros diferentes.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Reduce la longitud de la secuencia a una quinta parte, manteniendo solo las activaciones de patrones más fuertes para la siguiente capa.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box conv">
                    <h4>Conv1D (128 filtros) + ReLU</h4>
                    <p class="layer-desc">Una segunda capa convolucional busca patrones más complejos y de mayor duración a partir de las características detectadas por la capa anterior.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Vuelve a reducir la secuencia para condensar aún más la información.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box dense">
                    <h4>Flatten & Capa Densa</h4>
                    <p class="layer-desc">La capa Flatten convierte la secuencia final en un solo vector largo, que se conecta a una capa densa para la clasificación.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box output">
                    <h4>Capa de Salida (Softmax)</h4>
                    <p class="layer-desc">La capa final tiene 7 neuronas, una por cada emoción, y usa Softmax para asignar una probabilidad a cada una.</p>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="21">
            <h2>20. Resumen de Neuronas y Parámetros</h2>
            <div class="cnn-details">
                <p>
                    Esta tabla detalla la estructura de nuestro modelo. La **"Forma de Salida"** muestra cómo cambian las dimensiones de los datos después de cada capa. Los **"Parámetros"** son los pesos o "conocimientos" que el modelo aprende durante el entrenamiento. En total, nuestro modelo debe aprender y ajustar **171,783 parámetros** para poder realizar la clasificación.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa (Tipo)</th>
                            <th>Forma de Salida</th>
                            <th>Parámetros</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>conv1d</td>
                            <td>(None, 180, 256)</td>
                            <td>1,536</td>
                        </tr>
                        <tr>
                            <td>max_pooling1d</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>conv1d_1</td>
                            <td>(None, 36, 128)</td>
                            <td>163,968</td>
                        </tr>
                         <tr>
                            <td>max_pooling1d_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>flatten</td>
                            <td>(None, 896)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dense</td>
                            <td>(None, 7)</td>
                            <td>6,279</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="2"><strong>Total de Parámetros Entrenables</strong></td>
                            <td><strong>171,783</strong></td>
                        </tr>
                    </tbody>
                </table>
                 <footer class="cnn-table-footnote">
                    <strong>Parámetros:</strong> Son los "conocimientos" internos de la red que se ajustan durante el entrenamiento.
                </footer>
            </div>
        </div>

        <!-- Diapositivas Finales -->
        <div class="slide hidden" data-slide="22">
            <h2>21. Recursos</h2>
            <div class="dataset-card">
                <h3>Lenguaje y Frameworks</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal</li>
                    <li><strong>TensorFlow/Keras:</strong> Desarrollo de CNN 1D</li>
                    <li><strong>Librosa:</strong> Extracción de características de audio</li>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y métricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Hardware y Plataformas</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU.</li>
                    <li><strong>GitHub:</strong> Control de versiones.</li>
                </ul>
            </div>
        </div>
        <div class="slide hidden" data-slide="23">
            <h2>22. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo CNN 1D para clasificar 7 emociones.</li>
                        <li>Pipeline de extracción de 180 características.</li>
                        <li>Comparación visual 3D de PCA y LDA.</li>
                        <li>Métricas de rendimiento del modelo.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>Análisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicación de usuario final.</li>
                        <li>El modelo no opera en tiempo real.</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="slide hidden farewell-slide" data-slide="24">
            <h1 class="farewell-title">¡Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de características y clasificar emociones complejas en la voz.
            </p>
            <div class="highlight-box">
                <h3>🤔 ¿Preguntas o Comentarios?</h3>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro Pérez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">← Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente →</button>
    </div>

    <script src="presentation/src/js/script.js"></script>
</body>
</html>
