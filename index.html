<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation\src\css\styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 48</div>

    <div class="presentation-container">
        
        <!-- DIAPOSITIVA 1: TÍTULO -->
        <div class="slide" data-slide="1">
            <h1>Análisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Acústicos para la Clasificación del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque técnico para la modelación de características prosódicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro Pérez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Gutiérrez</strong></p>
            </div>
        </div>

        <!-- DIAPOSITIVA 2: JUSTIFICACIÓN -->
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificación Técnica</h2>
            <div class="highlight-box">
                <h3>¿Por qué Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma acústica compleja, rica en información latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Características Prosódicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribución de energía en las frecuencias (formantes) varía sistemáticamente con la emoción.</p></div>
                <div class="stat-item"><h3>Señal No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con técnicas de IA.</p></div>
            </div>
            <p><strong>¿Por qué Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracción de Patrones:</strong> Capacidad para identificar automáticamente características complejas en espectrogramas, indetectables para el análisis tradicional.</li>
                <li><strong>Análisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificación consistente y reproducible de las características vocales.</li>
                <li><strong>Modelado de Alta Dimensión:</strong> Habilidad para procesar miles de características extraídas de una sola señal de audio.</li>
            </ul>
            <footer class="footnote">
                <strong>Prosódicas:</strong> Características relacionadas con el ritmo, entonación y acentuación del habla. <strong>Espectrales:</strong> Propiedades relacionadas con la distribución de frecuencias en la señal de audio.
            </footer>
        </div>

        <!-- DIAPOSITIVA 3: PROBLEMA -->
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripción del Problema</h2>
            <div class="highlight-box">
                <h3>Problema Técnico Principal</h3>
                <p>El desafío de clasificar estados emocionales a partir de la señal del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¿Qué reto técnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones acústicos para 7 emociones distintas: <strong>alegría, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabación.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la señal hasta la clasificación.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente. <strong>Alta dimensionalidad:</strong> Datos con muchas características o variables, lo que aumenta la complejidad computacional.
            </footer>
        </div>

        <!-- DIAPOSITIVA 4: OBJETIVO GENERAL -->
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificación de emociones humanas a partir del análisis de características acústicas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la señal hasta la predicción del modelo.</p>
            </div>
        </div>

        <!-- DIAPOSITIVA 5: OBJETIVOS ESPECÍFICOS -->
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Específicos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar múltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar características acústicas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Diseñar, entrenar y optimizar modelos especializados de aprendizaje automático</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando métricas estándar de clasificación</div>
            <div class="methodology-step"><strong>5.</strong> Implementar técnicas de reducción de dimensionalidad y visualización</div>
            <footer class="footnote">
                <strong>Reducción de dimensionalidad:</strong> Técnicas para disminuir el número de variables en un dataset manteniendo la información más importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 6: METODOLOGÍA -->
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodología Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo cíclico, que nos permite refinar y mejorar continuamente nuestro modelo basándonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentación y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisición de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. Análisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracción de Características</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluación y Resultados</h4>
                </div>
            </div>
            <footer class="footnote">
                <strong>Metodología iterativa:</strong> Proceso de desarrollo que se repite en ciclos, permitiendo mejoras continuas basadas en resultados previos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 7: DATASETS -->
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisición de Conjuntos de Datos</h2>
            <div class="dataset-card">
                <h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 864 grabaciones de audio</li>
                    <li><strong>Características:</strong> Español mexicano, 6 emociones + neutralidad</li>
                    <li><strong>Utilidad:</strong> Adaptación específica al habla mexicana</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 2,496 grabaciones vocales</li>
                    <li><strong>Características:</strong> Calidad profesional, 24 actores</li>
                    <li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 4,800 muestras de audio</li>
                    <li><strong>Características:</strong> Alta calidad, actrices entrenadas</li>
                    <li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Dataset:</strong> Conjunto de datos organizados para entrenamiento de modelos. <strong>Benchmarks:</strong> Estándares de referencia para medir el rendimiento de un modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 8: DISTRIBUCIÓN FINAL -->
        <div class="slide hidden" data-slide="8">
            <h2>7. Distribución Final de Datos</h2>
            <div class="highlight-box">
                <h3>📈 Total de archivos procesados: 7,296</h3>
                <p>Después de la integración y limpieza de todos los datasets</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>1,184</h3><p>Fearful, Happy, Angry, Sad, Disgust (cada una)</p></div>
                <div class="stat-item"><h3>992</h3><p>Neutral</p></div>
                <div class="stat-item"><h3>384</h3><p>Surprised</p></div>
            </div>
            <h3>Ventajas de esta distribución:</h3>
            <ul>
                <li><strong>Balanceada:</strong> La mayoría de emociones tienen representación similar</li>
                <li><strong>Suficiente:</strong> Cada emoción tiene al menos 384 muestras para entrenamiento</li>
                <li><strong>Diversa:</strong> Múltiples hablantes, idiomas y contextos</li>
                <li><strong>Calidad:</strong> Grabaciones profesionales y controladas</li>
            </ul>
            <footer class="footnote">
                <strong>Distribución balanceada:</strong> Cuando todas las clases o categorías tienen aproximadamente la misma cantidad de ejemplos en el dataset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 9: EDA -->
        <div class="slide hidden" data-slide="9">
            <h2>8. Análisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigación:</h3>
            <ul>
                <li>¿Existen diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¿Cómo varían las características prosódicas entre emociones?</li>
                <li>¿Qué nivel de variabilidad existe dentro de cada categoría?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribución de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gráficos de Violín de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gráficos de Dispersión 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>EDA:</strong> Análisis Exploratorio de Datos, proceso de examinar datasets para descubrir patrones y relaciones. <strong>Pitch:</strong> Frecuencia fundamental de la voz, relacionada con qué tan aguda o grave suena.
            </footer>
        </div>

        <!-- DIAPOSITIVA 10: PITCH -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualización: Distribución del Pitch</h2>
            <div class="single-chart-container">
                <h3>Análisis de Pitch: Distribución de Frecuencia Fundamental (F0) por Emoción</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoción">
                <p>
                    Esta gráfica nos muestra cómo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoción. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores más altos significan un tono más agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qué tan comunes son ciertos tonos para una emoción.
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energía como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos más agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos más graves y monótonos.
                </div>
            </div>
            <footer class="footnote">
                <strong>Frecuencia fundamental (F0):</strong> La frecuencia más baja de una señal periódica, determina el pitch percibido. <strong>Hertz (Hz):</strong> Unidad de medida de frecuencia, equivale a ciclos por segundo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 11: MFCCS -->
        <div class="slide hidden" data-slide="11">
            <h2>10. Visualización y Análisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>Análisis Espectral: Distribución de los Primeros 13 MFCCs por Emoción</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gráficos de Violín de MFCCs por Emoción" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualización nos permite comparar la "forma" del sonido para cada emoción a través de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "violín" muestra el rango y la concentración de valores para un coeficiente (eje X) y una emoción (color).
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribución para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posición de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoción de otra.
                </div>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretación General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energía total o sonoridad de la señal.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles más finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>MFCCs:</strong> Coeficientes que capturan las características del espectro de frecuencias de manera similar a como el oído humano percibe el sonido. <strong>Cepstral:</strong> Análisis en el dominio del cepstrum, útil para separar la fuente del filtro en señales de voz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 12: FLUJO DE PROCESAMIENTO -->
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoción, debemos traducir la onda de sonido a un lenguaje que entienda: los números. Este proceso se llama <strong>Extracción de Características</strong>. A continuación, veremos el paso a paso de cómo convertimos un archivo de audio en un único vector de 180 características.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">🔊</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">#️⃣</div>
                    <div class="flow-text">Digitalización (Muestreo)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">🖼️</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">📊</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                     <div class="flow-icon">🧬</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Vector:</strong> Lista ordenada de números que representa las características extraídas del audio. <strong>Frames:</strong> Pequeños segmentos de audio analizados individualmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 13: DIGITALIZACIÓN -->
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 1: Digitalización y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalización</h4>
                    <p>Una onda de sonido es una señal analógica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de números, donde cada número es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es estática. Para analizarla, la dividimos en pequeños segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tamaño de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colección de muchos arreglos pequeños (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio wave sampling and framing.png" alt="Diagrama de Digitalización y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en múltiples ventanas (frames) para su análisis.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Muestreo:</strong> Proceso de convertir una señal continua en una secuencia discreta de valores. <strong>Amplitud:</strong> Intensidad o fuerza de la onda sonora en un momento dado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 14: FFT -->
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¿Qué es y para qué sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qué frecuencias la componen. La <strong>Transformada Rápida de Fourier (FFT)</strong> es la herramienta matemática que lo hace posible.</p>
                    <p>La FFT descompone la señal del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energía vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qué tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matemático: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un número complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El número total de muestras en la ventana.</li>
                            <li>$k$: El índice de la frecuencia que se está calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time domain to frequency domain FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>FFT:</strong> Algoritmo eficiente para calcular la Transformada Discreta de Fourier. <strong>Dominio del tiempo vs. frecuencia:</strong> Dos formas de representar la misma señal, enfocándose en cuándo ocurren los eventos vs. qué frecuencias contiene.
            </footer>
        </div>

        <!-- DIAPOSITIVA 15: MFCCs -->
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Más allá del Espectro</h4>
                    <p>El espectro de la FFT es útil, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho más inteligente de resumir la información del espectro, imitando cómo funciona el oído humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logarítmica, similar a nuestra percepción auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energías, de nuevo, para imitar cómo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operación que comprime toda esa información espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripción numérica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC block diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Escala Mel:</strong> Escala perceptual de frecuencias que imita cómo el oído humano percibe diferencias en el tono. <strong>DCT:</strong> Transformada que convierte señales al dominio de frecuencia usando solo funciones coseno. <strong>Timbre:</strong> Cualidad que diferencia sonidos con el mismo pitch y volumen.
            </footer>
        </div>

        <!-- DIAPOSITIVA 16: OPERACIÓN PROMEDIO -->
        <div class="slide hidden" data-slide="16">
            <h2 class="technical-title">Paso 4: Operación de Promedio para Vector Final</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del Análisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de características, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 características.</p>
                    <p>Para obtener un único vector $V$ que represente todo el audio, calculamos la media de cada característica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">Cálculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                            <li>$V_j$: Es el valor final de la característica $j$ en nuestro vector.</li>
                            <li>$T$: Es el número total de ventanas (frames) en el audio.</li>
                            <li>$M_{t,j}$: Es el valor de la característica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                    <h4>¿Por qué usar el promedio?</h4>
                   <ul>
                        <li><strong>Reducción temporal:</strong> Condensa información variable en el tiempo a un valor estable</li>
                        <li><strong>Robustez:</strong> El promedio es menos sensible a valores atípicos en ventanas individuales</li>
                        <li><strong>Representatividad:</strong> Captura las características dominantes del audio completo</li>
                        <li><strong>Compatibilidad:</strong> Produce un vector de tamaño fijo para cualquier duración de audio</li>
                    </ul>
                   <p>Este proceso condensa la información temporal en una sola "ficha técnica" que describe las propiedades acústicas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature matrix averaging to vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
            <footer class="footnote">
                <strong>Promedio aritmético:</strong> Suma de todos los valores dividida entre el número total de valores. <strong>Vector de tamaño fijo:</strong> Representación numérica con dimensiones constantes, independiente de la duración del audio original.
            </footer>
        </div>

        <!-- DIAPOSITIVA 17: NORMALIZACIÓN DE AUDIO -->
        <div class="slide hidden" data-slide="17">
            <h2 class="technical-title">Normalización de Audio: ¿Por qué es Crucial?</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¿Qué es la Normalización de Audio?</h4>
                    <p>La normalización ajusta la amplitud de una señal de audio para que su valor máximo sea 1.0 y el mínimo sea -1.0, estandarizando el volumen entre diferentes grabaciones.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Fórmula de Normalización</span>
                        <p>$x_{norm}[n] = \frac{x[n]}{\max(|x[n]|)}$</p>
                        <ul>
                            <li>$x[n]$: Señal de audio original</li>
                            <li>$x_{norm}[n]$: Señal normalizada</li>
                            <li>$\max(|x[n]|)$: Valor absoluto máximo de la señal</li>
                        </ul>
                    </div>

                    <h4>Ventajas Críticas de la Normalización:</h4>
                    <div class="advantages-disadvantages">
                        <div class="advantages">
                            <h4>✅ Beneficios Técnicos</h4>
                            <ul>
                                <li><strong>Consistencia de Volumen:</strong> Elimina diferencias de grabación entre micrófonos y entornos</li>
                                <li><strong>Estabilidad Numérica:</strong> Previene overflow y underflow en cálculos posteriores</li>
                                <li><strong>Mejor Convergencia:</strong> Los algoritmos de ML convergen más rápido con datos normalizados</li>
                                <li><strong>Robustez:</strong> Reduce sensibilidad a variaciones de hardware de grabación</li>
                            </ul>
                        </div>
                        <div class="disadvantages">
                            <h4>⚠️ Sin Normalización</h4>
                            <ul>
                                <li><strong>Sesgo por Volumen:</strong> Grabaciones más fuertes dominarían el entrenamiento</li>
                                <li><strong>Características Distorsionadas:</strong> MFCCs y otras características serían inconsistentes</li>
                                <li><strong>Gradientes Inestables:</strong> Entrenamiento del modelo sería errático</li>
                                <li><strong>Clasificación Sesgada:</strong> El modelo podría aprender a clasificar por volumen, no por emoción</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Overflow/Underflow:</strong> Errores numéricos cuando los valores son demasiado grandes o pequeños para ser representados. <strong>Convergencia:</strong> Proceso por el cual un algoritmo de aprendizaje alcanza una solución estable.
            </footer>
        </div>

        <!-- DIAPOSITIVA 18: PREPROCESAMIENTO -->
        <div class="slide hidden" data-slide="18">
            <h2>11. Preprocesamiento para Reducción Dimensional</h2>
            <h3>¿Qué fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarización de Características (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    Técnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las características con rangos de valores más grandes (como el pitch) dominarían a las de rangos más pequeños (como los MFCCs), sesgando el análisis.
                </p>
            </div>
            <p>La estandarización asegura que todas las características contribuyan de manera equitativa al análisis, resultando en un modelo más justo y preciso.</p>
            <footer class="footnote">
                <strong>PCA:</strong> Análisis de Componentes Principales, técnica para reducir dimensiones preservando varianza. <strong>LDA:</strong> Análisis Discriminante Lineal, reduce dimensiones maximizando separación entre clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 19: STANDARD SCALER -->
        <div class="slide hidden" data-slide="19">
            <h2>12. ¿Cómo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. Cálculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las características (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">μ = (Σx) / n</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>2. Cálculo de la Desviación Estándar</h4>
                    <p>Luego, calcula la desviación estándar, que mide cuánta variación o dispersión existe respecto a la media.</p>
                     <p class="formula">σ = √[Σ(x-μ)² / n]</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>3. Transformación (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviación estándar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - μ) / σ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> Técnica de preprocesamiento que estandariza las características al remover la media y escalar a una varianza unitaria. <strong>Z-score:</strong> Medida estadística que indica cuántas desviaciones estándar está un valor de la media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 20: PCA VS LDA -->
        <div class="slide hidden" data-slide="20">
            <h2>13. Reducción de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>Análisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersión de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>Análisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separación entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyección para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegría"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por sí mismo. <strong>Varianza:</strong> Medida de dispersión que indica cuánto se alejan los datos de su media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 21: PCA 3D -->
        <div class="slide hidden" data-slide="21">
            <h2>14. Visualización 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gráfica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoción. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersión natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Componentes Principales:</strong> Nuevos ejes calculados que capturan la máxima variabilidad de los datos originales. <strong>Proyección:</strong> Transformación de datos de alta dimensión a un espacio de menor dimensión.
            </footer>
        </div>

        <!-- DIAPOSITIVA 22: LDA 3D -->
        <div class="slide hidden" data-slide="22">
            <h2>15. Visualización 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aquí, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separación entre las emociones. El resultado es una separación mucho más clara y cúmulos más compactos, lo que confirma visualmente que nuestras características son muy efectivas para la clasificación.
            </p>
            <footer class="footnote">
                <strong>Discriminantes Lineales (LD):</strong> Combinaciones lineales de las características originales que mejor separan las clases. <strong>Cúmulos:</strong> Agrupaciones de puntos similares en el espacio de características.
            </footer>
        </div>

        <!-- DIAPOSITIVA 23: COMPARACIÓN PCA LDA -->
        <div class="slide hidden" data-slide="23">
            <h2>16. Comparación Final y Reflexión</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>Análisis de Componentes Principales (PCA)</th>
                        <th>Análisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersión general de los datos</td>
                        <td>Muestra qué tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opinión Reflexiva</h3>
            <p>Al comparar ambas técnicas, <strong>LDA demuestra ser más efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es útil para entender la estructura general de la varianza en los datos, LDA, al ser un método supervisado, logra crear proyecciones donde las emociones forman cúmulos más definidos y distinguibles.</p>
            <footer class="footnote">
                <strong>Separabilidad:</strong> Medida de qué tan bien se pueden distinguir diferentes clases en un espacio de características.
            </footer>
        </div>

        <!-- DIAPOSITIVA 24: INTRODUCCIÓN CNN 1D -->
        <div class="slide hidden" data-slide="24">
            <h2>17. Introducción a las Redes Neuronales Convolucionales 1D</h2>
            <div class="highlight-box">
                <h3>¿Qué es una CNN 1D?</h3>
                <p>Una Red Neuronal Convolucional 1D es un tipo especializado de red neuronal diseñada para procesar secuencias de datos, como series temporales o características extraídas de audio.</p>
            </div>
            <h3>¿Por qué CNN 1D para Audio?</h3>
            <ul>
                <li><strong>Detección de Patrones Locales:</strong> Identifica patrones en secuencias cortas de características</li>
                <li><strong>Invarianza Traslacional:</strong> Reconoce patrones independientemente de su posición en la secuencia</li>
                <li><strong>Jerarquía de Características:</strong> Capas superiores detectan patrones más complejos</li>
                <li><strong>Eficiencia Computacional:</strong> Menos parámetros que redes densas equivalentes</li>
            </ul>
            <h3>Nuestra Arquitectura</h3>
            <p>Diseñamos una CNN 1D que toma como entrada un vector de 180 características y produce probabilidades para 7 emociones diferentes.</p>
            <footer class="footnote">
                <strong>CNN:</strong> Convolutional Neural Network, tipo de red neuronal que usa operaciones de convolución. <strong>1D:</strong> Unidimensional, opera sobre secuencias lineales de datos. <strong>Invarianza traslacional:</strong> Propiedad que permite reconocer patrones sin importar su posición.
            </footer>
        </div>

        <!-- DIAPOSITIVA 25: CAPA DE ENTRADA -->
        <div class="slide hidden" data-slide="25">
            <h2>18. Capa de Entrada (Input Layer)</h2>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Entrada:</strong> <span class="output-shape">(None, 180, 1)</span></p>
                        <p><strong>Neuronas:</strong> 180</p>
                        <p><strong>Función:</strong> Recibir el vector de características</p>
                        <p><strong>Parámetros:</strong> 0 (no aprende, solo recibe datos)</p>
                    </div>
                </div>
            </div>
            <h3>Detalles Técnicos</h3>
            <ul>
                <li><strong>180 características:</strong> MFCCs (40) + Chroma (12) + Mel-spectrograms (128)</li>
                <li><strong>Dimensión 1:</strong> Cada característica es un valor escalar</li>
                <li><strong>None (batch):</strong> Permite procesar múltiples audios simultáneamente</li>
                <li><strong>Formato secuencial:</strong> Las características mantienen su orden temporal promediado</li>
            </ul>
            <h3>Preprocesamiento Previo</h3>
            <p>Antes de entrar a la red, los datos fueron:</p>
            <ul>
                <li>✅ Normalizados (StandardScaler)</li>
                <li>✅ Reformateados a shape (180, 1)</li>
                <li>✅ Divididos en train/validation/test</li>
            </ul>
            <footer class="footnote">
                <strong>Batch:</strong> Conjunto de muestras procesadas simultáneamente para eficiencia computacional. <strong>Shape:</strong> Dimensiones de los datos (filas, columnas, canales). <strong>Escalar:</strong> Un solo número, no un vector o matriz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 26: PRIMERA CONVOLUCIÓN -->
        <div class="slide hidden" data-slide="26">
            <h2>19. Primera Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_1 (128 filtros, kernel=5)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 1)</p>
                        <p><strong>Salida:</strong> (None, 180, 128)</p>
                        <p><strong>Neuronas:</strong> 23,040 (180 × 128)</p>
                        <p><strong>Parámetros:</strong> 768</p>
                        <p><strong>Activación:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¿Qué hace esta capa?</h3>
            <ul>
                <li><strong>128 filtros diferentes:</strong> Cada uno aprende a detectar un patrón específico</li>
                <li><strong>Kernel size 5:</strong> Cada filtro analiza 5 características consecutivas</li>
                <li><strong>Padding 'same':</strong> Mantiene la longitud original (180)</li>
                <li><strong>ReLU:</strong> Introduce no-linealidad, activa solo valores positivos</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Convolución</span>
                <p>Para cada posición $i$ y filtro $f$:</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{4} w_j^f \cdot x_{i+j} + b^f\right)$</p>
                <ul>
                    <li>$w_j^f$: Peso $j$ del filtro $f$</li>
                    <li>$x_{i+j}$: Característica en posición $i+j$</li>
                    <li>$b^f$: Bias del filtro $f$</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Filtro/Kernel:</strong> Conjunto de pesos que se desliza sobre los datos para detectar patrones. <strong>Padding:</strong> Técnica para mantener las dimensiones después de la convolución. <strong>Bias:</strong> Parámetro adicional que permite ajustar el umbral de activación.
            </footer>
        </div>

        <!-- DIAPOSITIVA 27: FUNCIÓN RELU -->
        <div class="slide hidden" data-slide="27">
            <h2>20. Función de Activación ReLU</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Rectified Linear Unit (ReLU)</h4>
                    <p>ReLU es la función de activación más popular en redes neuronales profundas debido a su simplicidad y efectividad.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Definición Matemática</span>
                        <p>$f(x) = \max(0, x) = \begin{cases} 
                        x & \text{si } x > 0 \\
                        0 & \text{si } x \leq 0
                        \end{cases}$</p>
                    </div>

                    <h4>¿Por qué ReLU es tan efectiva?</h4>
                    <div class="stats-grid">
                        <div class="stat-item">
                            <h3>Simplicidad</h3>
                            <p>Computacionalmente muy eficiente, solo requiere una comparación</p>
                        </div>
                        <div class="stat-item">
                            <h3>No Saturación</h3>
                            <p>No se satura para valores positivos, evita el problema del gradiente desvaneciente</p>
                        </div>
                        <div class="stat-item">
                            <h3>Esparsidad</h3>
                            <p>Produce activaciones dispersas (muchos ceros), mejorando la eficiencia</p>
                        </div>
                    </div>

                    <h4>Efectos en Nuestro Modelo</h4>
                    <ul>
                        <li><strong>Filtrado de Ruido:</strong> Elimina activaciones negativas (ruido)</li>
                        <li><strong>Detección de Características:</strong> Solo permite pasar patrones "positivos"</li>
                        <li><strong>No-linealidad:</strong> Permite al modelo aprender relaciones complejas</li>
                        <li><strong>Convergencia Rápida:</strong> Facilita el entrenamiento eficiente</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Activación:</strong> Función que determina si una neurona debe ser activada basándose en su entrada. <strong>Gradiente desvaneciente:</strong> Problema donde los gradientes se vuelven muy pequeños, ralentizando el aprendizaje. <strong>Saturación:</strong> Cuando una función se vuelve plana y deja de cambiar significativamente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 28: BATCH NORMALIZATION -->
        <div class="slide hidden" data-slide="28">
            <h2>21. Normalización por Lotes (Batch Normalization)</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm" style="background: linear-gradient(135deg, var(--color-accent), var(--color-secondary));">
                    <h4>Batch Normalization</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 128)</p>
                        <p><strong>Salida:</strong> (None, 180, 128)</p>
                        <p><strong>Neuronas:</strong> 23,040 (180 × 128)</p>
                        <p><strong>Parámetros:</strong> 512 (γ y β para cada canal)</p>
                    </div>
                </div>
            </div>
            
            <div class="formula-block">
                <span class="formula-title">Operación de Batch Normalization</span>
                <p>Para cada característica en el batch:</p>
                <p>$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$</p>
                <p>$y = \gamma \hat{x} + \beta$</p>
                <ul>
                    <li>$\mu_B$: Media del batch</li>
                    <li>$\sigma_B^2$: Varianza del batch</li>
                    <li>$\gamma, \beta$: Parámetros aprendibles</li>
                    <li>$\epsilon$: Constante pequeña para estabilidad numérica</li>
                </ul>
            </div>

            <h3>Beneficios de Batch Normalization</h3>
            <ul>
                <li><strong>Estabiliza el entrenamiento:</strong> Reduce la sensibilidad a la inicialización de pesos</li>
                <li><strong>Acelera convergencia:</strong> Permite usar learning rates más altos</li>
                <li><strong>Efecto regularizador:</strong> Reduce overfitting de forma natural</li>
                <li><strong>Gradientes estables:</strong> Mantiene distribuciones de activación consistentes</li>
            </ul>
            <footer class="footnote">
                <strong>Batch Normalization:</strong> Técnica que normaliza las entradas de cada capa usando estadísticas del mini-batch. <strong>Learning rate:</strong> Velocidad de aprendizaje del modelo. <strong>Regularización:</strong> Técnicas para prevenir overfitting.
            </footer>
        </div>

        <!-- DIAPOSITIVA 29: MAX POOLING -->
        <div class="slide hidden" data-slide="29">
            <h2>22. Capa de Max Pooling 1D</h2>
            <div class="neural-architecture">
                <div class="layer-box pool">
                    <h4>MaxPooling1D (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 128)</p>
                        <p><strong>Neuronas:</strong> 11,520 (90 × 128)</p>
                        <p><strong>Parámetros:</strong> 0 (sin aprendizaje)</p>
                        <p><strong>Reducción:</strong> 50% en dimensión temporal</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operación MaxPooling</span>
                <p>Para cada ventana de tamaño 2:</p>
                <p>$y[i] = \max(x[2i], x[2i+1])$</p>
                <p>Se toma el valor máximo de cada par de elementos consecutivos</p>
            </div>

            <h3>¿Por qué MaxPooling?</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Ventajas</h4>
                    <ul>
                        <li><strong>Reduce dimensionalidad:</strong> 180 → 90 elementos</li>
                        <li><strong>Invarianza local:</strong> Robustez a pequeños desplazamientos</li>
                        <li><strong>Extrae características dominantes:</strong> Preserva activaciones más fuertes</li>
                        <li><strong>Eficiencia computacional:</strong> Menos parámetros en capas siguientes</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>⚠️ Consideraciones</h4>
                    <ul>
                        <li><strong>Pérdida de información:</strong> Descarta 50% de los datos</li>
                        <li><strong>Pérdida de ubicación exacta:</strong> Solo preserva la característica más fuerte</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Max Pooling:</strong> Operación que toma el valor máximo de una ventana de datos, reduciendo dimensionalidad. <strong>Invarianza:</strong> Propiedad de mantener el resultado ante pequeños cambios en la entrada.
            </footer>
        </div>

        <!-- DIAPOSITIVA 30: DROPOUT -->
        <div class="slide hidden" data-slide="30">
            <h2>23. Capa de Dropout</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 90, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 128)</p>
                        <p><strong>Neuronas:</strong> 11,520 (90 × 128)</p>
                        <p><strong>Parámetros:</strong> 0</p>
                        <p><strong>Neuronas desactivadas:</strong> 30% aleatoriamente</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operación de Dropout</span>
                <p>Durante el entrenamiento:</p>
                <p>$y_i = \begin{cases} 
                0 & \text{con probabilidad } p = 0.3 \\
                \frac{x_i}{1-p} & \text{con probabilidad } 1-p = 0.7
                \end{cases}$</p>
                <p>Durante la inferencia: $y_i = x_i$ (sin dropout)</p>
            </div>

            <h3>¿Cómo Previene el Overfitting?</h3>
            <ul>
                <li><strong>Ensembles implícitos:</strong> Cada forward pass usa una red ligeramente diferente</li>
                <li><strong>Reduce co-adaptación:</strong> Las neuronas no pueden depender unas de otras</li>
                <li><strong>Generalización:</strong> Fuerza al modelo a ser robusto ante pérdida de información</li>
                <li><strong>Regularización estocástica:</strong> Añade ruido controlado durante entrenamiento</li>
            </ul>

            <h3>Impacto en Nuestro Modelo</h3>
            <p>Con 30% de dropout:</p>
            <ul>
                <li>De 11,520 neuronas → ~8,064 activas por iteración</li>
                <li>Configuraciones posibles: 2^11,520 (número astronómico)</li>
                <li>Cada batch entrena un "submodelo" diferente</li>
            </ul>
            <footer class="footnote">
                <strong>Overfitting:</strong> Cuando el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Co-adaptación:</strong> Dependencia excesiva entre neuronas que reduce robustez. <strong>Estocástico:</strong> Que involucra aleatoriedad o probabilidad.
            </footer>
        </div>

        <!-- DIAPOSITIVA 31: SEGUNDA CONVOLUCIÓN -->
        <div class="slide hidden" data-slide="31">
            <h2>24. Segunda Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_2 (64 filtros, kernel=5)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 90, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 64)</p>
                        <p><strong>Neuronas:</strong> 5,760 (90 × 64)</p>
                        <p><strong>Parámetros:</strong> 41,024</p>
                        <p><strong>Función:</strong> Detectar patrones de nivel superior</p>
                    </div>
                </div>
            </div>

            <h3>Diferencias con la Primera Capa</h3>
            <div class="comparison-table">
                <table style="width: 100%;">
                    <thead>
                        <tr>
                            <th>Aspecto</th>
                            <th>Conv1D_1</th>
                            <th>Conv1D_2</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Filtros</strong></td>
                            <td>128</td>
                            <td>64</td>
                        </tr>
                        <tr>
                            <td><strong>Entrada</strong></td>
                            <td>1 canal (características)</td>
                            <td>128 canales (mapas de características)</td>
                        </tr>
                        <tr>
                            <td><strong>Detecta</strong></td>
                            <td>Patrones básicos</td>
                            <td>Combinaciones de patrones</td>
                        </tr>
                        <tr>
                            <td><strong>Parámetros</strong></td>
                            <td>768</td>
                            <td>41,024</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="formula-block">
                <span class="formula-title">Cálculo de Parámetros</span>
                <p>Para Conv1D_2:</p>
                <p>$(kernel\_size \times input\_channels \times output\_channels) + output\_channels$</p>
                <p>$(5 \times 128 \times 64) + 64 = 40,960 + 64 = 41,024$</p>
            </div>

            <h3>¿Qué Aprende Esta Capa?</h3>
            <ul>
                <li><strong>Patrones complejos:</strong> Combinaciones de las características detectadas en Conv1D_1</li>
                <li><strong>Representaciones abstractas:</strong> Características más específicas de emociones</li>
                <li><strong>Jerarquía:</strong> Desde características básicas hacia conceptos emocionales</li>
            </ul>
            <footer class="footnote">
                <strong>Mapas de características:</strong> Salidas de filtros convolucionales que representan la presencia de patrones específicos. <strong>Jerarquía de características:</strong> Proceso donde capas más profundas aprenden conceptos más abstractos y complejos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 32: GLOBAL AVERAGE POOLING -->
        <div class="slide hidden" data-slide="32">
            <h2>25. Global Average Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pool" style="background: linear-gradient(135deg, var(--color-primary), var(--color-accent));">
                    <h4>Global Average Pooling 1D</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 45, 64)</p>
                        <p><strong>Salida:</strong> (None, 64)</p>
                        <p><strong>Neuronas:</strong> 64</p>
                        <p><strong>Parámetros:</strong> 0</p>
                        <p><strong>Operación:</strong> Promedio de toda la secuencia temporal</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operación Global Average Pooling</span>
                <p>Para cada canal $c$:</p>
                <p>$y_c = \frac{1}{L} \sum_{i=1}^{L} x_{i,c}$</p>
                <ul>
                    <li>$L$: Longitud de la secuencia (45)</li>
                    <li>$x_{i,c}$: Valor en posición $i$ del canal $c$</li>
                    <li>$y_c$: Valor promedio del canal $c$</li>
                </ul>
            </div>

            <h3>Ventajas sobre Flatten Tradicional</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Global Average Pooling</h4>
                    <ul>
                        <li><strong>Menos parámetros:</strong> 64 características vs 2,880 con Flatten</li>
                        <li><strong>Previene overfitting:</strong> Menos complejidad del modelo</li>
                        <li><strong>Invarianza a longitud:</strong> Funciona con secuencias de diferente tamaño</li>
                        <li><strong>Interpretabilidad:</strong> Cada canal representa una característica global</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>⚠️ Flatten Traditional</h4>
                    <ul>
                        <li><strong>Muchos parámetros:</strong> 45×64 = 2,880 características</li>
                        <li><strong>Propenso a overfitting:</strong> Más complejidad</li>
                        <li><strong>Dependiente de posición:</strong> Sensible al orden exacto</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Global Average Pooling:</strong> Técnica que calcula el promedio de cada mapa de características en toda la secuencia temporal. <strong>Flatten:</strong> Operación que convierte matrices multidimensionales en vectores 1D. <strong>Interpretabilidad:</strong> Facilidad para entender qué está aprendiendo el modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 33: CAPA DENSA INTERMEDIA -->
        <div class="slide hidden" data-slide="33">
            <h2>26. Capa Densa Intermedia</h2>
            <div class="neural-architecture">
                <div class="layer-box dense">
                    <h4>Dense Intermedia (64 neuronas)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 64)</p>
                        <p><strong>Salida:</strong> (None, 64)</p>
                        <p><strong>Neuronas:</strong> 64</p>
                        <p><strong>Parámetros:</strong> 4,160</p>
                        <p><strong>Activación:</strong> ReLU</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operación de Capa Densa</span>
                <p>$y = ReLU(W \cdot x + b)$</p>
                <ul>
                    <li>$W$: Matriz de pesos (64×64)</li>
                    <li>$x$: Vector de entrada (64 elementos)</li>
                    <li>$b$: Vector de bias (64 elementos)</li>
                    <li>Parámetros totales: $(64 \times 64) + 64 = 4,160$</li>
                </ul>
            </div>

            <h3>¿Por qué una Capa Intermedia?</h3>
            <ul>
                <li><strong>Procesamiento adicional:</strong> Permite combinaciones no-lineales de características</li>
                <li><strong>Capacidad de representación:</strong> Añade flexibilidad al modelo</li>
                <li><strong>Transición suave:</strong> Gradual desde características hacia clasificación</li>
                <li><strong>Regularización:</strong> Con dropout, previene overfitting</li>
            </ul>

            <h3>Patrón de Arquitectura</h3>
            <p>Seguimos el patrón común:</p>
            <div class="methodology-step">Características extraídas → Procesamiento intermedio → Clasificación final</div>
            <footer class="footnote">
                <strong>Capa densa/fully connected:</strong> Capa donde cada neurona está conectada a todas las neuronas de la capa anterior. <strong>Matriz de pesos:</strong> Tabla de números que determinan la fuerza de las conexiones entre neuronas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 34: CAPA DE SALIDA -->
        <div class="slide hidden" data-slide="34">
            <h2>27. Capa de Salida con Softmax</h2>
            <div class="neural-architecture">
                <div class="layer-box output">
                    <h4>Capa de Salida (7 emociones)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 64)</p>
                        <p><strong>Salida:</strong> (None, 7)</p>
                        <p><strong>Neuronas:</strong> 7</p>
                        <p><strong>Parámetros:</strong> 455</p>
                        <p><strong>Activación:</strong> Softmax</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Función Softmax</span>
                <p>Para cada emoción $i$:</p>
                <p>$P(emoción_i) = \frac{e^{z_i}}{\sum_{j=1}^{7} e^{z_j}}$</p>
                <ul>
                    <li>$z_i$: Puntuación (logit) para la emoción $i$</li>
                    <li>$P(emoción_i)$: Probabilidad de la emoción $i$</li>
                    <li>$\sum_{i=1}^{7} P(emoción_i) = 1$: Las probabilidades suman 1</li>
                </ul>
            </div>

            <h3>Las 7 Emociones de Salida</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Angry</h3><p>Neurona 0</p></div>
                <div class="stat-item"><h3>Disgust</h3><p>Neurona 1</p></div>
                <div class="stat-item"><h3>Fearful</h3><p>Neurona 2</p></div>
                <div class="stat-item"><h3>Happy</h3><p>Neurona 3</p></div>
                <div class="stat-item"><h3>Neutral</h3><p>Neurona 4</p></div>
                <div class="stat-item"><h3>Sad</h3><p>Neurona 5</p></div>
                <div class="stat-item"><h3>Surprised</h3><p>Neurona 6</p></div>
            </div>

            <h3>Ejemplo de Predicción</h3>
            <p><strong>Logits:</strong> [2.1, -0.5, 3.2, 0.8, -1.2, 1.5, 0.3]</p>
            <p><strong>Softmax:</strong> [0.196, 0.015, 0.589, 0.053, 0.007, 0.107, 0.032]</p>
            <p><strong>Predicción:</strong> Fearful (58.9% de confianza)</p>
            <footer class="footnote">
                <strong>Softmax:</strong> Función que convierte puntuaciones en probabilidades válidas. <strong>Logits:</strong> Puntuaciones brutas antes de aplicar softmax. <strong>Distribución de probabilidad:</strong> Conjunto de probabilidades que suman 1.
            </footer>
        </div>

        <!-- DIAPOSITIVA 35: RESUMEN ARQUITECTURA -->
        <div class="slide hidden" data-slide="35">
            <h2>28. Resumen Completo de la Arquitectura</h2>
            <div class="cnn-details">
                <p>
                    Nuestra CNN 1D procesa secuencialmente 180 características de audio a través de múltiples capas especializadas, transformando gradualmente datos numéricos en predicciones emocionales.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa</th>
                            <th>Tipo</th>
                            <th>Entrada → Salida</th>
                            <th>Neuronas</th>
                            <th>Parámetros</th>
                            <th>Función Principal</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Input</strong></td>
                            <td>Entrada</td>
                            <td>(180, 1)</td>
                            <td>180</td>
                            <td>0</td>
                            <td>Recibir características</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_1</strong></td>
                            <td>Convolución</td>
                            <td>(180, 1) → (180, 128)</td>
                            <td>23,040</td>
                            <td>768</td>
                            <td>Detectar patrones básicos</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_1</strong></td>
                            <td>Normalización</td>
                            <td>(180, 128) → (180, 128)</td>
                            <td>23,040</td>
                            <td>512</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_1</strong></td>
                            <td>Pooling</td>
                            <td>(180, 128) → (90, 128)</td>
                            <td>11,520</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_1</strong></td>
                            <td>Regularización</td>
                            <td>(90, 128) → (90, 128)</td>
                            <td>11,520</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_2</strong></td>
                            <td>Convolución</td>
                            <td>(90, 128) → (90, 64)</td>
                            <td>5,760</td>
                            <td>41,024</td>
                            <td>Patrones complejos</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_2</strong></td>
                            <td>Normalización</td>
                            <td>(90, 64) → (90, 64)</td>
                            <td>5,760</td>
                            <td>256</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_2</strong></td>
                            <td>Pooling</td>
                            <td>(90, 64) → (45, 64)</td>
                            <td>2,880</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_2</strong></td>
                            <td>Regularización</td>
                            <td>(45, 64) → (45, 64)</td>
                            <td>2,880</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>GlobalAvgPool</strong></td>
                            <td>Pooling Global</td>
                            <td>(45, 64) → (64)</td>
                            <td>64</td>
                            <td>0</td>
                            <td>Condensar características</td>
                        </tr>
                        <tr>
                            <td><strong>Dense_Inter</strong></td>
                            <td>Densa</td>
                            <td>(64) → (64)</td>
                            <td>64</td>
                            <td>4,160</td>
                            <td>Procesamiento intermedio</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_Final</strong></td>
                            <td>Regularización</td>
                            <td>(64) → (64)</td>
                            <td>64</td>
                            <td>0</td>
                            <td>Regularización final</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Clasificación</td>
                            <td>(64) → (7)</td>
                            <td>7</td>
                            <td>455</td>
                            <td>Predicción emocional</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="4"><strong>TOTAL</strong></td>
                            <td><strong>47,175</strong></td>
                            <td><strong>Clasificación de emociones</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>Parámetros totales:</strong> 47,175 pesos y bias que el modelo debe aprender durante el entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 36: OPTIMIZADOR ADAM -->
        <div class="slide hidden" data-slide="36">
            <h2>29. Optimizador ADAM</h2>
            <div class="highlight-box">
                <h3>Adaptive Moment Estimation (ADAM)</h3>
                <p>ADAM combina las ventajas de AdaGrad y RMSprop, adaptando el learning rate individualmente para cada parámetro.</p>
            </div>

            <div class="formula-block">
                <span class="formula-title">Algoritmo ADAM Completo</span>
                <p><strong>Inicialización:</strong> $m_0 = 0$, $v_0 = 0$, $t = 0$</p>
                <p><strong>En cada iteración:</strong></p>
                <p>$t = t + 1$</p>
                <p>$g_t = \nabla_{\theta} J(\theta_{t-1})$ (gradiente)</p>
                <p>$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ (momento primer orden)</p>
                <p>$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ (momento segundo orden)</p>
                <p>$\hat{m}_t = \frac{m_t}{1-\beta_1^t}$ (corrección bias momento)</p>
                <p>$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$ (corrección bias varianza)</p>
                <p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$ (actualización)</p>
            </div>

            <h3>Configuración en Nuestro Modelo</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>α = 0.001</h3><p>Learning rate</p></div>
                <div class="stat-item"><h3>β₁ = 0.9</h3><p>Momento primer orden</p></div>
                <div class="stat-item"><h3>β₂ = 0.999</h3><p>Momento segundo orden</p></div>
                <div class="stat-item"><h3>ε = 1e-7</h3><p>Estabilidad numérica</p></div>
            </div>
            <footer class="footnote">
                <strong>Optimizador:</strong> Algoritmo que ajusta los pesos del modelo para minimizar la función de pérdida. <strong>Learning rate:</strong> Velocidad de aprendizaje del modelo. <strong>Momento:</strong> Técnica que usa información de gradientes pasados para acelerar convergencia.
            </footer>
        </div>

        <!-- DIAPOSITIVA 37: FUNCIÓN DE PÉRDIDA -->
        <div class="slide hidden" data-slide="37">
            <h2>30. Función de Pérdida: Categorical Crossentropy</h2>
            <div class="formula-block">
                <span class="formula-title">Categorical Crossentropy</span>
                <p>$\mathcal{L} = -\sum_{i=1}^{7} y_i \log(\hat{y}_i)$</p>
                <ul>
                    <li>$y_i$: Etiqueta verdadera (one-hot encoded)</li>
                    <li>$\hat{y}_i$: Probabilidad predicha para la clase $i$</li>
                    <li>Solo la clase verdadera contribuye a la pérdida</li>
                </ul>
            </div>

            <h3>¿Por qué Categorical Crossentropy?</h3>
            <ul>
                <li><strong>Clasificación multiclase:</strong> Ideal para problemas con múltiples categorías exclusivas</li>
                <li><strong>Penalización logarítmica:</strong> Penaliza fuertemente predicciones incorrectas muy confiadas</li>
                <li><strong>Compatible con Softmax:</strong> Funciona perfectamente con la activación de salida</li>
                <li><strong>Derivadas suaves:</strong> Facilita el backpropagation</li>
            </ul>

            <h3>Ejemplo de Cálculo</h3>
            <div class="methodology-step">
                <strong>Situación:</strong> Audio real = "Happy", predicción = [0.1, 0.05, 0.15, 0.6, 0.05, 0.03, 0.02]
            </div>
            <div class="methodology-step">
                <strong>One-hot real:</strong> [0, 0, 0, 1, 0, 0, 0]
            </div>
            <div class="methodology-step">
                <strong>Pérdida:</strong> $-\log(0.6) = 0.511$ (relativamente baja, buena predicción)
            </div>
            <footer class="footnote">
                <strong>One-hot encoding:</strong> Representación donde solo un elemento es 1 y el resto son 0. <strong>Backpropagation:</strong> Algoritmo para calcular gradientes y entrenar redes neuronales. <strong>Logaritmo natural:</strong> Función matemática que crece más lentamente, penalizando errores grandes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 38: PROCESO DE ENTRENAMIENTO -->
        <div class="slide hidden" data-slide="38">
            <h2>31. Proceso de Entrenamiento</h2>
            <div class="highlight-box">
                <h3>Configuración de Entrenamiento</h3>
                <p>El modelo se entrenó durante 50 épocas con monitoreo continuo del rendimiento en validación.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>50</h3><p>Épocas entrenadas</p></div>
                <div class="stat-item"><h3>64</h3><p>Tamaño de batch</p></div>
                <div class="stat-item"><h3>4,668</h3><p>Muestras entrenamiento</p></div>
                <div class="stat-item"><h3>1,168</h3><p>Muestras validación</p></div>
            </div>

            <h3>División de Datos</h3>
            <ul>
                <li><strong>Entrenamiento (64%):</strong> 4,668 muestras para aprender patrones</li>
                <li><strong>Validación (16%):</strong> 1,168 muestras para ajustar hiperparámetros</li>
                <li><strong>Prueba (20%):</strong> 1,460 muestras para evaluación final</li>
            </ul>

            <h3>Callbacks Utilizados</h3>
            <div class="methodology-step"><strong>ModelCheckpoint:</strong> Guarda el mejor modelo basado en accuracy de validación</div>
            <div class="methodology-step"><strong>ReduceLROnPlateau:</strong> Reduce learning rate cuando no hay mejora</div>
            <div class="methodology-step"><strong>EarlyStopping:</strong> Detiene entrenamiento si hay overfitting</div>
            <footer class="footnote">
                <strong>Época:</strong> Una pasada completa por todo el dataset de entrenamiento. <strong>Batch:</strong> Subconjunto de datos procesado simultáneamente. <strong>Callbacks:</strong> Funciones que se ejecutan durante el entrenamiento para monitoreo y control.
            </footer>
        </div>

        <!-- DIAPOSITIVA 39: RESULTADOS FINALES -->
        <div class="slide hidden" data-slide="39">
            <h2>32. Resultados del Entrenamiento</h2>
            <div class="highlight-box">
                <h3>🎉 Rendimiento Excepcional Alcanzado</h3>
                <p>El modelo logró una precisión del <strong>89.73%</strong> en el conjunto de prueba, superando expectativas iniciales.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>89.73%</h3><p>Precisión en prueba</p></div>
                <div class="stat-item"><h3>89.04%</h3><p>Precisión final entrenamiento</p></div>
                <div class="stat-item"><h3>89.38%</h3><p>Precisión final validación</p></div>
                <div class="stat-item"><h3>50</h3><p>Épocas completadas</p></div>
            </div>

            <h3>Interpretación de Resultados</h3>
            <ul>
                <li><strong>Excelente generalización:</strong> Precisión similar en train, validation y test</li>
                <li><strong>Sin overfitting:</strong> Las curvas de entrenamiento y validación son consistentes</li>
                <li><strong>Convergencia estable:</strong> El modelo aprendió de forma progresiva y estable</li>
                <li><strong>Robustez:</strong> Funciona bien con datos no vistos durante entrenamiento</li>
            </ul>

            <h3>Comparación con Benchmarks</h3>
            <p>Nuestro resultado de 89.73% es competitivo con modelos state-of-the-art en reconocimiento de emociones en voz, considerando:</p>
            <ul>
                <li>Múltiples datasets combinados</li>
                <li>7 clases emocionales</li>
                <li>Arquitectura relativamente simple (47K parámetros)</li>
            </ul>
            <footer class="footnote">
                <strong>Generalización:</strong> Capacidad del modelo de funcionar bien con datos nuevos no vistos durante entrenamiento. <strong>State-of-the-art:</strong> El mejor rendimiento actual en el campo de investigación. <strong>Benchmarks:</strong> Estándares de comparación en la literatura científica.
            </footer>
        </div>

        <!-- DIAPOSITIVA 40: MATRIZ DE CONFUSIÓN -->
        <div class="slide hidden" data-slide="40">
            <h2>33. Análisis Detallado por Emoción</h2>
            <div class="single-chart-container">
                <h3>Matriz de Confusión - Rendimiento por Emoción</h3>
                <img src="/content/confusion_matrix.png" alt="Matriz de Confusión" style="max-width: 90%; border-radius: 15px;">
            </div>
            
            <h3>Reporte de Clasificación</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Emoción</th>
                        <th>Precisión</th>
                        <th>Recall</th>
                        <th>F1-Score</th>
                        <th>Soporte</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Angry</strong></td>
                        <td>0.89</td>
                        <td>0.93</td>
                        <td>0.91</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Disgust</strong></td>
                        <td>0.89</td>
                        <td>0.92</td>
                        <td>0.90</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Fearful</strong></td>
                        <td>0.94</td>
                        <td>0.89</td>
                        <td>0.91</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Happy</strong></td>
                        <td>0.92</td>
                        <td>0.85</td>
                        <td>0.88</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Neutral</strong></td>
                        <td>0.93</td>
                        <td>0.96</td>
                        <td>0.94</td>
                        <td>198</td>
                    </tr>
                    <tr>
                        <td><strong>Sad</strong></td>
                        <td>0.92</td>
                        <td>0.86</td>
                        <td>0.89</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Surprised</strong></td>
                        <td>0.67</td>
                        <td>0.83</td>
                        <td>0.74</td>
                        <td>77</td>
                    </tr>
                </tbody>
            </table>

            <h3>Análisis de Rendimiento</h3>
            <ul>
                <li><strong>Mejor clasificada:</strong> Neutral (F1: 0.94) - Características muy distintivas</li>
                <li><strong>Más desafiante:</strong> Surprised (F1: 0.74) - Menos muestras de entrenamiento</li>
                <li><strong>Rendimiento balanceado:</strong> Angry, Disgust, Fearful (~0.90 F1-Score)</li>
                <li><strong>Confusiones comunes:</strong> Happy ↔ Surprised, Sad ↔ Fearful</li>
            </ul>
            <footer class="footnote">
                <strong>Precisión:</strong> Porcentaje de predicciones correctas para una clase. <strong>Recall:</strong> Porcentaje de casos reales detectados correctamente. <strong>F1-Score:</strong> Media armónica entre precisión y recall. <strong>Soporte:</strong> Número de muestras reales de cada clase.
            </footer>
        </div>

        <!-- DIAPOSITIVA 41: CURVAS DE ENTRENAMIENTO -->
        <div class="slide hidden" data-slide="41">
            <h2>34. Curvas de Entrenamiento</h2>
            <div class="single-chart-container">
                <h3>Evolución del Entrenamiento - Loss y Accuracy</h3>
                <img src="/content/training_curves.png" alt="Curvas de Entrenamiento" style="max-width: 100%; border-radius: 15px;">
            </div>

            <h3>Interpretación de las Curvas</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>✅ Indicadores Positivos</h4>
                    <ul>
                        <li><strong>Convergencia estable:</strong> Pérdida disminuye consistentemente</li>
                        <li><strong>Sin overfitting:</strong> Train y validation siguen trayectorias similares</li>
                        <li><strong>Precisión creciente:</strong> Mejora progresiva hasta ~90%</li>
                        <li><strong>Estabilización:</strong> Alcanza plateau cerca de la época 40</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>📊 Observaciones</h4>
                    <ul>
                        <li><strong>Convergencia lenta inicial:</strong> Primeras 10 épocas con mejora gradual</li>
                        <li><strong>Estabilización final:</strong> Últimas épocas sin mejora significativa</li>
                        <li><strong>Variabilidad validación:</strong> Ligeras fluctuaciones normales</li>
                    </ul>
                </div>
            </div>

            <h3>Conclusiones del Entrenamiento</h3>
            <ul>
                <li><strong>Entrenamiento exitoso:</strong> El modelo aprendió efectivamente los patrones</li>
                <li><strong>Buena generalización:</strong> Rendimiento similar en train y validation</li>
                <li><strong>Punto óptimo alcanzado:</strong> ~50 épocas fueron suficientes</li>
                <li><strong>Arquitectura adecuada:</strong> Complejidad apropiada para el problema</li>
            </ul>
            <footer class="footnote">
                <strong>Loss/Pérdida:</strong> Medida de qué tan incorrectas son las predicciones del modelo. <strong>Accuracy:</strong> Porcentaje de predicciones correctas. <strong>Plateau:</strong> Zona donde la métrica se estabiliza sin mejoras significativas. <strong>Época:</strong> Una pasada completa por todos los datos de entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 42: IMPLEMENTACIÓN TÉCNICA -->
        <div class="slide hidden" data-slide="42">
            <h2>35. Implementación del Modelo CNN 1D</h2>
            <h3>1. Preparación de Datos (Código Real del Proyecto)</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Preprocesamiento de Datos</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las características
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Añadir una dimensión para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            </div>
            <p class="code-description">Dividimos los datos estratificadamente, estandarizamos y reformateamos para CNN 1D. La dimensión extra es requerida por las capas Conv1D de Keras.</p>
            
            <h3>2. Definición y Entrenamiento del Modelo</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Arquitectura del Modelo</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D, BatchNormalization

model = Sequential([
    Conv1D(128, 5, padding='same', activation='relu', input_shape=(180, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    Conv1D(64, 5, padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    GlobalAveragePooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.15),
    Dense(7, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>
            </div>
            <p class="code-description">Arquitectura completa con BatchNormalization y GlobalAveragePooling para mejor rendimiento y menor overfitting.</p>
            <footer class="footnote">
                <strong>Estratificado:</strong> Mantiene la proporción de clases en train y test. <strong>expand_dims:</strong> Añade una dimensión para compatibilidad con Conv1D. <strong>Sequential:</strong> Modelo donde las capas se apilan secuencialmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 43: FUNCIÓN DE PREDICCIÓN -->
        <div class="slide hidden" data-slide="43">
            <h2>36. Función de Predicción en Tiempo Real</h2>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Predicción de Emociones</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>def predict_emotion(audio_file_path, model, scaler, label_encoder, extractor):
    """
    Predice la emoción de un archivo de audio usando el modelo entrenado
    """
    try:
        # Extraer características del audio
        features = extractor.extract_features(audio_file_path)
        if features is None:
            return {"error": "No se pudieron extraer características"}
        
        # Preprocesar características
        features_scaled = scaler.transform(features.reshape(1, -1))
        features_cnn = np.expand_dims(features_scaled, axis=2)
        
        # Predecir con el modelo
        predictions = model.predict(features_cnn, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        
        # Decodificar resultado
        predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]
        
        return {
            "predicted_emotion": predicted_emotion,
            "confidence": confidence,
            "all_probabilities": {
                emotion: float(prob) 
                for emotion, prob in zip(label_encoder.classes_, predictions[0])
            }
        }
        
    except Exception as e:
        return {"error": f"Error en predicción: {str(e)}"}

# Ejemplo de uso
result = predict_emotion("audio.wav", model, scaler, label_encoder, extractor)
print(f"Emoción: {result['predicted_emotion']}")
print(f"Confianza: {result['confidence']:.2%}")</code></pre>
            </div>
            <p class="code-description">Función completa que toma un archivo de audio y devuelve la emoción predicha con su nivel de confianza.</p>
            <footer class="footnote">
                <strong>Pipeline de predicción:</strong> Secuencia completa desde audio crudo hasta predicción final. <strong>Verbose:</strong> Parámetro que controla la cantidad de información mostrada durante ejecución. <strong>inverse_transform:</strong> Convierte códigos numéricos de vuelta a etiquetas originales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 44: RECURSOS TÉCNICOS -->
        <div class="slide hidden" data-slide="44">
            <h2>37. Recursos y Tecnologías Utilizadas</h2>
            <div class="dataset-card">
                <h3>Lenguajes y Frameworks Principales</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal de desarrollo</li>
                    <li><strong>TensorFlow 2.18.0:</strong> Framework de deep learning</li>
                    <li><strong>Keras:</strong> API de alto nivel para construcción de modelos</li>
                    <li><strong>NumPy:</strong> Computación numérica eficiente</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Librerías de Procesamiento de Audio</h3>
                <ul>
                    <li><strong>Librosa:</strong> Extracción de características de audio</li>
                    <li><strong>SciPy:</strong> Procesamiento de señales</li>
                    <li><strong>PyAudio:</strong> Manejo de streams de audio</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Herramientas de Análisis y Visualización</h3>
                <ul>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y métricas de evaluación</li>
                    <li><strong>Matplotlib/Seaborn:</strong> Visualización de datos y resultados</li>
                    <li><strong>Pandas:</strong> Manipulación y análisis de datos</li>
                    <li><strong>Plotly:</strong> Visualizaciones interactivas 3D</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Plataforma de Desarrollo</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU gratuita</li>
                    <li><strong>Kaggle:</strong> Fuente de datasets y kernels de referencia</li>
                    <li><strong>GitHub:</strong> Control de versiones y colaboración</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Framework:</strong> Conjunto de herramientas y librerías que facilitan el desarrollo. <strong>API:</strong> Interfaz de programación que simplifica el uso de funcionalidades complejas. <strong>GPU:</strong> Unidad de procesamiento gráfico, acelera el entrenamiento de redes neuronales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 45: ALCANCE Y LIMITACIONES -->
        <div class="slide hidden" data-slide="45">
            <h2>38. Alcance del Proyecto y Limitaciones</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>✅ Incluido en el Proyecto</h3>
                    <ul>
                        <li><strong>Modelo CNN 1D completo:</strong> Arquitectura optimizada para clasificar 7 emociones</li>
                        <li><strong>Pipeline de extracción:</strong> 180 características avanzadas (MFCCs + Chroma + Mel)</li>
                        <li><strong>Análisis comparativo:</strong> Visualización 3D de PCA vs LDA</li>
                        <li><strong>Evaluación exhaustiva:</strong> Métricas detalladas y matriz de confusión</li>
                        <li><strong>Función de predicción:</strong> Sistema listo para audio en tiempo real</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>⚠️ Limitaciones y Exclusiones</h3>
                    <ul>
                        <li><strong>Solo modalidad de audio:</strong> No incluye análisis de texto o video</li>
                        <li><strong>Sin interfaz de usuario:</strong> No se desarrolló aplicación web/móvil</li>
                        <li><strong>No tiempo real:</strong> Requiere procesamiento previo del audio</li>
                        <li><strong>Idiomas limitados:</strong> Entrenado principalmente en inglés</li>
                        <li><strong>Duración fija:</strong> Optimizado para clips de 2-4 segundos</li>
                    </ul>
                </div>
            </div>

            <h3>Consideraciones Técnicas</h3>
            <ul>
                <li><strong>Hardware requerido:</strong> Mínimo 4GB RAM para inferencia, 8GB+ para entrenamiento</li>
                <li><strong>Tiempo de procesamiento:</strong> ~2-5ms por audio en CPU, <1ms en GPU</li>
                <li><strong>Formato de entrada:</strong> Archivos WAV de 22.05kHz recomendados</li>
                <li><strong>Escalabilidad:</strong> Modelo puede procesar batches para mayor eficiencia</li>
            </ul>
            <footer class="footnote">
                <strong>Modalidad:</strong> Tipo de dato de entrada (audio, texto, imagen, etc.). <strong>Tiempo real:</strong> Procesamiento instantáneo mientras se recibe la entrada. <strong>Inferencia:</strong> Proceso de hacer predicciones con un modelo ya entrenado. <strong>Batch:</strong> Procesamiento de múltiples muestras simultáneamente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 46: MÉTRICAS AVANZADAS -->
        <div class="slide hidden" data-slide="46">
            <h2>39. Métricas de Evaluación Detalladas</h2>
            <div class="formula-block">
                <span class="formula-title">Métricas de Clasificación Multiclase</span>
                <p><strong>Accuracy Global:</strong> $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{1310}{1460} = 89.73\%$</p>
                <p><strong>Precision por clase:</strong> $\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}$</p>
                <p><strong>Recall por clase:</strong> $\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}$</p>
                <p><strong>F1-Score:</strong> $F1_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}$</p>
            </div>

            <h3>Análisis de Rendimiento Avanzado</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Macro Avg</h3><p>F1: 0.88<br>Promedio simple</p></div>
                <div class="stat-item"><h3>Weighted Avg</h3><p>F1: 0.90<br>Ponderado por soporte</p></div>
                <div class="stat-item"><h3>Kappa Score</h3><p>κ ≈ 0.87<br>Acuerdo ajustado</p></div>
                <div class="stat-item"><h3>Top-2 Accuracy</h3><p>~96%<br>Predicción correcta en top-2</p></div>
            </div>

            <h3>Interpretación de Métricas</h3>
            <ul>
                <li><strong>Macro vs Weighted:</strong> Diferencia mínima indica balance entre clases</li>
                <li><strong>Kappa alto (0.87):</strong> Acuerdo substancial, descarta coincidencia casual</li>
                <li><strong>Top-2 accuracy:</strong> En 96% de casos, la emoción correcta está entre las 2 más probables</li>
                <li><strong>Consistencia cross-métrica:</strong> Todas las métricas son coherentemente altas</li>
            </ul>

            <h3>Comparación con Literatura</h3>
            <p>Nuestro F1-Score de 0.90 es competitivo comparado con estudios recientes:</p>
            <ul>
                <li>RNN + LSTM: 85-88% (Issa et al., 2020)</li>
                <li>CNN 2D + Spectrograms: 87-91% (Zhao et al., 2019)</li>
                <li><strong>Nuestro CNN 1D: 90%</strong> (Este trabajo)</li>
            </ul>
            <footer class="footnote">
                <strong>Macro avg:</strong> Promedio simple de métricas por clase. <strong>Weighted avg:</strong> Promedio ponderado por número de muestras. <strong>Kappa:</strong> Medida de acuerdo que considera la concordancia por azar. <strong>Top-k accuracy:</strong> Métrica que considera correctas las k predicciones más probables.
            </footer>
        </div>

        <!-- DIAPOSITIVA 47: ANÁLISIS DE COMPLEJIDAD -->
        <div class="slide hidden" data-slide="47">
            <h2>40. Análisis de Complejidad Computacional</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Complejidad Temporal por Operación</h4>
                    
                    <div class="formula-block">
                        <span class="formula-title">Análisis Big O del Modelo</span>
                        <p><strong>Conv1D_1:</strong> $O(L \times K \times C_{in} \times C_{out}) = O(180 \times 5 \times 1 \times 128) = O(115,200)$</p>
                        <p><strong>Conv1D_2:</strong> $O(90 \times 5 \times 128 \times 64) = O(3,686,400)$</p>
                        <p><strong>Dense layers:</strong> $O(64^2 + 64 \times 7) = O(4,544)$</p>
                        <p><strong>Total por muestra:</strong> $O(3,806,144)$ ≈ $O(3.8M)$ operaciones</p>
                    </div>

                    <h4>Memoria Requerida</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Componente</th>
                                <th>Tamaño (MB)</th>
                                <th>Descripción</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Parámetros del modelo</td>
                                <td>0.18</td>
                                <td>47,175 × 4 bytes (float32)</td>
                            </tr>
                            <tr>
                                <td>Activaciones (batch=64)</td>
                                <td>3.2</td>
                                <td>Almacenamiento intermedio</td>
                            </tr>
                            <tr>
                                <td>Gradientes (entrenamiento)</td>
                                <td>0.18</td>
                                <td>Mismo tamaño que parámetros</td>
                            </tr>
                            <tr>
                                <td><strong>Total mínimo (inferencia)</strong></td>
                                <td><strong>3.4</strong></td>
                                <td>Muy eficiente en memoria</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3>Escalabilidad y Deployment</h3>
            <ul>
                <li><strong>Edge Computing:</strong> Suficientemente ligero para dispositivos móviles</li>
                <li><strong>Batch Processing:</strong> Eficiencia aumenta linealmente con tamaño de batch</li>
                <li><strong>Paralelización:</strong> Compatible con GPU para procesamiento masivo</li>
                <li><strong>Cuantización:</strong> Potencial reducción a INT8 (75% menos memoria)</li>
            </ul>
            <footer class="footnote">
                <strong>Big O:</strong> Notación que describe la complejidad algoritmica en función del tamaño de entrada. <strong>Edge Computing:</strong> Procesamiento en dispositivos locales en lugar de servidores remotos. <strong>Cuantización:</strong> Reducir precisión numérica para ahorrar memoria y acelerar inferencia.
            </footer>
        </div>

        <!-- DIAPOSITIVA 48: AGRADECIMIENTOS -->
        <div class="slide hidden" data-slide="48">
            <h1 class="farewell-title">¡Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de características y clasificar emociones complejas en la voz humana.
            </p>
            <div class="highlight-box">
                <h3>🤔 ¿Preguntas o Comentarios?</h3>
                <p>Hemos cubierto desde los fundamentos matemáticos hasta la implementación técnica completa del modelo CNN 1D para reconocimiento de emociones en audio, incluyendo análisis detallado de cada componente arquitectural.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>89.73%</h3><p>Precisión Final</p></div>
                <div class="stat-item"><h3>47,175</h3><p>Parámetros Totales</p></div>
                <div class="stat-item"><h3>7,296</h3><p>Audios Procesados</p></div>
                <div class="stat-item"><h3>180</h3><p>Características Extraídas</p></div>
            </div>

            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro Pérez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>

            <div class="methodology-step" style="margin-top: 30px; text-align: center;">
                <strong>Repositorio del Proyecto:</strong> <a href="https://github.com/your-repo" style="color: var(--color-primary);">github.com/emotion-recognition-cnn1d</a>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">← Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente →</button>
    </div>

    <script src="presentation\src\js\script.js"></script>
</body>
</html>