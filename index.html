<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation\src\css\styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 48</div>

    <div class="presentation-container">
        
        <!-- DIAPOSITIVA 1: T√çTULO -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>

        <!-- DIAPOSITIVA 2: JUSTIFICACI√ìN -->
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
            <footer class="footnote">
                <strong>Pros√≥dicas:</strong> Caracter√≠sticas relacionadas con el ritmo, entonaci√≥n y acentuaci√≥n del habla. <strong>Espectrales:</strong> Propiedades relacionadas con la distribuci√≥n de frecuencias en la se√±al de audio.
            </footer>
        </div>

        <!-- DIAPOSITIVA 3: PROBLEMA -->
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente. <strong>Alta dimensionalidad:</strong> Datos con muchas caracter√≠sticas o variables, lo que aumenta la complejidad computacional.
            </footer>
        </div>

        <!-- DIAPOSITIVA 4: OBJETIVO GENERAL -->
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>

        <!-- DIAPOSITIVA 5: OBJETIVOS ESPEC√çFICOS -->
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
            <footer class="footnote">
                <strong>Reducci√≥n de dimensionalidad:</strong> T√©cnicas para disminuir el n√∫mero de variables en un dataset manteniendo la informaci√≥n m√°s importante.
            </footer>
        </div>

        <!-- DIAPOSITIVA 6: METODOLOG√çA -->
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, que nos permite refinar y mejorar continuamente nuestro modelo bas√°ndonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentaci√≥n y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisici√≥n de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. An√°lisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracci√≥n de Caracter√≠sticas</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluaci√≥n y Resultados</h4>
                </div>
            </div>
            <footer class="footnote">
                <strong>Metodolog√≠a iterativa:</strong> Proceso de desarrollo que se repite en ciclos, permitiendo mejoras continuas basadas en resultados previos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 7: DATASETS -->
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card">
                <h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 864 grabaciones de audio</li>
                    <li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li>
                    <li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 2,496 grabaciones vocales</li>
                    <li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li>
                    <li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3>
                <ul>
                    <li><strong>Cantidad:</strong> 4,800 muestras de audio</li>
                    <li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li>
                    <li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Dataset:</strong> Conjunto de datos organizados para entrenamiento de modelos. <strong>Benchmarks:</strong> Est√°ndares de referencia para medir el rendimiento de un modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 8: DISTRIBUCI√ìN FINAL -->
        <div class="slide hidden" data-slide="8">
            <h2>7. Distribuci√≥n Final de Datos</h2>
            <div class="highlight-box">
                <h3>üìà Total de archivos procesados: 7,296</h3>
                <p>Despu√©s de la integraci√≥n y limpieza de todos los datasets</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>1,184</h3><p>Fearful, Happy, Angry, Sad, Disgust (cada una)</p></div>
                <div class="stat-item"><h3>992</h3><p>Neutral</p></div>
                <div class="stat-item"><h3>384</h3><p>Surprised</p></div>
            </div>
            <h3>Ventajas de esta distribuci√≥n:</h3>
            <ul>
                <li><strong>Balanceada:</strong> La mayor√≠a de emociones tienen representaci√≥n similar</li>
                <li><strong>Suficiente:</strong> Cada emoci√≥n tiene al menos 384 muestras para entrenamiento</li>
                <li><strong>Diversa:</strong> M√∫ltiples hablantes, idiomas y contextos</li>
                <li><strong>Calidad:</strong> Grabaciones profesionales y controladas</li>
            </ul>
            <footer class="footnote">
                <strong>Distribuci√≥n balanceada:</strong> Cuando todas las clases o categor√≠as tienen aproximadamente la misma cantidad de ejemplos en el dataset.
            </footer>
        </div>

        <!-- DIAPOSITIVA 9: EDA -->
        <div class="slide hidden" data-slide="9">
            <h2>8. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribuci√≥n de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gr√°ficos de Viol√≠n de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>EDA:</strong> An√°lisis Exploratorio de Datos, proceso de examinar datasets para descubrir patrones y relaciones. <strong>Pitch:</strong> Frecuencia fundamental de la voz, relacionada con qu√© tan aguda o grave suena.
            </footer>
        </div>

        <!-- DIAPOSITIVA 10: PITCH -->
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Pitch: Distribuci√≥n de Frecuencia Fundamental (F0) por Emoci√≥n</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica nos muestra c√≥mo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoci√≥n. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores m√°s altos significan un tono m√°s agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qu√© tan comunes son ciertos tonos para una emoci√≥n.
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energ√≠a como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos m√°s agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos m√°s graves y mon√≥tonos.
                </div>
            </div>
            <footer class="footnote">
                <strong>Frecuencia fundamental (F0):</strong> La frecuencia m√°s baja de una se√±al peri√≥dica, determina el pitch percibido. <strong>Hertz (Hz):</strong> Unidad de medida de frecuencia, equivale a ciclos por segundo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 11: MFCCS -->
        <div class="slide hidden" data-slide="11">
            <h2>10. Visualizaci√≥n y An√°lisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>An√°lisis Espectral: Distribuci√≥n de los Primeros 13 MFCCs por Emoci√≥n</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gr√°ficos de Viol√≠n de MFCCs por Emoci√≥n" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualizaci√≥n nos permite comparar la "forma" del sonido para cada emoci√≥n a trav√©s de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "viol√≠n" muestra el rango y la concentraci√≥n de valores para un coeficiente (eje X) y una emoci√≥n (color).
                </p>
                <div class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribuci√≥n para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posici√≥n de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoci√≥n de otra.
                </div>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretaci√≥n General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energ√≠a total o sonoridad de la se√±al.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles m√°s finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>MFCCs:</strong> Coeficientes que capturan las caracter√≠sticas del espectro de frecuencias de manera similar a como el o√≠do humano percibe el sonido. <strong>Cepstral:</strong> An√°lisis en el dominio del cepstrum, √∫til para separar la fuente del filtro en se√±ales de voz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 12: FLUJO DE PROCESAMIENTO -->
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoci√≥n, debemos traducir la onda de sonido a un lenguaje que entienda: los n√∫meros. Este proceso se llama <strong>Extracci√≥n de Caracter√≠sticas</strong>. A continuaci√≥n, veremos el paso a paso de c√≥mo convertimos un archivo de audio en un √∫nico vector de 180 caracter√≠sticas.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">üîä</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">#Ô∏è‚É£</div>
                    <div class="flow-text">Digitalizaci√≥n (Muestreo)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üñºÔ∏è</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üìä</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                     <div class="flow-icon">üß¨</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Vector:</strong> Lista ordenada de n√∫meros que representa las caracter√≠sticas extra√≠das del audio. <strong>Frames:</strong> Peque√±os segmentos de audio analizados individualmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 13: DIGITALIZACI√ìN -->
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 1: Digitalizaci√≥n y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalizaci√≥n</h4>
                    <p>Una onda de sonido es una se√±al anal√≥gica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de n√∫meros, donde cada n√∫mero es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es est√°tica. Para analizarla, la dividimos en peque√±os segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tama√±o de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colecci√≥n de muchos arreglos peque√±os (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio wave sampling and framing.png" alt="Diagrama de Digitalizaci√≥n y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en m√∫ltiples ventanas (frames) para su an√°lisis.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Muestreo:</strong> Proceso de convertir una se√±al continua en una secuencia discreta de valores. <strong>Amplitud:</strong> Intensidad o fuerza de la onda sonora en un momento dado.
            </footer>
        </div>

        <!-- DIAPOSITIVA 14: FFT -->
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¬øQu√© es y para qu√© sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qu√© frecuencias la componen. La <strong>Transformada R√°pida de Fourier (FFT)</strong> es la herramienta matem√°tica que lo hace posible.</p>
                    <p>La FFT descompone la se√±al del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energ√≠a vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qu√© tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matem√°tico: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un n√∫mero complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El n√∫mero total de muestras en la ventana.</li>
                            <li>$k$: El √≠ndice de la frecuencia que se est√° calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time domain to frequency domain FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>FFT:</strong> Algoritmo eficiente para calcular la Transformada Discreta de Fourier. <strong>Dominio del tiempo vs. frecuencia:</strong> Dos formas de representar la misma se√±al, enfoc√°ndose en cu√°ndo ocurren los eventos vs. qu√© frecuencias contiene.
            </footer>
        </div>

        <!-- DIAPOSITIVA 15: MFCCs -->
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>M√°s all√° del Espectro</h4>
                    <p>El espectro de la FFT es √∫til, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho m√°s inteligente de resumir la informaci√≥n del espectro, imitando c√≥mo funciona el o√≠do humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logar√≠tmica, similar a nuestra percepci√≥n auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energ√≠as, de nuevo, para imitar c√≥mo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operaci√≥n que comprime toda esa informaci√≥n espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripci√≥n num√©rica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC block diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Escala Mel:</strong> Escala perceptual de frecuencias que imita c√≥mo el o√≠do humano percibe diferencias en el tono. <strong>DCT:</strong> Transformada que convierte se√±ales al dominio de frecuencia usando solo funciones coseno. <strong>Timbre:</strong> Cualidad que diferencia sonidos con el mismo pitch y volumen.
            </footer>
        </div>

        <!-- DIAPOSITIVA 16: OPERACI√ìN PROMEDIO -->
        <div class="slide hidden" data-slide="16">
            <h2 class="technical-title">Paso 4: Operaci√≥n de Promedio para Vector Final</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del An√°lisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de caracter√≠sticas, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 caracter√≠sticas.</p>
                    <p>Para obtener un √∫nico vector $V$ que represente todo el audio, calculamos la media de cada caracter√≠stica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">C√°lculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                            <li>$V_j$: Es el valor final de la caracter√≠stica $j$ en nuestro vector.</li>
                            <li>$T$: Es el n√∫mero total de ventanas (frames) en el audio.</li>
                            <li>$M_{t,j}$: Es el valor de la caracter√≠stica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                    <h4>¬øPor qu√© usar el promedio?</h4>
                   <ul>
                        <li><strong>Reducci√≥n temporal:</strong> Condensa informaci√≥n variable en el tiempo a un valor estable</li>
                        <li><strong>Robustez:</strong> El promedio es menos sensible a valores at√≠picos en ventanas individuales</li>
                        <li><strong>Representatividad:</strong> Captura las caracter√≠sticas dominantes del audio completo</li>
                        <li><strong>Compatibilidad:</strong> Produce un vector de tama√±o fijo para cualquier duraci√≥n de audio</li>
                    </ul>
                   <p>Este proceso condensa la informaci√≥n temporal en una sola "ficha t√©cnica" que describe las propiedades ac√∫sticas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature matrix averaging to vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
            <footer class="footnote">
                <strong>Promedio aritm√©tico:</strong> Suma de todos los valores dividida entre el n√∫mero total de valores. <strong>Vector de tama√±o fijo:</strong> Representaci√≥n num√©rica con dimensiones constantes, independiente de la duraci√≥n del audio original.
            </footer>
        </div>

        <!-- DIAPOSITIVA 17: NORMALIZACI√ìN DE AUDIO -->
        <div class="slide hidden" data-slide="17">
            <h2 class="technical-title">Normalizaci√≥n de Audio: ¬øPor qu√© es Crucial?</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øQu√© es la Normalizaci√≥n de Audio?</h4>
                    <p>La normalizaci√≥n ajusta la amplitud de una se√±al de audio para que su valor m√°ximo sea 1.0 y el m√≠nimo sea -1.0, estandarizando el volumen entre diferentes grabaciones.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">F√≥rmula de Normalizaci√≥n</span>
                        <p>$x_{norm}[n] = \frac{x[n]}{\max(|x[n]|)}$</p>
                        <ul>
                            <li>$x[n]$: Se√±al de audio original</li>
                            <li>$x_{norm}[n]$: Se√±al normalizada</li>
                            <li>$\max(|x[n]|)$: Valor absoluto m√°ximo de la se√±al</li>
                        </ul>
                    </div>

                    <h4>Ventajas Cr√≠ticas de la Normalizaci√≥n:</h4>
                    <div class="advantages-disadvantages">
                        <div class="advantages">
                            <h4>‚úÖ Beneficios T√©cnicos</h4>
                            <ul>
                                <li><strong>Consistencia de Volumen:</strong> Elimina diferencias de grabaci√≥n entre micr√≥fonos y entornos</li>
                                <li><strong>Estabilidad Num√©rica:</strong> Previene overflow y underflow en c√°lculos posteriores</li>
                                <li><strong>Mejor Convergencia:</strong> Los algoritmos de ML convergen m√°s r√°pido con datos normalizados</li>
                                <li><strong>Robustez:</strong> Reduce sensibilidad a variaciones de hardware de grabaci√≥n</li>
                            </ul>
                        </div>
                        <div class="disadvantages">
                            <h4>‚ö†Ô∏è Sin Normalizaci√≥n</h4>
                            <ul>
                                <li><strong>Sesgo por Volumen:</strong> Grabaciones m√°s fuertes dominar√≠an el entrenamiento</li>
                                <li><strong>Caracter√≠sticas Distorsionadas:</strong> MFCCs y otras caracter√≠sticas ser√≠an inconsistentes</li>
                                <li><strong>Gradientes Inestables:</strong> Entrenamiento del modelo ser√≠a err√°tico</li>
                                <li><strong>Clasificaci√≥n Sesgada:</strong> El modelo podr√≠a aprender a clasificar por volumen, no por emoci√≥n</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <footer class="footnote">
                <strong>Overflow/Underflow:</strong> Errores num√©ricos cuando los valores son demasiado grandes o peque√±os para ser representados. <strong>Convergencia:</strong> Proceso por el cual un algoritmo de aprendizaje alcanza una soluci√≥n estable.
            </footer>
        </div>

        <!-- DIAPOSITIVA 18: PREPROCESAMIENTO -->
        <div class="slide hidden" data-slide="18">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
            <footer class="footnote">
                <strong>PCA:</strong> An√°lisis de Componentes Principales, t√©cnica para reducir dimensiones preservando varianza. <strong>LDA:</strong> An√°lisis Discriminante Lineal, reduce dimensiones maximizando separaci√≥n entre clases.
            </footer>
        </div>

        <!-- DIAPOSITIVA 19: STANDARD SCALER -->
        <div class="slide hidden" data-slide="19">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria. <strong>Z-score:</strong> Medida estad√≠stica que indica cu√°ntas desviaciones est√°ndar est√° un valor de la media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 20: PCA VS LDA -->
        <div class="slide hidden" data-slide="20">
            <h2>13. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo. <strong>Varianza:</strong> Medida de dispersi√≥n que indica cu√°nto se alejan los datos de su media.
            </footer>
        </div>

        <!-- DIAPOSITIVA 21: PCA 3D -->
        <div class="slide hidden" data-slide="21">
            <h2>14. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Componentes Principales:</strong> Nuevos ejes calculados que capturan la m√°xima variabilidad de los datos originales. <strong>Proyecci√≥n:</strong> Transformaci√≥n de datos de alta dimensi√≥n a un espacio de menor dimensi√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 22: LDA 3D -->
        <div class="slide hidden" data-slide="22">
            <h2>15. Visualizaci√≥n 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separaci√≥n entre las emociones. El resultado es una separaci√≥n mucho m√°s clara y c√∫mulos m√°s compactos, lo que confirma visualmente que nuestras caracter√≠sticas son muy efectivas para la clasificaci√≥n.
            </p>
            <footer class="footnote">
                <strong>Discriminantes Lineales (LD):</strong> Combinaciones lineales de las caracter√≠sticas originales que mejor separan las clases. <strong>C√∫mulos:</strong> Agrupaciones de puntos similares en el espacio de caracter√≠sticas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 23: COMPARACI√ìN PCA LDA -->
        <div class="slide hidden" data-slide="23">
            <h2>16. Comparaci√≥n Final y Reflexi√≥n</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersi√≥n general de los datos</td>
                        <td>Muestra qu√© tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opini√≥n Reflexiva</h3>
            <p>Al comparar ambas t√©cnicas, <strong>LDA demuestra ser m√°s efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es √∫til para entender la estructura general de la varianza en los datos, LDA, al ser un m√©todo supervisado, logra crear proyecciones donde las emociones forman c√∫mulos m√°s definidos y distinguibles.</p>
            <footer class="footnote">
                <strong>Separabilidad:</strong> Medida de qu√© tan bien se pueden distinguir diferentes clases en un espacio de caracter√≠sticas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 24: INTRODUCCI√ìN CNN 1D -->
        <div class="slide hidden" data-slide="24">
            <h2>17. Introducci√≥n a las Redes Neuronales Convolucionales 1D</h2>
            <div class="highlight-box">
                <h3>¬øQu√© es una CNN 1D?</h3>
                <p>Una Red Neuronal Convolucional 1D es un tipo especializado de red neuronal dise√±ada para procesar secuencias de datos, como series temporales o caracter√≠sticas extra√≠das de audio.</p>
            </div>
            <h3>¬øPor qu√© CNN 1D para Audio?</h3>
            <ul>
                <li><strong>Detecci√≥n de Patrones Locales:</strong> Identifica patrones en secuencias cortas de caracter√≠sticas</li>
                <li><strong>Invarianza Traslacional:</strong> Reconoce patrones independientemente de su posici√≥n en la secuencia</li>
                <li><strong>Jerarqu√≠a de Caracter√≠sticas:</strong> Capas superiores detectan patrones m√°s complejos</li>
                <li><strong>Eficiencia Computacional:</strong> Menos par√°metros que redes densas equivalentes</li>
            </ul>
            <h3>Nuestra Arquitectura</h3>
            <p>Dise√±amos una CNN 1D que toma como entrada un vector de 180 caracter√≠sticas y produce probabilidades para 7 emociones diferentes.</p>
            <footer class="footnote">
                <strong>CNN:</strong> Convolutional Neural Network, tipo de red neuronal que usa operaciones de convoluci√≥n. <strong>1D:</strong> Unidimensional, opera sobre secuencias lineales de datos. <strong>Invarianza traslacional:</strong> Propiedad que permite reconocer patrones sin importar su posici√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 25: CAPA DE ENTRADA -->
        <div class="slide hidden" data-slide="25">
            <h2>18. Capa de Entrada (Input Layer)</h2>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Entrada:</strong> <span class="output-shape">(None, 180, 1)</span></p>
                        <p><strong>Neuronas:</strong> 180</p>
                        <p><strong>Funci√≥n:</strong> Recibir el vector de caracter√≠sticas</p>
                        <p><strong>Par√°metros:</strong> 0 (no aprende, solo recibe datos)</p>
                    </div>
                </div>
            </div>
            <h3>Detalles T√©cnicos</h3>
            <ul>
                <li><strong>180 caracter√≠sticas:</strong> MFCCs (40) + Chroma (12) + Mel-spectrograms (128)</li>
                <li><strong>Dimensi√≥n 1:</strong> Cada caracter√≠stica es un valor escalar</li>
                <li><strong>None (batch):</strong> Permite procesar m√∫ltiples audios simult√°neamente</li>
                <li><strong>Formato secuencial:</strong> Las caracter√≠sticas mantienen su orden temporal promediado</li>
            </ul>
            <h3>Preprocesamiento Previo</h3>
            <p>Antes de entrar a la red, los datos fueron:</p>
            <ul>
                <li>‚úÖ Normalizados (StandardScaler)</li>
                <li>‚úÖ Reformateados a shape (180, 1)</li>
                <li>‚úÖ Divididos en train/validation/test</li>
            </ul>
            <footer class="footnote">
                <strong>Batch:</strong> Conjunto de muestras procesadas simult√°neamente para eficiencia computacional. <strong>Shape:</strong> Dimensiones de los datos (filas, columnas, canales). <strong>Escalar:</strong> Un solo n√∫mero, no un vector o matriz.
            </footer>
        </div>

        <!-- DIAPOSITIVA 26: PRIMERA CONVOLUCI√ìN -->
        <div class="slide hidden" data-slide="26">
            <h2>19. Primera Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_1 (128 filtros, kernel=5)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 1)</p>
                        <p><strong>Salida:</strong> (None, 180, 128)</p>
                        <p><strong>Neuronas:</strong> 23,040 (180 √ó 128)</p>
                        <p><strong>Par√°metros:</strong> 768</p>
                        <p><strong>Activaci√≥n:</strong> ReLU</p>
                    </div>
                </div>
            </div>
            <h3>¬øQu√© hace esta capa?</h3>
            <ul>
                <li><strong>128 filtros diferentes:</strong> Cada uno aprende a detectar un patr√≥n espec√≠fico</li>
                <li><strong>Kernel size 5:</strong> Cada filtro analiza 5 caracter√≠sticas consecutivas</li>
                <li><strong>Padding 'same':</strong> Mantiene la longitud original (180)</li>
                <li><strong>ReLU:</strong> Introduce no-linealidad, activa solo valores positivos</li>
            </ul>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Convoluci√≥n</span>
                <p>Para cada posici√≥n $i$ y filtro $f$:</p>
                <p>$y_i^f = ReLU\left(\sum_{j=0}^{4} w_j^f \cdot x_{i+j} + b^f\right)$</p>
                <ul>
                    <li>$w_j^f$: Peso $j$ del filtro $f$</li>
                    <li>$x_{i+j}$: Caracter√≠stica en posici√≥n $i+j$</li>
                    <li>$b^f$: Bias del filtro $f$</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Filtro/Kernel:</strong> Conjunto de pesos que se desliza sobre los datos para detectar patrones. <strong>Padding:</strong> T√©cnica para mantener las dimensiones despu√©s de la convoluci√≥n. <strong>Bias:</strong> Par√°metro adicional que permite ajustar el umbral de activaci√≥n.
            </footer>
        </div>

        <!-- DIAPOSITIVA 27: FUNCI√ìN RELU -->
        <div class="slide hidden" data-slide="27">
            <h2>20. Funci√≥n de Activaci√≥n ReLU</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Rectified Linear Unit (ReLU)</h4>
                    <p>ReLU es la funci√≥n de activaci√≥n m√°s popular en redes neuronales profundas debido a su simplicidad y efectividad.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Definici√≥n Matem√°tica</span>
                        <p>$f(x) = \max(0, x) = \begin{cases} 
                        x & \text{si } x > 0 \\
                        0 & \text{si } x \leq 0
                        \end{cases}$</p>
                    </div>

                    <h4>¬øPor qu√© ReLU es tan efectiva?</h4>
                    <div class="stats-grid">
                        <div class="stat-item">
                            <h3>Simplicidad</h3>
                            <p>Computacionalmente muy eficiente, solo requiere una comparaci√≥n</p>
                        </div>
                        <div class="stat-item">
                            <h3>No Saturaci√≥n</h3>
                            <p>No se satura para valores positivos, evita el problema del gradiente desvaneciente</p>
                        </div>
                        <div class="stat-item">
                            <h3>Esparsidad</h3>
                            <p>Produce activaciones dispersas (muchos ceros), mejorando la eficiencia</p>
                        </div>
                    </div>

                    <h4>Efectos en Nuestro Modelo</h4>
                    <ul>
                        <li><strong>Filtrado de Ruido:</strong> Elimina activaciones negativas (ruido)</li>
                        <li><strong>Detecci√≥n de Caracter√≠sticas:</strong> Solo permite pasar patrones "positivos"</li>
                        <li><strong>No-linealidad:</strong> Permite al modelo aprender relaciones complejas</li>
                        <li><strong>Convergencia R√°pida:</strong> Facilita el entrenamiento eficiente</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Activaci√≥n:</strong> Funci√≥n que determina si una neurona debe ser activada bas√°ndose en su entrada. <strong>Gradiente desvaneciente:</strong> Problema donde los gradientes se vuelven muy peque√±os, ralentizando el aprendizaje. <strong>Saturaci√≥n:</strong> Cuando una funci√≥n se vuelve plana y deja de cambiar significativamente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 28: BATCH NORMALIZATION -->
        <div class="slide hidden" data-slide="28">
            <h2>21. Normalizaci√≥n por Lotes (Batch Normalization)</h2>
            <div class="neural-architecture">
                <div class="layer-box batch-norm" style="background: linear-gradient(135deg, var(--color-accent), var(--color-secondary));">
                    <h4>Batch Normalization</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 128)</p>
                        <p><strong>Salida:</strong> (None, 180, 128)</p>
                        <p><strong>Neuronas:</strong> 23,040 (180 √ó 128)</p>
                        <p><strong>Par√°metros:</strong> 512 (Œ≥ y Œ≤ para cada canal)</p>
                    </div>
                </div>
            </div>
            
            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Batch Normalization</span>
                <p>Para cada caracter√≠stica en el batch:</p>
                <p>$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$</p>
                <p>$y = \gamma \hat{x} + \beta$</p>
                <ul>
                    <li>$\mu_B$: Media del batch</li>
                    <li>$\sigma_B^2$: Varianza del batch</li>
                    <li>$\gamma, \beta$: Par√°metros aprendibles</li>
                    <li>$\epsilon$: Constante peque√±a para estabilidad num√©rica</li>
                </ul>
            </div>

            <h3>Beneficios de Batch Normalization</h3>
            <ul>
                <li><strong>Estabiliza el entrenamiento:</strong> Reduce la sensibilidad a la inicializaci√≥n de pesos</li>
                <li><strong>Acelera convergencia:</strong> Permite usar learning rates m√°s altos</li>
                <li><strong>Efecto regularizador:</strong> Reduce overfitting de forma natural</li>
                <li><strong>Gradientes estables:</strong> Mantiene distribuciones de activaci√≥n consistentes</li>
            </ul>
            <footer class="footnote">
                <strong>Batch Normalization:</strong> T√©cnica que normaliza las entradas de cada capa usando estad√≠sticas del mini-batch. <strong>Learning rate:</strong> Velocidad de aprendizaje del modelo. <strong>Regularizaci√≥n:</strong> T√©cnicas para prevenir overfitting.
            </footer>
        </div>

        <!-- DIAPOSITIVA 29: MAX POOLING -->
        <div class="slide hidden" data-slide="29">
            <h2>22. Capa de Max Pooling 1D</h2>
            <div class="neural-architecture">
                <div class="layer-box pool">
                    <h4>MaxPooling1D (pool_size=2)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 180, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 128)</p>
                        <p><strong>Neuronas:</strong> 11,520 (90 √ó 128)</p>
                        <p><strong>Par√°metros:</strong> 0 (sin aprendizaje)</p>
                        <p><strong>Reducci√≥n:</strong> 50% en dimensi√≥n temporal</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operaci√≥n MaxPooling</span>
                <p>Para cada ventana de tama√±o 2:</p>
                <p>$y[i] = \max(x[2i], x[2i+1])$</p>
                <p>Se toma el valor m√°ximo de cada par de elementos consecutivos</p>
            </div>

            <h3>¬øPor qu√© MaxPooling?</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Ventajas</h4>
                    <ul>
                        <li><strong>Reduce dimensionalidad:</strong> 180 ‚Üí 90 elementos</li>
                        <li><strong>Invarianza local:</strong> Robustez a peque√±os desplazamientos</li>
                        <li><strong>Extrae caracter√≠sticas dominantes:</strong> Preserva activaciones m√°s fuertes</li>
                        <li><strong>Eficiencia computacional:</strong> Menos par√°metros en capas siguientes</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>‚ö†Ô∏è Consideraciones</h4>
                    <ul>
                        <li><strong>P√©rdida de informaci√≥n:</strong> Descarta 50% de los datos</li>
                        <li><strong>P√©rdida de ubicaci√≥n exacta:</strong> Solo preserva la caracter√≠stica m√°s fuerte</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Max Pooling:</strong> Operaci√≥n que toma el valor m√°ximo de una ventana de datos, reduciendo dimensionalidad. <strong>Invarianza:</strong> Propiedad de mantener el resultado ante peque√±os cambios en la entrada.
            </footer>
        </div>

        <!-- DIAPOSITIVA 30: DROPOUT -->
        <div class="slide hidden" data-slide="30">
            <h2>23. Capa de Dropout</h2>
            <div class="neural-architecture">
                <div class="layer-box dropout">
                    <h4>Dropout (rate=0.3)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 90, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 128)</p>
                        <p><strong>Neuronas:</strong> 11,520 (90 √ó 128)</p>
                        <p><strong>Par√°metros:</strong> 0</p>
                        <p><strong>Neuronas desactivadas:</strong> 30% aleatoriamente</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Dropout</span>
                <p>Durante el entrenamiento:</p>
                <p>$y_i = \begin{cases} 
                0 & \text{con probabilidad } p = 0.3 \\
                \frac{x_i}{1-p} & \text{con probabilidad } 1-p = 0.7
                \end{cases}$</p>
                <p>Durante la inferencia: $y_i = x_i$ (sin dropout)</p>
            </div>

            <h3>¬øC√≥mo Previene el Overfitting?</h3>
            <ul>
                <li><strong>Ensembles impl√≠citos:</strong> Cada forward pass usa una red ligeramente diferente</li>
                <li><strong>Reduce co-adaptaci√≥n:</strong> Las neuronas no pueden depender unas de otras</li>
                <li><strong>Generalizaci√≥n:</strong> Fuerza al modelo a ser robusto ante p√©rdida de informaci√≥n</li>
                <li><strong>Regularizaci√≥n estoc√°stica:</strong> A√±ade ruido controlado durante entrenamiento</li>
            </ul>

            <h3>Impacto en Nuestro Modelo</h3>
            <p>Con 30% de dropout:</p>
            <ul>
                <li>De 11,520 neuronas ‚Üí ~8,064 activas por iteraci√≥n</li>
                <li>Configuraciones posibles: 2^11,520 (n√∫mero astron√≥mico)</li>
                <li>Cada batch entrena un "submodelo" diferente</li>
            </ul>
            <footer class="footnote">
                <strong>Overfitting:</strong> Cuando el modelo memoriza datos de entrenamiento pero falla en datos nuevos. <strong>Co-adaptaci√≥n:</strong> Dependencia excesiva entre neuronas que reduce robustez. <strong>Estoc√°stico:</strong> Que involucra aleatoriedad o probabilidad.
            </footer>
        </div>

        <!-- DIAPOSITIVA 31: SEGUNDA CONVOLUCI√ìN -->
        <div class="slide hidden" data-slide="31">
            <h2>24. Segunda Capa Convolucional</h2>
            <div class="neural-architecture">
                <div class="layer-box conv">
                    <h4>Conv1D_2 (64 filtros, kernel=5)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 90, 128)</p>
                        <p><strong>Salida:</strong> (None, 90, 64)</p>
                        <p><strong>Neuronas:</strong> 5,760 (90 √ó 64)</p>
                        <p><strong>Par√°metros:</strong> 41,024</p>
                        <p><strong>Funci√≥n:</strong> Detectar patrones de nivel superior</p>
                    </div>
                </div>
            </div>

            <h3>Diferencias con la Primera Capa</h3>
            <div class="comparison-table">
                <table style="width: 100%;">
                    <thead>
                        <tr>
                            <th>Aspecto</th>
                            <th>Conv1D_1</th>
                            <th>Conv1D_2</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Filtros</strong></td>
                            <td>128</td>
                            <td>64</td>
                        </tr>
                        <tr>
                            <td><strong>Entrada</strong></td>
                            <td>1 canal (caracter√≠sticas)</td>
                            <td>128 canales (mapas de caracter√≠sticas)</td>
                        </tr>
                        <tr>
                            <td><strong>Detecta</strong></td>
                            <td>Patrones b√°sicos</td>
                            <td>Combinaciones de patrones</td>
                        </tr>
                        <tr>
                            <td><strong>Par√°metros</strong></td>
                            <td>768</td>
                            <td>41,024</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="formula-block">
                <span class="formula-title">C√°lculo de Par√°metros</span>
                <p>Para Conv1D_2:</p>
                <p>$(kernel\_size \times input\_channels \times output\_channels) + output\_channels$</p>
                <p>$(5 \times 128 \times 64) + 64 = 40,960 + 64 = 41,024$</p>
            </div>

            <h3>¬øQu√© Aprende Esta Capa?</h3>
            <ul>
                <li><strong>Patrones complejos:</strong> Combinaciones de las caracter√≠sticas detectadas en Conv1D_1</li>
                <li><strong>Representaciones abstractas:</strong> Caracter√≠sticas m√°s espec√≠ficas de emociones</li>
                <li><strong>Jerarqu√≠a:</strong> Desde caracter√≠sticas b√°sicas hacia conceptos emocionales</li>
            </ul>
            <footer class="footnote">
                <strong>Mapas de caracter√≠sticas:</strong> Salidas de filtros convolucionales que representan la presencia de patrones espec√≠ficos. <strong>Jerarqu√≠a de caracter√≠sticas:</strong> Proceso donde capas m√°s profundas aprenden conceptos m√°s abstractos y complejos.
            </footer>
        </div>

        <!-- DIAPOSITIVA 32: GLOBAL AVERAGE POOLING -->
        <div class="slide hidden" data-slide="32">
            <h2>25. Global Average Pooling</h2>
            <div class="neural-architecture">
                <div class="layer-box pool" style="background: linear-gradient(135deg, var(--color-primary), var(--color-accent));">
                    <h4>Global Average Pooling 1D</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 45, 64)</p>
                        <p><strong>Salida:</strong> (None, 64)</p>
                        <p><strong>Neuronas:</strong> 64</p>
                        <p><strong>Par√°metros:</strong> 0</p>
                        <p><strong>Operaci√≥n:</strong> Promedio de toda la secuencia temporal</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operaci√≥n Global Average Pooling</span>
                <p>Para cada canal $c$:</p>
                <p>$y_c = \frac{1}{L} \sum_{i=1}^{L} x_{i,c}$</p>
                <ul>
                    <li>$L$: Longitud de la secuencia (45)</li>
                    <li>$x_{i,c}$: Valor en posici√≥n $i$ del canal $c$</li>
                    <li>$y_c$: Valor promedio del canal $c$</li>
                </ul>
            </div>

            <h3>Ventajas sobre Flatten Tradicional</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Global Average Pooling</h4>
                    <ul>
                        <li><strong>Menos par√°metros:</strong> 64 caracter√≠sticas vs 2,880 con Flatten</li>
                        <li><strong>Previene overfitting:</strong> Menos complejidad del modelo</li>
                        <li><strong>Invarianza a longitud:</strong> Funciona con secuencias de diferente tama√±o</li>
                        <li><strong>Interpretabilidad:</strong> Cada canal representa una caracter√≠stica global</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>‚ö†Ô∏è Flatten Traditional</h4>
                    <ul>
                        <li><strong>Muchos par√°metros:</strong> 45√ó64 = 2,880 caracter√≠sticas</li>
                        <li><strong>Propenso a overfitting:</strong> M√°s complejidad</li>
                        <li><strong>Dependiente de posici√≥n:</strong> Sensible al orden exacto</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
                <strong>Global Average Pooling:</strong> T√©cnica que calcula el promedio de cada mapa de caracter√≠sticas en toda la secuencia temporal. <strong>Flatten:</strong> Operaci√≥n que convierte matrices multidimensionales en vectores 1D. <strong>Interpretabilidad:</strong> Facilidad para entender qu√© est√° aprendiendo el modelo.
            </footer>
        </div>

        <!-- DIAPOSITIVA 33: CAPA DENSA INTERMEDIA -->
        <div class="slide hidden" data-slide="33">
            <h2>26. Capa Densa Intermedia</h2>
            <div class="neural-architecture">
                <div class="layer-box dense">
                    <h4>Dense Intermedia (64 neuronas)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 64)</p>
                        <p><strong>Salida:</strong> (None, 64)</p>
                        <p><strong>Neuronas:</strong> 64</p>
                        <p><strong>Par√°metros:</strong> 4,160</p>
                        <p><strong>Activaci√≥n:</strong> ReLU</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Operaci√≥n de Capa Densa</span>
                <p>$y = ReLU(W \cdot x + b)$</p>
                <ul>
                    <li>$W$: Matriz de pesos (64√ó64)</li>
                    <li>$x$: Vector de entrada (64 elementos)</li>
                    <li>$b$: Vector de bias (64 elementos)</li>
                    <li>Par√°metros totales: $(64 \times 64) + 64 = 4,160$</li>
                </ul>
            </div>

            <h3>¬øPor qu√© una Capa Intermedia?</h3>
            <ul>
                <li><strong>Procesamiento adicional:</strong> Permite combinaciones no-lineales de caracter√≠sticas</li>
                <li><strong>Capacidad de representaci√≥n:</strong> A√±ade flexibilidad al modelo</li>
                <li><strong>Transici√≥n suave:</strong> Gradual desde caracter√≠sticas hacia clasificaci√≥n</li>
                <li><strong>Regularizaci√≥n:</strong> Con dropout, previene overfitting</li>
            </ul>

            <h3>Patr√≥n de Arquitectura</h3>
            <p>Seguimos el patr√≥n com√∫n:</p>
            <div class="methodology-step">Caracter√≠sticas extra√≠das ‚Üí Procesamiento intermedio ‚Üí Clasificaci√≥n final</div>
            <footer class="footnote">
                <strong>Capa densa/fully connected:</strong> Capa donde cada neurona est√° conectada a todas las neuronas de la capa anterior. <strong>Matriz de pesos:</strong> Tabla de n√∫meros que determinan la fuerza de las conexiones entre neuronas.
            </footer>
        </div>

        <!-- DIAPOSITIVA 34: CAPA DE SALIDA -->
        <div class="slide hidden" data-slide="34">
            <h2>27. Capa de Salida con Softmax</h2>
            <div class="neural-architecture">
                <div class="layer-box output">
                    <h4>Capa de Salida (7 emociones)</h4>
                    <div class="layer-info">
                        <p><strong>Entrada:</strong> (None, 64)</p>
                        <p><strong>Salida:</strong> (None, 7)</p>
                        <p><strong>Neuronas:</strong> 7</p>
                        <p><strong>Par√°metros:</strong> 455</p>
                        <p><strong>Activaci√≥n:</strong> Softmax</p>
                    </div>
                </div>
            </div>

            <div class="formula-block">
                <span class="formula-title">Funci√≥n Softmax</span>
                <p>Para cada emoci√≥n $i$:</p>
                <p>$P(emoci√≥n_i) = \frac{e^{z_i}}{\sum_{j=1}^{7} e^{z_j}}$</p>
                <ul>
                    <li>$z_i$: Puntuaci√≥n (logit) para la emoci√≥n $i$</li>
                    <li>$P(emoci√≥n_i)$: Probabilidad de la emoci√≥n $i$</li>
                    <li>$\sum_{i=1}^{7} P(emoci√≥n_i) = 1$: Las probabilidades suman 1</li>
                </ul>
            </div>

            <h3>Las 7 Emociones de Salida</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Angry</h3><p>Neurona 0</p></div>
                <div class="stat-item"><h3>Disgust</h3><p>Neurona 1</p></div>
                <div class="stat-item"><h3>Fearful</h3><p>Neurona 2</p></div>
                <div class="stat-item"><h3>Happy</h3><p>Neurona 3</p></div>
                <div class="stat-item"><h3>Neutral</h3><p>Neurona 4</p></div>
                <div class="stat-item"><h3>Sad</h3><p>Neurona 5</p></div>
                <div class="stat-item"><h3>Surprised</h3><p>Neurona 6</p></div>
            </div>

            <h3>Ejemplo de Predicci√≥n</h3>
            <p><strong>Logits:</strong> [2.1, -0.5, 3.2, 0.8, -1.2, 1.5, 0.3]</p>
            <p><strong>Softmax:</strong> [0.196, 0.015, 0.589, 0.053, 0.007, 0.107, 0.032]</p>
            <p><strong>Predicci√≥n:</strong> Fearful (58.9% de confianza)</p>
            <footer class="footnote">
                <strong>Softmax:</strong> Funci√≥n que convierte puntuaciones en probabilidades v√°lidas. <strong>Logits:</strong> Puntuaciones brutas antes de aplicar softmax. <strong>Distribuci√≥n de probabilidad:</strong> Conjunto de probabilidades que suman 1.
            </footer>
        </div>

        <!-- DIAPOSITIVA 35: RESUMEN ARQUITECTURA -->
        <div class="slide hidden" data-slide="35">
            <h2>28. Resumen Completo de la Arquitectura</h2>
            <div class="cnn-details">
                <p>
                    Nuestra CNN 1D procesa secuencialmente 180 caracter√≠sticas de audio a trav√©s de m√∫ltiples capas especializadas, transformando gradualmente datos num√©ricos en predicciones emocionales.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa</th>
                            <th>Tipo</th>
                            <th>Entrada ‚Üí Salida</th>
                            <th>Neuronas</th>
                            <th>Par√°metros</th>
                            <th>Funci√≥n Principal</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Input</strong></td>
                            <td>Entrada</td>
                            <td>(180, 1)</td>
                            <td>180</td>
                            <td>0</td>
                            <td>Recibir caracter√≠sticas</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_1</strong></td>
                            <td>Convoluci√≥n</td>
                            <td>(180, 1) ‚Üí (180, 128)</td>
                            <td>23,040</td>
                            <td>768</td>
                            <td>Detectar patrones b√°sicos</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_1</strong></td>
                            <td>Normalizaci√≥n</td>
                            <td>(180, 128) ‚Üí (180, 128)</td>
                            <td>23,040</td>
                            <td>512</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_1</strong></td>
                            <td>Pooling</td>
                            <td>(180, 128) ‚Üí (90, 128)</td>
                            <td>11,520</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_1</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>(90, 128) ‚Üí (90, 128)</td>
                            <td>11,520</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Conv1D_2</strong></td>
                            <td>Convoluci√≥n</td>
                            <td>(90, 128) ‚Üí (90, 64)</td>
                            <td>5,760</td>
                            <td>41,024</td>
                            <td>Patrones complejos</td>
                        </tr>
                        <tr>
                            <td><strong>BatchNorm_2</strong></td>
                            <td>Normalizaci√≥n</td>
                            <td>(90, 64) ‚Üí (90, 64)</td>
                            <td>5,760</td>
                            <td>256</td>
                            <td>Estabilizar entrenamiento</td>
                        </tr>
                        <tr>
                            <td><strong>MaxPool_2</strong></td>
                            <td>Pooling</td>
                            <td>(90, 64) ‚Üí (45, 64)</td>
                            <td>2,880</td>
                            <td>0</td>
                            <td>Reducir dimensionalidad</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_2</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>(45, 64) ‚Üí (45, 64)</td>
                            <td>2,880</td>
                            <td>0</td>
                            <td>Prevenir overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>GlobalAvgPool</strong></td>
                            <td>Pooling Global</td>
                            <td>(45, 64) ‚Üí (64)</td>
                            <td>64</td>
                            <td>0</td>
                            <td>Condensar caracter√≠sticas</td>
                        </tr>
                        <tr>
                            <td><strong>Dense_Inter</strong></td>
                            <td>Densa</td>
                            <td>(64) ‚Üí (64)</td>
                            <td>64</td>
                            <td>4,160</td>
                            <td>Procesamiento intermedio</td>
                        </tr>
                        <tr>
                            <td><strong>Dropout_Final</strong></td>
                            <td>Regularizaci√≥n</td>
                            <td>(64) ‚Üí (64)</td>
                            <td>64</td>
                            <td>0</td>
                            <td>Regularizaci√≥n final</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Clasificaci√≥n</td>
                            <td>(64) ‚Üí (7)</td>
                            <td>7</td>
                            <td>455</td>
                            <td>Predicci√≥n emocional</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="4"><strong>TOTAL</strong></td>
                            <td><strong>47,175</strong></td>
                            <td><strong>Clasificaci√≥n de emociones</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <footer class="footnote">
                <strong>Par√°metros totales:</strong> 47,175 pesos y bias que el modelo debe aprender durante el entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 36: OPTIMIZADOR ADAM -->
        <div class="slide hidden" data-slide="36">
            <h2>29. Optimizador ADAM</h2>
            <div class="highlight-box">
                <h3>Adaptive Moment Estimation (ADAM)</h3>
                <p>ADAM combina las ventajas de AdaGrad y RMSprop, adaptando el learning rate individualmente para cada par√°metro.</p>
            </div>

            <div class="formula-block">
                <span class="formula-title">Algoritmo ADAM Completo</span>
                <p><strong>Inicializaci√≥n:</strong> $m_0 = 0$, $v_0 = 0$, $t = 0$</p>
                <p><strong>En cada iteraci√≥n:</strong></p>
                <p>$t = t + 1$</p>
                <p>$g_t = \nabla_{\theta} J(\theta_{t-1})$ (gradiente)</p>
                <p>$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ (momento primer orden)</p>
                <p>$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ (momento segundo orden)</p>
                <p>$\hat{m}_t = \frac{m_t}{1-\beta_1^t}$ (correcci√≥n bias momento)</p>
                <p>$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$ (correcci√≥n bias varianza)</p>
                <p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$ (actualizaci√≥n)</p>
            </div>

            <h3>Configuraci√≥n en Nuestro Modelo</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Œ± = 0.001</h3><p>Learning rate</p></div>
                <div class="stat-item"><h3>Œ≤‚ÇÅ = 0.9</h3><p>Momento primer orden</p></div>
                <div class="stat-item"><h3>Œ≤‚ÇÇ = 0.999</h3><p>Momento segundo orden</p></div>
                <div class="stat-item"><h3>Œµ = 1e-7</h3><p>Estabilidad num√©rica</p></div>
            </div>
            <footer class="footnote">
                <strong>Optimizador:</strong> Algoritmo que ajusta los pesos del modelo para minimizar la funci√≥n de p√©rdida. <strong>Learning rate:</strong> Velocidad de aprendizaje del modelo. <strong>Momento:</strong> T√©cnica que usa informaci√≥n de gradientes pasados para acelerar convergencia.
            </footer>
        </div>

        <!-- DIAPOSITIVA 37: FUNCI√ìN DE P√âRDIDA -->
        <div class="slide hidden" data-slide="37">
            <h2>30. Funci√≥n de P√©rdida: Categorical Crossentropy</h2>
            <div class="formula-block">
                <span class="formula-title">Categorical Crossentropy</span>
                <p>$\mathcal{L} = -\sum_{i=1}^{7} y_i \log(\hat{y}_i)$</p>
                <ul>
                    <li>$y_i$: Etiqueta verdadera (one-hot encoded)</li>
                    <li>$\hat{y}_i$: Probabilidad predicha para la clase $i$</li>
                    <li>Solo la clase verdadera contribuye a la p√©rdida</li>
                </ul>
            </div>

            <h3>¬øPor qu√© Categorical Crossentropy?</h3>
            <ul>
                <li><strong>Clasificaci√≥n multiclase:</strong> Ideal para problemas con m√∫ltiples categor√≠as exclusivas</li>
                <li><strong>Penalizaci√≥n logar√≠tmica:</strong> Penaliza fuertemente predicciones incorrectas muy confiadas</li>
                <li><strong>Compatible con Softmax:</strong> Funciona perfectamente con la activaci√≥n de salida</li>
                <li><strong>Derivadas suaves:</strong> Facilita el backpropagation</li>
            </ul>

            <h3>Ejemplo de C√°lculo</h3>
            <div class="methodology-step">
                <strong>Situaci√≥n:</strong> Audio real = "Happy", predicci√≥n = [0.1, 0.05, 0.15, 0.6, 0.05, 0.03, 0.02]
            </div>
            <div class="methodology-step">
                <strong>One-hot real:</strong> [0, 0, 0, 1, 0, 0, 0]
            </div>
            <div class="methodology-step">
                <strong>P√©rdida:</strong> $-\log(0.6) = 0.511$ (relativamente baja, buena predicci√≥n)
            </div>
            <footer class="footnote">
                <strong>One-hot encoding:</strong> Representaci√≥n donde solo un elemento es 1 y el resto son 0. <strong>Backpropagation:</strong> Algoritmo para calcular gradientes y entrenar redes neuronales. <strong>Logaritmo natural:</strong> Funci√≥n matem√°tica que crece m√°s lentamente, penalizando errores grandes.
            </footer>
        </div>

        <!-- DIAPOSITIVA 38: PROCESO DE ENTRENAMIENTO -->
        <div class="slide hidden" data-slide="38">
            <h2>31. Proceso de Entrenamiento</h2>
            <div class="highlight-box">
                <h3>Configuraci√≥n de Entrenamiento</h3>
                <p>El modelo se entren√≥ durante 50 √©pocas con monitoreo continuo del rendimiento en validaci√≥n.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>50</h3><p>√âpocas entrenadas</p></div>
                <div class="stat-item"><h3>64</h3><p>Tama√±o de batch</p></div>
                <div class="stat-item"><h3>4,668</h3><p>Muestras entrenamiento</p></div>
                <div class="stat-item"><h3>1,168</h3><p>Muestras validaci√≥n</p></div>
            </div>

            <h3>Divisi√≥n de Datos</h3>
            <ul>
                <li><strong>Entrenamiento (64%):</strong> 4,668 muestras para aprender patrones</li>
                <li><strong>Validaci√≥n (16%):</strong> 1,168 muestras para ajustar hiperpar√°metros</li>
                <li><strong>Prueba (20%):</strong> 1,460 muestras para evaluaci√≥n final</li>
            </ul>

            <h3>Callbacks Utilizados</h3>
            <div class="methodology-step"><strong>ModelCheckpoint:</strong> Guarda el mejor modelo basado en accuracy de validaci√≥n</div>
            <div class="methodology-step"><strong>ReduceLROnPlateau:</strong> Reduce learning rate cuando no hay mejora</div>
            <div class="methodology-step"><strong>EarlyStopping:</strong> Detiene entrenamiento si hay overfitting</div>
            <footer class="footnote">
                <strong>√âpoca:</strong> Una pasada completa por todo el dataset de entrenamiento. <strong>Batch:</strong> Subconjunto de datos procesado simult√°neamente. <strong>Callbacks:</strong> Funciones que se ejecutan durante el entrenamiento para monitoreo y control.
            </footer>
        </div>

        <!-- DIAPOSITIVA 39: RESULTADOS FINALES -->
        <div class="slide hidden" data-slide="39">
            <h2>32. Resultados del Entrenamiento</h2>
            <div class="highlight-box">
                <h3>üéâ Rendimiento Excepcional Alcanzado</h3>
                <p>El modelo logr√≥ una precisi√≥n del <strong>89.73%</strong> en el conjunto de prueba, superando expectativas iniciales.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>89.73%</h3><p>Precisi√≥n en prueba</p></div>
                <div class="stat-item"><h3>89.04%</h3><p>Precisi√≥n final entrenamiento</p></div>
                <div class="stat-item"><h3>89.38%</h3><p>Precisi√≥n final validaci√≥n</p></div>
                <div class="stat-item"><h3>50</h3><p>√âpocas completadas</p></div>
            </div>

            <h3>Interpretaci√≥n de Resultados</h3>
            <ul>
                <li><strong>Excelente generalizaci√≥n:</strong> Precisi√≥n similar en train, validation y test</li>
                <li><strong>Sin overfitting:</strong> Las curvas de entrenamiento y validaci√≥n son consistentes</li>
                <li><strong>Convergencia estable:</strong> El modelo aprendi√≥ de forma progresiva y estable</li>
                <li><strong>Robustez:</strong> Funciona bien con datos no vistos durante entrenamiento</li>
            </ul>

            <h3>Comparaci√≥n con Benchmarks</h3>
            <p>Nuestro resultado de 89.73% es competitivo con modelos state-of-the-art en reconocimiento de emociones en voz, considerando:</p>
            <ul>
                <li>M√∫ltiples datasets combinados</li>
                <li>7 clases emocionales</li>
                <li>Arquitectura relativamente simple (47K par√°metros)</li>
            </ul>
            <footer class="footnote">
                <strong>Generalizaci√≥n:</strong> Capacidad del modelo de funcionar bien con datos nuevos no vistos durante entrenamiento. <strong>State-of-the-art:</strong> El mejor rendimiento actual en el campo de investigaci√≥n. <strong>Benchmarks:</strong> Est√°ndares de comparaci√≥n en la literatura cient√≠fica.
            </footer>
        </div>

        <!-- DIAPOSITIVA 40: MATRIZ DE CONFUSI√ìN -->
        <div class="slide hidden" data-slide="40">
            <h2>33. An√°lisis Detallado por Emoci√≥n</h2>
            <div class="single-chart-container">
                <h3>Matriz de Confusi√≥n - Rendimiento por Emoci√≥n</h3>
                <img src="/content/confusion_matrix.png" alt="Matriz de Confusi√≥n" style="max-width: 90%; border-radius: 15px;">
            </div>
            
            <h3>Reporte de Clasificaci√≥n</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Emoci√≥n</th>
                        <th>Precisi√≥n</th>
                        <th>Recall</th>
                        <th>F1-Score</th>
                        <th>Soporte</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Angry</strong></td>
                        <td>0.89</td>
                        <td>0.93</td>
                        <td>0.91</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Disgust</strong></td>
                        <td>0.89</td>
                        <td>0.92</td>
                        <td>0.90</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Fearful</strong></td>
                        <td>0.94</td>
                        <td>0.89</td>
                        <td>0.91</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Happy</strong></td>
                        <td>0.92</td>
                        <td>0.85</td>
                        <td>0.88</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Neutral</strong></td>
                        <td>0.93</td>
                        <td>0.96</td>
                        <td>0.94</td>
                        <td>198</td>
                    </tr>
                    <tr>
                        <td><strong>Sad</strong></td>
                        <td>0.92</td>
                        <td>0.86</td>
                        <td>0.89</td>
                        <td>237</td>
                    </tr>
                    <tr>
                        <td><strong>Surprised</strong></td>
                        <td>0.67</td>
                        <td>0.83</td>
                        <td>0.74</td>
                        <td>77</td>
                    </tr>
                </tbody>
            </table>

            <h3>An√°lisis de Rendimiento</h3>
            <ul>
                <li><strong>Mejor clasificada:</strong> Neutral (F1: 0.94) - Caracter√≠sticas muy distintivas</li>
                <li><strong>M√°s desafiante:</strong> Surprised (F1: 0.74) - Menos muestras de entrenamiento</li>
                <li><strong>Rendimiento balanceado:</strong> Angry, Disgust, Fearful (~0.90 F1-Score)</li>
                <li><strong>Confusiones comunes:</strong> Happy ‚Üî Surprised, Sad ‚Üî Fearful</li>
            </ul>
            <footer class="footnote">
                <strong>Precisi√≥n:</strong> Porcentaje de predicciones correctas para una clase. <strong>Recall:</strong> Porcentaje de casos reales detectados correctamente. <strong>F1-Score:</strong> Media arm√≥nica entre precisi√≥n y recall. <strong>Soporte:</strong> N√∫mero de muestras reales de cada clase.
            </footer>
        </div>

        <!-- DIAPOSITIVA 41: CURVAS DE ENTRENAMIENTO -->
        <div class="slide hidden" data-slide="41">
            <h2>34. Curvas de Entrenamiento</h2>
            <div class="single-chart-container">
                <h3>Evoluci√≥n del Entrenamiento - Loss y Accuracy</h3>
                <img src="/content/training_curves.png" alt="Curvas de Entrenamiento" style="max-width: 100%; border-radius: 15px;">
            </div>

            <h3>Interpretaci√≥n de las Curvas</h3>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>‚úÖ Indicadores Positivos</h4>
                    <ul>
                        <li><strong>Convergencia estable:</strong> P√©rdida disminuye consistentemente</li>
                        <li><strong>Sin overfitting:</strong> Train y validation siguen trayectorias similares</li>
                        <li><strong>Precisi√≥n creciente:</strong> Mejora progresiva hasta ~90%</li>
                        <li><strong>Estabilizaci√≥n:</strong> Alcanza plateau cerca de la √©poca 40</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>üìä Observaciones</h4>
                    <ul>
                        <li><strong>Convergencia lenta inicial:</strong> Primeras 10 √©pocas con mejora gradual</li>
                        <li><strong>Estabilizaci√≥n final:</strong> √öltimas √©pocas sin mejora significativa</li>
                        <li><strong>Variabilidad validaci√≥n:</strong> Ligeras fluctuaciones normales</li>
                    </ul>
                </div>
            </div>

            <h3>Conclusiones del Entrenamiento</h3>
            <ul>
                <li><strong>Entrenamiento exitoso:</strong> El modelo aprendi√≥ efectivamente los patrones</li>
                <li><strong>Buena generalizaci√≥n:</strong> Rendimiento similar en train y validation</li>
                <li><strong>Punto √≥ptimo alcanzado:</strong> ~50 √©pocas fueron suficientes</li>
                <li><strong>Arquitectura adecuada:</strong> Complejidad apropiada para el problema</li>
            </ul>
            <footer class="footnote">
                <strong>Loss/P√©rdida:</strong> Medida de qu√© tan incorrectas son las predicciones del modelo. <strong>Accuracy:</strong> Porcentaje de predicciones correctas. <strong>Plateau:</strong> Zona donde la m√©trica se estabiliza sin mejoras significativas. <strong>√âpoca:</strong> Una pasada completa por todos los datos de entrenamiento.
            </footer>
        </div>

        <!-- DIAPOSITIVA 42: IMPLEMENTACI√ìN T√âCNICA -->
        <div class="slide hidden" data-slide="42">
            <h2>35. Implementaci√≥n del Modelo CNN 1D</h2>
            <h3>1. Preparaci√≥n de Datos (C√≥digo Real del Proyecto)</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Preprocesamiento de Datos</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# A√±adir una dimensi√≥n para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            </div>
            <p class="code-description">Dividimos los datos estratificadamente, estandarizamos y reformateamos para CNN 1D. La dimensi√≥n extra es requerida por las capas Conv1D de Keras.</p>
            
            <h3>2. Definici√≥n y Entrenamiento del Modelo</h3>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Arquitectura del Modelo</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D, BatchNormalization

model = Sequential([
    Conv1D(128, 5, padding='same', activation='relu', input_shape=(180, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    Conv1D(64, 5, padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    GlobalAveragePooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.15),
    Dense(7, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>
            </div>
            <p class="code-description">Arquitectura completa con BatchNormalization y GlobalAveragePooling para mejor rendimiento y menor overfitting.</p>
            <footer class="footnote">
                <strong>Estratificado:</strong> Mantiene la proporci√≥n de clases en train y test. <strong>expand_dims:</strong> A√±ade una dimensi√≥n para compatibilidad con Conv1D. <strong>Sequential:</strong> Modelo donde las capas se apilan secuencialmente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 43: FUNCI√ìN DE PREDICCI√ìN -->
        <div class="slide hidden" data-slide="43">
            <h2>36. Funci√≥n de Predicci√≥n en Tiempo Real</h2>
            <div class="code-block-container">
                <div class="code-block-header">
                    <span class="code-title">Predicci√≥n de Emociones</span>
                    <div class="window-controls">
                        <span class="control-btn minimize"></span>
                        <span class="control-btn maximize"></span>
                        <span class="control-btn close"></span>
                    </div>
                </div>
                <pre><code>def predict_emotion(audio_file_path, model, scaler, label_encoder, extractor):
    """
    Predice la emoci√≥n de un archivo de audio usando el modelo entrenado
    """
    try:
        # Extraer caracter√≠sticas del audio
        features = extractor.extract_features(audio_file_path)
        if features is None:
            return {"error": "No se pudieron extraer caracter√≠sticas"}
        
        # Preprocesar caracter√≠sticas
        features_scaled = scaler.transform(features.reshape(1, -1))
        features_cnn = np.expand_dims(features_scaled, axis=2)
        
        # Predecir con el modelo
        predictions = model.predict(features_cnn, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        
        # Decodificar resultado
        predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]
        
        return {
            "predicted_emotion": predicted_emotion,
            "confidence": confidence,
            "all_probabilities": {
                emotion: float(prob) 
                for emotion, prob in zip(label_encoder.classes_, predictions[0])
            }
        }
        
    except Exception as e:
        return {"error": f"Error en predicci√≥n: {str(e)}"}

# Ejemplo de uso
result = predict_emotion("audio.wav", model, scaler, label_encoder, extractor)
print(f"Emoci√≥n: {result['predicted_emotion']}")
print(f"Confianza: {result['confidence']:.2%}")</code></pre>
            </div>
            <p class="code-description">Funci√≥n completa que toma un archivo de audio y devuelve la emoci√≥n predicha con su nivel de confianza.</p>
            <footer class="footnote">
                <strong>Pipeline de predicci√≥n:</strong> Secuencia completa desde audio crudo hasta predicci√≥n final. <strong>Verbose:</strong> Par√°metro que controla la cantidad de informaci√≥n mostrada durante ejecuci√≥n. <strong>inverse_transform:</strong> Convierte c√≥digos num√©ricos de vuelta a etiquetas originales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 44: RECURSOS T√âCNICOS -->
        <div class="slide hidden" data-slide="44">
            <h2>37. Recursos y Tecnolog√≠as Utilizadas</h2>
            <div class="dataset-card">
                <h3>Lenguajes y Frameworks Principales</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal de desarrollo</li>
                    <li><strong>TensorFlow 2.18.0:</strong> Framework de deep learning</li>
                    <li><strong>Keras:</strong> API de alto nivel para construcci√≥n de modelos</li>
                    <li><strong>NumPy:</strong> Computaci√≥n num√©rica eficiente</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Librer√≠as de Procesamiento de Audio</h3>
                <ul>
                    <li><strong>Librosa:</strong> Extracci√≥n de caracter√≠sticas de audio</li>
                    <li><strong>SciPy:</strong> Procesamiento de se√±ales</li>
                    <li><strong>PyAudio:</strong> Manejo de streams de audio</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Herramientas de An√°lisis y Visualizaci√≥n</h3>
                <ul>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y m√©tricas de evaluaci√≥n</li>
                    <li><strong>Matplotlib/Seaborn:</strong> Visualizaci√≥n de datos y resultados</li>
                    <li><strong>Pandas:</strong> Manipulaci√≥n y an√°lisis de datos</li>
                    <li><strong>Plotly:</strong> Visualizaciones interactivas 3D</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Plataforma de Desarrollo</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU gratuita</li>
                    <li><strong>Kaggle:</strong> Fuente de datasets y kernels de referencia</li>
                    <li><strong>GitHub:</strong> Control de versiones y colaboraci√≥n</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Framework:</strong> Conjunto de herramientas y librer√≠as que facilitan el desarrollo. <strong>API:</strong> Interfaz de programaci√≥n que simplifica el uso de funcionalidades complejas. <strong>GPU:</strong> Unidad de procesamiento gr√°fico, acelera el entrenamiento de redes neuronales.
            </footer>
        </div>

        <!-- DIAPOSITIVA 45: ALCANCE Y LIMITACIONES -->
        <div class="slide hidden" data-slide="45">
            <h2>38. Alcance del Proyecto y Limitaciones</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>‚úÖ Incluido en el Proyecto</h3>
                    <ul>
                        <li><strong>Modelo CNN 1D completo:</strong> Arquitectura optimizada para clasificar 7 emociones</li>
                        <li><strong>Pipeline de extracci√≥n:</strong> 180 caracter√≠sticas avanzadas (MFCCs + Chroma + Mel)</li>
                        <li><strong>An√°lisis comparativo:</strong> Visualizaci√≥n 3D de PCA vs LDA</li>
                        <li><strong>Evaluaci√≥n exhaustiva:</strong> M√©tricas detalladas y matriz de confusi√≥n</li>
                        <li><strong>Funci√≥n de predicci√≥n:</strong> Sistema listo para audio en tiempo real</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>‚ö†Ô∏è Limitaciones y Exclusiones</h3>
                    <ul>
                        <li><strong>Solo modalidad de audio:</strong> No incluye an√°lisis de texto o video</li>
                        <li><strong>Sin interfaz de usuario:</strong> No se desarroll√≥ aplicaci√≥n web/m√≥vil</li>
                        <li><strong>No tiempo real:</strong> Requiere procesamiento previo del audio</li>
                        <li><strong>Idiomas limitados:</strong> Entrenado principalmente en ingl√©s</li>
                        <li><strong>Duraci√≥n fija:</strong> Optimizado para clips de 2-4 segundos</li>
                    </ul>
                </div>
            </div>

            <h3>Consideraciones T√©cnicas</h3>
            <ul>
                <li><strong>Hardware requerido:</strong> M√≠nimo 4GB RAM para inferencia, 8GB+ para entrenamiento</li>
                <li><strong>Tiempo de procesamiento:</strong> ~2-5ms por audio en CPU, <1ms en GPU</li>
                <li><strong>Formato de entrada:</strong> Archivos WAV de 22.05kHz recomendados</li>
                <li><strong>Escalabilidad:</strong> Modelo puede procesar batches para mayor eficiencia</li>
            </ul>
            <footer class="footnote">
                <strong>Modalidad:</strong> Tipo de dato de entrada (audio, texto, imagen, etc.). <strong>Tiempo real:</strong> Procesamiento instant√°neo mientras se recibe la entrada. <strong>Inferencia:</strong> Proceso de hacer predicciones con un modelo ya entrenado. <strong>Batch:</strong> Procesamiento de m√∫ltiples muestras simult√°neamente.
            </footer>
        </div>

        <!-- DIAPOSITIVA 46: M√âTRICAS AVANZADAS -->
        <div class="slide hidden" data-slide="46">
            <h2>39. M√©tricas de Evaluaci√≥n Detalladas</h2>
            <div class="formula-block">
                <span class="formula-title">M√©tricas de Clasificaci√≥n Multiclase</span>
                <p><strong>Accuracy Global:</strong> $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{1310}{1460} = 89.73\%$</p>
                <p><strong>Precision por clase:</strong> $\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}$</p>
                <p><strong>Recall por clase:</strong> $\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}$</p>
                <p><strong>F1-Score:</strong> $F1_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}$</p>
            </div>

            <h3>An√°lisis de Rendimiento Avanzado</h3>
            <div class="stats-grid">
                <div class="stat-item"><h3>Macro Avg</h3><p>F1: 0.88<br>Promedio simple</p></div>
                <div class="stat-item"><h3>Weighted Avg</h3><p>F1: 0.90<br>Ponderado por soporte</p></div>
                <div class="stat-item"><h3>Kappa Score</h3><p>Œ∫ ‚âà 0.87<br>Acuerdo ajustado</p></div>
                <div class="stat-item"><h3>Top-2 Accuracy</h3><p>~96%<br>Predicci√≥n correcta en top-2</p></div>
            </div>

            <h3>Interpretaci√≥n de M√©tricas</h3>
            <ul>
                <li><strong>Macro vs Weighted:</strong> Diferencia m√≠nima indica balance entre clases</li>
                <li><strong>Kappa alto (0.87):</strong> Acuerdo substancial, descarta coincidencia casual</li>
                <li><strong>Top-2 accuracy:</strong> En 96% de casos, la emoci√≥n correcta est√° entre las 2 m√°s probables</li>
                <li><strong>Consistencia cross-m√©trica:</strong> Todas las m√©tricas son coherentemente altas</li>
            </ul>

            <h3>Comparaci√≥n con Literatura</h3>
            <p>Nuestro F1-Score de 0.90 es competitivo comparado con estudios recientes:</p>
            <ul>
                <li>RNN + LSTM: 85-88% (Issa et al., 2020)</li>
                <li>CNN 2D + Spectrograms: 87-91% (Zhao et al., 2019)</li>
                <li><strong>Nuestro CNN 1D: 90%</strong> (Este trabajo)</li>
            </ul>
            <footer class="footnote">
                <strong>Macro avg:</strong> Promedio simple de m√©tricas por clase. <strong>Weighted avg:</strong> Promedio ponderado por n√∫mero de muestras. <strong>Kappa:</strong> Medida de acuerdo que considera la concordancia por azar. <strong>Top-k accuracy:</strong> M√©trica que considera correctas las k predicciones m√°s probables.
            </footer>
        </div>

        <!-- DIAPOSITIVA 47: AN√ÅLISIS DE COMPLEJIDAD -->
        <div class="slide hidden" data-slide="47">
            <h2>40. An√°lisis de Complejidad Computacional</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Complejidad Temporal por Operaci√≥n</h4>
                    
                    <div class="formula-block">
                        <span class="formula-title">An√°lisis Big O del Modelo</span>
                        <p><strong>Conv1D_1:</strong> $O(L \times K \times C_{in} \times C_{out}) = O(180 \times 5 \times 1 \times 128) = O(115,200)$</p>
                        <p><strong>Conv1D_2:</strong> $O(90 \times 5 \times 128 \times 64) = O(3,686,400)$</p>
                        <p><strong>Dense layers:</strong> $O(64^2 + 64 \times 7) = O(4,544)$</p>
                        <p><strong>Total por muestra:</strong> $O(3,806,144)$ ‚âà $O(3.8M)$ operaciones</p>
                    </div>

                    <h4>Memoria Requerida</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Componente</th>
                                <th>Tama√±o (MB)</th>
                                <th>Descripci√≥n</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Par√°metros del modelo</td>
                                <td>0.18</td>
                                <td>47,175 √ó 4 bytes (float32)</td>
                            </tr>
                            <tr>
                                <td>Activaciones (batch=64)</td>
                                <td>3.2</td>
                                <td>Almacenamiento intermedio</td>
                            </tr>
                            <tr>
                                <td>Gradientes (entrenamiento)</td>
                                <td>0.18</td>
                                <td>Mismo tama√±o que par√°metros</td>
                            </tr>
                            <tr>
                                <td><strong>Total m√≠nimo (inferencia)</strong></td>
                                <td><strong>3.4</strong></td>
                                <td>Muy eficiente en memoria</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3>Escalabilidad y Deployment</h3>
            <ul>
                <li><strong>Edge Computing:</strong> Suficientemente ligero para dispositivos m√≥viles</li>
                <li><strong>Batch Processing:</strong> Eficiencia aumenta linealmente con tama√±o de batch</li>
                <li><strong>Paralelizaci√≥n:</strong> Compatible con GPU para procesamiento masivo</li>
                <li><strong>Cuantizaci√≥n:</strong> Potencial reducci√≥n a INT8 (75% menos memoria)</li>
            </ul>
            <footer class="footnote">
                <strong>Big O:</strong> Notaci√≥n que describe la complejidad algoritmica en funci√≥n del tama√±o de entrada. <strong>Edge Computing:</strong> Procesamiento en dispositivos locales en lugar de servidores remotos. <strong>Cuantizaci√≥n:</strong> Reducir precisi√≥n num√©rica para ahorrar memoria y acelerar inferencia.
            </footer>
        </div>

        <!-- DIAPOSITIVA 48: AGRADECIMIENTOS -->
        <div class="slide hidden" data-slide="48">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de caracter√≠sticas y clasificar emociones complejas en la voz humana.
            </p>
            <div class="highlight-box">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
                <p>Hemos cubierto desde los fundamentos matem√°ticos hasta la implementaci√≥n t√©cnica completa del modelo CNN 1D para reconocimiento de emociones en audio, incluyendo an√°lisis detallado de cada componente arquitectural.</p>
            </div>

            <div class="stats-grid">
                <div class="stat-item"><h3>89.73%</h3><p>Precisi√≥n Final</p></div>
                <div class="stat-item"><h3>47,175</h3><p>Par√°metros Totales</p></div>
                <div class="stat-item"><h3>7,296</h3><p>Audios Procesados</p></div>
                <div class="stat-item"><h3>180</h3><p>Caracter√≠sticas Extra√≠das</p></div>
            </div>

            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>

            <div class="methodology-step" style="margin-top: 30px; text-align: center;">
                <strong>Repositorio del Proyecto:</strong> <a href="https://github.com/your-repo" style="color: var(--color-primary);">github.com/emotion-recognition-cnn1d</a>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="presentation\src\js\script.js"></script>
</body>
</html>