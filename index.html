<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 34</div>

    <div class="presentation-container">
        <!-- Diapositivas 1-14 se mantienen intactas -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
        </div>
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
        </div>
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, que nos permite refinar y mejorar continuamente nuestro modelo bas√°ndonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentaci√≥n y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisici√≥n de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. An√°lisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracci√≥n de Caracter√≠sticas</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluaci√≥n y Resultados</h4>
                </div>
            </div>
            <footer class="footnote" style="position: relative; border-top: none; text-align: center; margin-top: 20px;">
                Los resultados de la fase de <strong>Evaluaci√≥n</strong> informan ajustes en las fases anteriores, creando un ciclo de refinamiento.
            </footer>
        </div>
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituy√≥ un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>
        <div class="slide hidden" data-slide="8">
            <h2>7. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribuci√≥n de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gr√°ficos de Viol√≠n de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>Nota:</strong> Las siguientes visualizaciones fueron generadas con un script de Python para analizar las caracter√≠sticas del dataset RAVDESS.
            </footer>
        </div>
        <div class="slide hidden" data-slide="9">
            <h2>8. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Pitch: Distribuci√≥n de Frecuencia Fundamental (F0) por Emoci√≥n</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica nos muestra c√≥mo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoci√≥n. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores m√°s altos significan un tono m√°s agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qu√© tan comunes son ciertos tonos para una emoci√≥n.
                </p>
                <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energ√≠a como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos m√°s agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos m√°s graves y mon√≥tonos.
                </footer>
            </div>
        </div>
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n y An√°lisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>An√°lisis Espectral: Distribuci√≥n de los Primeros 13 MFCCs por Emoci√≥n</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gr√°ficos de Viol√≠n de MFCCs por Emoci√≥n" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualizaci√≥n nos permite comparar la "forma" del sonido para cada emoci√≥n a trav√©s de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "viol√≠n" muestra el rango y la concentraci√≥n de valores para un coeficiente (eje X) y una emoci√≥n (color).
                </p>
                 <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribuci√≥n para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posici√≥n de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoci√≥n de otra. Coeficientes con distribuciones muy distintas entre colores son altamente informativos para la clasificaci√≥n.
                </footer>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretaci√≥n General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energ√≠a total o sonoridad de la se√±al.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles m√°s finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <div class="slide hidden" data-slide="11">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoci√≥n, debemos traducir la onda de sonido a un lenguaje que entienda: los n√∫meros. Este proceso se llama <strong>Extracci√≥n de Caracter√≠sticas</strong>. A continuaci√≥n, veremos el paso a paso de c√≥mo convertimos un archivo de audio en un √∫nico vector de 180 caracter√≠sticas.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">üîä</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">#Ô∏è‚É£</div>
                    <div class="flow-text">Digitalizaci√≥n (Muestreo)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üñºÔ∏è</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üìä</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                     <div class="flow-icon">üß¨</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">Este es el coraz√≥n del preprocesamiento: transformar datos no estructurados (audio) en datos estructurados (un vector).</footer>
        </div>
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">Paso 1: Digitalizaci√≥n y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalizaci√≥n</h4>
                    <p>Una onda de sonido es una se√±al anal√≥gica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de n√∫meros, donde cada n√∫mero es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es est√°tica. Para analizarla, la dividimos en peque√±os segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tama√±o de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colecci√≥n de muchos arreglos peque√±os (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio wave sampling and framing.png" alt="Diagrama de Digitalizaci√≥n y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en m√∫ltiples ventanas (frames) para su an√°lisis.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¬øQu√© es y para qu√© sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qu√© frecuencias la componen. La <strong>Transformada R√°pida de Fourier (FFT)</strong> es la herramienta matem√°tica que lo hace posible.</p>
                    <p>La FFT descompone la se√±al del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energ√≠a vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qu√© tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matem√°tico: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un n√∫mero complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El n√∫mero total de muestras en la ventana.</li>
                            <li>$k$: El √≠ndice de la frecuencia que se est√° calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time domain to frequency domain FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>M√°s all√° del Espectro</h4>
                    <p>El espectro de la FFT es √∫til, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho m√°s inteligente de resumir la informaci√≥n del espectro, imitando c√≥mo funciona el o√≠do humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logar√≠tmica, similar a nuestra percepci√≥n auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energ√≠as, de nuevo, para imitar c√≥mo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operaci√≥n que comprime toda esa informaci√≥n espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripci√≥n num√©rica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC block diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 4: El Vector Final de Caracter√≠sticas</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del An√°lisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de caracter√≠sticas, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 caracter√≠sticas.</p>
                    <p>Para obtener un √∫nico vector $V$ que represente todo el audio, calculamos la media de cada caracter√≠stica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">C√°lculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                           <li>$V_j$: Es el valor final de la caracter√≠stica $j$ en nuestro vector.</li>
                           <li>$T$: Es el n√∫mero total de ventanas (frames) en el audio.</li>
                           <li>$M_{t,j}$: Es el valor de la caracter√≠stica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                   <p>Este proceso condensa la informaci√≥n temporal en una sola "ficha t√©cnica" que describe las propiedades ac√∫sticas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature matrix averaging to vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="16">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
        </div>
        <div class="slide hidden" data-slide="17">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>
        <div class="slide hidden" data-slide="18">
            <h2>13. Implementaci√≥n del Modelo CNN 1D</h2>
            <h3>1. Preparaci√≥n de Datos (C√≥digo Real del Proyecto)</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# A√±adir una dimensi√≥n para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            <p class="code-description">Dividimos los datos, asegurando que cada emoci√≥n est√© representada por igual en ambos conjuntos (`stratify=y`). Luego, escalamos los datos y a√±adimos una dimensi√≥n extra, que es el formato que espera la capa `Conv1D` de Keras.</p>
            
            <h3>2. Definici√≥n y Entrenamiento del Modelo</h3>
            <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense

model = Sequential([
    Conv1D(256, 5, padding='same', activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Conv1D(128, 5, padding='same', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Flatten(),
    Dense(y_encoded.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train_cnn, y_train, epochs=100, batch_size=64, validation_split=0.2
)</code></pre>
            <p class="code-description">Definimos la arquitectura de la CNN 1D, la compilamos con el optimizador 'adam' y la funci√≥n de p√©rdida para clasificaci√≥n multiclase. Finalmente, la entrenamos con los datos de entrenamiento, usando un 20% de estos para validaci√≥n interna en cada √©poca.</p>
        </div>
        <div class="slide hidden" data-slide="19">
            <h2>14. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo.
            </footer>
        </div>
        <div class="slide hidden" data-slide="20">
            <h2>15. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qu√© tan dispersos est√°n los datos. <strong>Dispersi√≥n:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>
        <div class="slide hidden" data-slide="21">
            <h2>16. Visualizaci√≥n 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separaci√≥n entre las emociones. El resultado es una separaci√≥n mucho m√°s clara y c√∫mulos m√°s compactos, lo que confirma visualmente que nuestras caracter√≠sticas son muy efectivas para la clasificaci√≥n.
            </p>
        </div>
        <div class="slide hidden" data-slide="22">
            <h2>17. Comparaci√≥n Final y Reflexi√≥n</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersi√≥n general de los datos</td>
                        <td>Muestra qu√© tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opini√≥n Reflexiva</h3>
            <p>Al comparar ambas t√©cnicas, <strong>LDA demuestra ser m√°s efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es √∫til para entender la estructura general de la varianza en los datos, LDA, al ser un m√©todo supervisado, logra crear proyecciones donde las emociones forman c√∫mulos m√°s definidos y distinguibles. Esto sugiere que las caracter√≠sticas extra√≠das, cuando se proyectan con un objetivo de clasificaci√≥n, son altamente discriminativas.</p>
        </div>
        <div class="slide hidden" data-slide="23">
            <h2>18. Fundamento Te√≥rico: Red Neuronal Convolucional 1D</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>¬øC√≥mo "Piensa" una CNN 1D?</h4>
                    <div class="code-block-container">
                        <div class="code-block-header">
                            <span class="code-title">Pseudoc√≥digo de Operaci√≥n</span>
                            <div class="window-controls">
                                <span class="control-btn minimize"></span>
                                <span class="control-btn maximize"></span>
                                <span class="control-btn close"></span>
                            </div>
                        </div>
                        <pre>
Entrada = Secuencia de Caracter√≠sticas (180 n√∫meros)

<span class="comment">// 1. Capas Convolucionales 1D (Detectores de Patrones)</span>
Para cada filtro en la capa:
    Desliza el filtro sobre la secuencia
    Calcula la suma ponderada (convoluci√≥n)
    Aplica funci√≥n de activaci√≥n (ReLU) -> Resalta patrones

<span class="comment">// 2. Capas de Pooling 1D (Compresores de Informaci√≥n)</span>
Reduce la longitud de la secuencia (ej. toma el valor m√°ximo)
-> Mantiene la informaci√≥n m√°s relevante y descarta el resto

<span class="comment">// 3. Capas Densas (Clasificador Final)</span>
Aplana la salida a un solo vector
Conecta todas las neuronas
Aplica Softmax para calcular probabilidades

<span class="comment">// 4. Salida</span>
Predicci√≥n = Emoci√≥n con la probabilidad m√°s alta
                        </pre>
                    </div>
                    <div class="activation-function-info">
                        <h4>Funciones de Activaci√≥n Clave</h4>
                        <p><strong>ReLU (Rectified Linear Unit):</strong> <span class="formula">ReLU(x) = max(0, x)</span>. Se usa en capas ocultas para introducir no-linealidad, permitiendo al modelo aprender relaciones complejas.</p>
                        <p><strong>Softmax:</strong> Se usa en la capa de salida para convertir las puntuaciones en una distribuci√≥n de probabilidad sobre las 7 emociones.</p>
                    </div>
                </div>
        
                <div class="cnn-side-panel">
                    <div class="advantages-disadvantages-1d">
                        <h4>Ventajas del Enfoque 1D</h4>
                        <ul>
                            <li>Analiza directamente la secuencia temporal de caracter√≠sticas.</li>
                            <li>Eficiente computacionalmente, menos par√°metros que una CNN 2D.</li>
                            <li>Arquitectura ideal para cualquier tipo de se√±al o serie de tiempo.</li>
                        </ul>
                        <h4>Desventajas</h4>
                        <ul>
                            <li>Puede perder informaci√≥n contextual que un espectrograma 2D s√≠ capturar√≠a.</li>
                            <li>Depende fuertemente de la calidad de las caracter√≠sticas extra√≠das manualmente.</li>
                        </ul>
                    </div>
                    <div class="cnn-audio-sample">
                        <h4>Muestra de Audio de Entrada (Disgusto)</h4>
                        <p>Este es un ejemplo del tipo de audio que procesa el modelo antes de la extracci√≥n de caracter√≠sticas.</p>
                        <audio controls src="presentation/src/audio/03-01-07-02-02-02-11.wav">
                            Tu navegador no soporta el elemento de audio.
                        </audio>
                    </div>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="24">
            <h2>19. Arquitectura del Modelo CNN 1D</h2>
            <p>Esta es la arquitectura de Keras que implementamos. Keras es una librer√≠a de alto nivel que facilita la construcci√≥n de redes neuronales. La red est√° dise√±ada para procesar la secuencia de 180 caracter√≠sticas extra√≠das de cada audio y aprender a clasificar la emoci√≥n.</p>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <p>Secuencia de Caracter√≠sticas<br><span class="layer-shape">(180 caracter√≠sticas, 1)</span></p>
                    <p class="layer-desc">Recibe el vector de 180 caracter√≠sticas como una secuencia temporal.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (256 filtros) + ReLU</h4>
                    <p class="layer-desc">La primera capa convolucional busca patrones temporales simples en la secuencia de entrada usando 256 filtros diferentes.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Reduce la longitud de la secuencia a una quinta parte, manteniendo solo las activaciones de patrones m√°s fuertes para la siguiente capa.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (128 filtros) + ReLU</h4>
                    <p class="layer-desc">Una segunda capa convolucional busca patrones m√°s complejos y de mayor duraci√≥n a partir de las caracter√≠sticas detectadas por la capa anterior.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Vuelve a reducir la secuencia para condensar a√∫n m√°s la informaci√≥n.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dense">
                    <h4>Flatten & Capa Densa</h4>
                    <p class="layer-desc">La capa Flatten convierte la secuencia final en un solo vector largo, que se conecta a una capa densa para la clasificaci√≥n.</p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box output">
                    <h4>Capa de Salida (Softmax)</h4>
                    <p class="layer-desc">La capa final tiene 7 neuronas, una por cada emoci√≥n, y usa Softmax para asignar una probabilidad a cada una.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="25">
            <h2 class="technical-title">Dentro de la "Caja Negra": La Red Neuronal</h2>
            <p>Ahora que tenemos nuestro vector de 180 caracter√≠sticas, es el turno de la Red Neuronal Convolucional 1D. Su trabajo es tomar este vector y, a trav√©s de una serie de capas y operaciones matem√°ticas, clasificarlo en una de las siete emociones. Veremos c√≥mo lo hace.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">üß¨</div>
                    <div class="flow-text">Vector de 180 Caracter√≠sticas</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üîç</div>
                    <div class="flow-text">Capas Convolucionales</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üß†</div>
                    <div class="flow-text">Capas Densas (Clasificaci√≥n)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                     <div class="flow-icon">üè∑Ô∏è</div>
                    <div class="flow-text">Predicci√≥n de Emoci√≥n</div>
                </div>
            </div>
             <footer class="footnote">El modelo aprende a asociar patrones en el vector de entrada con una etiqueta de emoci√≥n espec√≠fica.</footer>
        </div>
        <div class="slide hidden" data-slide="26">
            <h2 class="technical-title">Red Neuronal - Paso 1: Convoluci√≥n</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Los Detectores de Patrones</h4>
                    <p>La operaci√≥n de <strong>convoluci√≥n</strong> es el coraz√≥n de la CNN. Un "filtro" (un peque√±o vector de pesos que el modelo aprende) se desliza a lo largo de nuestro vector de 180 caracter√≠sticas.</p>
                    <p>En cada posici√≥n, calcula el <strong>producto punto</strong> entre el filtro y la secci√≥n del vector. El resultado es un n√∫mero alto si la secci√≥n del vector se "parece" al patr√≥n que el filtro est√° buscando.</p>
                    <p>Nuestro modelo usa 256 filtros en la primera capa, cada uno buscando un patr√≥n simple y diferente. El resultado son 256 "mapas de caracter√≠sticas" que nos dicen d√≥nde se encontraron esos patrones.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/1Dconvolutionanimationgif.gif" alt="Animaci√≥n de Convoluci√≥n 1D" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Un filtro (amarillo) se desliza sobre la entrada (azul) para producir un mapa de caracter√≠sticas (verde).</p>
                </div>
            </div>
            <footer class="footnote"><strong>Producto Punto:</strong> Una operaci√≥n matem√°tica que multiplica los elementos correspondientes de dos vectores y suma los resultados. Mide qu√© tan 'alineados' o similares son los dos vectores.</footer>
        </div>
        <div class="slide hidden" data-slide="27">
            <h2 class="technical-title">Red Neuronal - Paso 2: ReLU y Pooling</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Filtrar y Resumir</h4>
                    <p>Despu√©s de la convoluci√≥n, se aplican dos operaciones simples pero cruciales:</p>
                    <ol>
                        <li><strong>Activaci√≥n ReLU:</strong> A cada n√∫mero en los mapas de caracter√≠sticas, le aplicamos la funci√≥n `ReLU(x) = max(0, x)`. Esto elimina todas las activaciones negativas, permitiendo que el modelo se enfoque solo en la presencia significativa de un patr√≥n.</li>
                        <li><strong>MaxPooling:</strong> Reducimos el tama√±o de cada mapa de caracter√≠sticas. Por ejemplo, de cada 5 valores nos quedamos solo con el m√°s alto. Esto hace al modelo m√°s eficiente y robusto, ya que se concentra en si un patr√≥n apareci√≥ en una regi√≥n, no en su posici√≥n exacta.</li>
                    </ol>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/ReLU and Max Pooling diagram.png" alt="Diagrama de ReLU y Pooling" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">ReLU elimina los valores negativos. MaxPooling resume la informaci√≥n, qued√°ndose con los picos de activaci√≥n.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="28">
            <h2 class="technical-title">Red Neuronal - Paso 3: Capas Profundas</h2>
             <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Creando Abstracci√≥n</h4>
                    <p>El verdadero poder de las redes profundas viene de <strong>apilar</strong> estas capas. La salida de un bloque `Convoluci√≥n -> ReLU -> Pooling` se convierte en la entrada del siguiente.</p>
                    <p>Los filtros de la segunda capa ya no buscan patrones en el vector original, sino que buscan <strong>patrones de patrones</strong>. Por ejemplo, un filtro puede aprender a activarse si detecta el "patr√≥n A" seguido del "patr√≥n B" de la capa anterior. Esto crea una jerarqu√≠a de caracter√≠sticas, donde el modelo aprende conceptos cada vez m√°s complejos y abstractos.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/CNN feature hierarchy.png" alt="Arquitectura Profunda" style="width:90%; margin: 20px auto; display: block; border-radius: 10px;">
                     <p class="viz-caption">Las capas se apilan para que el modelo aprenda de lo simple a lo complejo.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="29">
            <h2 class="technical-title">Red Neuronal - Paso 4: Clasificaci√≥n Final</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>La Toma de Decisi√≥n</h4>
                    <ol>
                        <li><strong>Flatten (Aplanar):</strong> Despu√©s de la √∫ltima capa de pooling, todos los mapas de caracter√≠sticas se "aplanan" y se unen en un solo vector largu√≠simo.</li>
                        <li><strong>Capas Densas:</strong> Este vector entra a una o m√°s capas "densas", donde cada neurona est√° conectada a todas las del vector aplanado. Estas capas aprenden a "votar" por una emoci√≥n bas√°ndose en la combinaci√≥n de todos los patrones detectados.</li>
                        <li><strong>Capa de Salida (Softmax):</strong> La √∫ltima capa tiene 7 neuronas (una por emoci√≥n). La funci√≥n <strong>Softmax</strong> toma las 7 puntuaciones finales y las convierte en un vector de probabilidades que suma 1.</li>
                    </ol>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/neural network flatten dense layer softmax.png" alt="Diagrama de Capas Densas y Softmax" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Las caracter√≠sticas abstractas se conectan a las capas de decisi√≥n, que emiten una probabilidad para cada clase.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="30">
            <h2 class="technical-title">El Aprendizaje: ¬øC√≥mo se Entrena el Modelo?</h2>
            <p>El entrenamiento es un ciclo de optimizaci√≥n para ajustar los 171,783 pesos de los filtros y las capas densas. Se repite miles de veces hasta que el error es m√≠nimo.</p>
            <div class="training-steps-grid">
                <div class="training-step-card">
                    <div class="step-number">1</div>
                    <h4>Predicci√≥n (Forward Pass)</h4>
                    <p>El modelo recibe datos de entrada y genera una predicci√≥n de la emoci√≥n, propagando la informaci√≥n a trav√©s de sus capas.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">2</div>
                    <h4>C√°lculo de la P√©rdida (Error)</h4>
                    <p>Comparamos la predicci√≥n del modelo con la etiqueta de emoci√≥n real. La funci√≥n de p√©rdida (entrop√≠a cruzada categ√≥rica) cuantifica qu√© tan "equivocado" estuvo el modelo.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">3</div>
                    <h4>Backpropagation (Retropropagaci√≥n)</h4>
                    <p>El error calculado se propaga hacia atr√°s a trav√©s de la red. Esto nos permite determinar c√≥mo cada peso individual contribuy√≥ al error total.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">4</div>
                    <h4>Actualizaci√≥n de Pesos (Optimizaci√≥n)</h4>
                    <p>Utilizando un optimizador (como Adam), los pesos del modelo se ajustan ligeramente en la direcci√≥n que minimiza la funci√≥n de p√©rdida, preparando al modelo para una mejor predicci√≥n en la siguiente iteraci√≥n.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="31">
            <h2>20. Resumen de Neuronas y Par√°metros</h2>
            <div class="cnn-details">
                <p>
                    Esta tabla detalla la estructura de nuestro modelo. La <strong>Forma de Salida</strong> muestra c√≥mo cambian las dimensiones de los datos despu√©s de cada capa. Los <strong>Par√°metros</strong> son los pesos o "conocimientos" que el modelo aprende durante el entrenamiento. En total, nuestro modelo debe aprender y ajustar **171,783 par√°metros** para poder realizar la clasificaci√≥n.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa (Tipo)</th>
                            <th>Forma de Salida</th>
                            <th>Par√°metros</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>conv1d</td>
                            <td>(None, 180, 256)</td>
                            <td>1,536</td>
                        </tr>
                        <tr>
                            <td>max_pooling1d</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>conv1d_1</td>
                            <td>(None, 36, 128)</td>
                            <td>163,968</td>
                        </tr>
                         <tr>
                            <td>max_pooling1d_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>flatten</td>
                            <td>(None, 896)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dense</td>
                            <td>(None, 7)</td>
                            <td>6,279</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="2"><strong>Total de Par√°metros Entrenables</strong></td>
                            <td><strong>171,783</strong></td>
                        </tr>
                    </tbody>
                </table>
                 <footer class="cnn-table-footnote">
                    <strong>Par√°metros:</strong> Son los "conocimientos" internos de la red que se ajustan durante el entrenamiento.
                </footer>
            </div>
        </div>

        <!-- Diapositivas Finales -->
        <div class="slide hidden" data-slide="32">
            <h2>21. Recursos</h2>
            <div class="dataset-card">
                <h3>Lenguaje y Frameworks</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal</li>
                    <li><strong>TensorFlow/Keras:</strong> Desarrollo de CNN 1D</li>
                    <li><strong>Librosa:</strong> Extracci√≥n de caracter√≠sticas de audio</li>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y m√©tricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Hardware y Plataformas</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU.</li>
                    <li><strong>GitHub:</strong> Control de versiones.</li>
                </ul>
            </div>
        </div>
        <div class="slide hidden" data-slide="33">
            <h2>22. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo CNN 1D para clasificar 7 emociones.</li>
                        <li>Pipeline de extracci√≥n de 180 caracter√≠sticas.</li>
                        <li>Comparaci√≥n visual 3D de PCA y LDA.</li>
                        <li>M√©tricas de rendimiento del modelo.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>An√°lisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicaci√≥n de usuario final.</li>
                        <li>El modelo no opera en tiempo real.</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="34">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de caracter√≠sticas y clasificar emociones complejas en la voz.
            </p>
            <div class="highlight-box">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="presentation/src/js/script.js"></script>
</body>
</html>
