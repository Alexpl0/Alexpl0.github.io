<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 40</div>

    <div class="presentation-container">
        <!-- Diapositivas originales 1-23 se mantienen intactas -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>

        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
        </div>

        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>

        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>

        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
        </div>

        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, que nos permite refinar y mejorar continuamente nuestro modelo bas√°ndonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentaci√≥n y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisici√≥n de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. An√°lisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracci√≥n de Caracter√≠sticas</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluaci√≥n y Resultados</h4>
                </div>
            </div>
            <footer class="footnote" style="position: relative; border-top: none; text-align: center; margin-top: 20px;">
                Los resultados de la fase de <strong>Evaluaci√≥n</strong> informan ajustes en las fases anteriores, creando un ciclo de refinamiento.
            </footer>
        </div>

        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituy√≥ un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>

        <div class="slide hidden" data-slide="8">
            <h2>7. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribuci√≥n de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gr√°ficos de Viol√≠n de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>Nota:</strong> Las siguientes visualizaciones fueron generadas con un script de Python para analizar las caracter√≠sticas del dataset RAVDESS.
            </footer>
        </div>

        <div class="slide hidden" data-slide="9">
            <h2>8. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>An√°lisis de Pitch: Distribuci√≥n de Frecuencia Fundamental (F0) por Emoci√≥n</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica nos muestra c√≥mo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoci√≥n. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores m√°s altos significan un tono m√°s agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qu√© tan comunes son ciertos tonos para una emoci√≥n.
                </p>
                <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energ√≠a como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos m√°s agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos m√°s graves y mon√≥tonos.
                </footer>
            </div>
        </div>

        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n y An√°lisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>An√°lisis Espectral: Distribuci√≥n de los Primeros 13 MFCCs por Emoci√≥n</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gr√°ficos de Viol√≠n de MFCCs por Emoci√≥n" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualizaci√≥n nos permite comparar la "forma" del sonido para cada emoci√≥n a trav√©s de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "viol√≠n" muestra el rango y la concentraci√≥n de valores para un coeficiente (eje X) y una emoci√≥n (color).
                </p>
                 <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribuci√≥n para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posici√≥n de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoci√≥n de otra. Coeficientes con distribuciones muy distintas entre colores son altamente informativos para la clasificaci√≥n.
                </footer>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretaci√≥n General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energ√≠a total o sonoridad de la se√±al.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles m√°s finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="slide hidden" data-slide="11">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoci√≥n, debemos traducir la onda de sonido a un lenguaje que entienda: los n√∫meros. Este proceso se llama <strong>Extracci√≥n de Caracter√≠sticas</strong>. A continuaci√≥n, veremos el paso a paso de c√≥mo convertimos un archivo de audio en un √∫nico vector de 180 caracter√≠sticas.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">üîä</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">#Ô∏è‚É£</div>
                    <div class="flow-text">Digitalizaci√≥n (Muestreo)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üñºÔ∏è</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                    <div class="flow-icon">üìä</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">‚Üí</div>
                <div class="flow-step">
                     <div class="flow-icon">üß¨</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">Este es el coraz√≥n del preprocesamiento: transformar datos no estructurados (audio) en datos estructurados (un vector).</footer>
        </div>

        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">Paso 1: Digitalizaci√≥n y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalizaci√≥n</h4>
                    <p>Una onda de sonido es una se√±al anal√≥gica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de n√∫meros, donde cada n√∫mero es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es est√°tica. Para analizarla, la dividimos en peque√±os segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tama√±o de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colecci√≥n de muchos arreglos peque√±os (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio wave sampling and framing.png" alt="Diagrama de Digitalizaci√≥n y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en m√∫ltiples ventanas (frames) para su an√°lisis.</p>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¬øQu√© es y para qu√© sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qu√© frecuencias la componen. La <strong>Transformada R√°pida de Fourier (FFT)</strong> es la herramienta matem√°tica que lo hace posible.</p>
                    <p>La FFT descompone la se√±al del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energ√≠a vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qu√© tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matem√°tico: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un n√∫mero complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El n√∫mero total de muestras en la ventana.</li>
                            <li>$k$: El √≠ndice de la frecuencia que se est√° calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time domain to frequency domain FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>M√°s all√° del Espectro</h4>
                    <p>El espectro de la FFT es √∫til, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho m√°s inteligente de resumir la informaci√≥n del espectro, imitando c√≥mo funciona el o√≠do humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logar√≠tmica, similar a nuestra percepci√≥n auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energ√≠as, de nuevo, para imitar c√≥mo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operaci√≥n que comprime toda esa informaci√≥n espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripci√≥n num√©rica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC block diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 4: El Vector Final de Caracter√≠sticas</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del An√°lisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de caracter√≠sticas, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 caracter√≠sticas.</p>
                    <p>Para obtener un √∫nico vector $V$ que represente todo el audio, calculamos la media de cada caracter√≠stica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">C√°lculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                           <li>$V_j$: Es el valor final de la caracter√≠stica $j$ en nuestro vector.</li>
                           <li>$T$: Es el n√∫mero total de ventanas (frames) en el audio.</li>
                           <li>$M_{t,j}$: Es el valor de la caracter√≠stica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                   <p>Este proceso condensa la informaci√≥n temporal en una sola "ficha t√©cnica" que describe las propiedades ac√∫sticas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature matrix averaging to vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="16">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
        </div>

        <div class="slide hidden" data-slide="17">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>

        <div class="slide hidden" data-slide="18">
            <h2>13. Implementaci√≥n del Modelo CNN 1D</h2>
            <h3>1. Preparaci√≥n de Datos (C√≥digo Real del Proyecto)</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# A√±adir una dimensi√≥n para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            <p class="code-description">Dividimos los datos, asegurando que cada emoci√≥n est√© representada por igual en ambos conjuntos (`stratify=y`). Luego, escalamos los datos y a√±adimos una dimensi√≥n extra, que es el formato que espera la capa `Conv1D` de Keras.</p>
            
            <h3>2. Definici√≥n y Entrenamiento del Modelo</h3>
            <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense

model = Sequential([
    Conv1D(256, 5, padding='same', activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Conv1D(128, 5, padding='same', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Flatten(),
    Dense(y_encoded.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train_cnn, y_train, epochs=100, batch_size=64, validation_split=0.2
)</code></pre>
            <p class="code-description">Definimos la arquitectura de la CNN 1D, la compilamos con el optimizador 'adam' y la funci√≥n de p√©rdida para clasificaci√≥n multiclase. Finalmente, la entrenamos con los datos de entrenamiento, usando un 20% de estos para validaci√≥n interna en cada √©poca.</p>
        </div>

        <div class="slide hidden" data-slide="19">
            <h2>14. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo.
            </footer>
        </div>

        <div class="slide hidden" data-slide="20">
            <h2>15. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qu√© tan dispersos est√°n los datos. <strong>Dispersi√≥n:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>

        <div class="slide hidden" data-slide="21">
            <h2>16. Visualizaci√≥n 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separaci√≥n entre las emociones. El resultado es una separaci√≥n mucho m√°s clara y c√∫mulos m√°s compactos, lo que confirma visualmente que nuestras caracter√≠sticas son muy efectivas para la clasificaci√≥n.
            </p>
        </div>

        <div class="slide hidden" data-slide="22">
            <h2>17. Comparaci√≥n Final y Reflexi√≥n</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersi√≥n general de los datos</td>
                        <td>Muestra qu√© tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opini√≥n Reflexiva</h3>
            <p>Al comparar ambas t√©cnicas, <strong>LDA demuestra ser m√°s efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es √∫til para entender la estructura general de la varianza en los datos, LDA, al ser un m√©todo supervisado, logra crear proyecciones donde las emociones forman c√∫mulos m√°s definidos y distinguibles. Esto sugiere que las caracter√≠sticas extra√≠das, cuando se proyectan con un objetivo de clasificaci√≥n, son altamente discriminativas.</p>
        </div>

        <div class="slide hidden" data-slide="23">
            <h2>18. Fundamento Te√≥rico: Red Neuronal Convolucional 1D</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>¬øC√≥mo "Piensa" una CNN 1D?</h4>
                    <div class="code-block-container">
                        <div class="code-block-header">
                            <span class="code-title">Pseudoc√≥digo de Operaci√≥n</span>
                            <div class="window-controls">
                                <span class="control-btn minimize"></span>
                                <span class="control-btn maximize"></span>
                                <span class="control-btn close"></span>
                            </div>
                        </div>
                        <pre>
Entrada = Secuencia de Caracter√≠sticas (180 n√∫meros)

<span class="comment">// 1. Capas Convolucionales 1D (Detectores de Patrones)</span>
Para cada filtro en la capa:
    Desliza el filtro sobre la secuencia
    Calcula la suma ponderada (convoluci√≥n)
    Aplica funci√≥n de activaci√≥n (ReLU) -> Resalta patrones

<span class="comment">// 2. Capas de Pooling 1D (Compresores de Informaci√≥n)</span>
Reduce la longitud de la secuencia (ej. toma el valor m√°ximo)
-> Mantiene la informaci√≥n m√°s relevante y descarta el resto

<span class="comment">// 3. Capas Densas (Clasificador Final)</span>
Aplana la salida a un solo vector
Conecta todas las neuronas
Aplica Softmax para calcular probabilidades

<span class="comment">// 4. Salida</span>
Predicci√≥n = Emoci√≥n con la probabilidad m√°s alta
                        </pre>
                    </div>
                    <div class="activation-function-info">
                        <h4>Funciones de Activaci√≥n Clave</h4>
                        <p><strong>ReLU (Rectified Linear Unit):</strong> <span class="formula">ReLU(x) = max(0, x)</span>. Se usa en capas ocultas para introducir no-linealidad, permitiendo al modelo aprender relaciones complejas.</p>
                        <p><strong>Softmax:</strong> Se usa en la capa de salida para convertir las puntuaciones en una distribuci√≥n de probabilidad sobre las 7 emociones.</p>
                    </div>
                </div>
        
                <div class="cnn-side-panel">
                    <div class="advantages-disadvantages-1d">
                        <h4>Ventajas del Enfoque 1D</h4>
                        <ul>
                            <li>Analiza directamente la secuencia temporal de caracter√≠sticas.</li>
                            <li>Eficiente computacionalmente, menos par√°metros que una CNN 2D.</li>
                            <li>Arquitectura ideal para cualquier tipo de se√±al o serie de tiempo.</li>
                        </ul>
                        <h4>Desventajas</h4>
                        <ul>
                            <li>Puede perder informaci√≥n contextual que un espectrograma 2D s√≠ capturar√≠a.</li>
                            <li>Depende fuertemente de la calidad de las caracter√≠sticas extra√≠das manualmente.</li>
                        </ul>
                    </div>
                    <div class="cnn-audio-sample">
                        <h4>Muestra de Audio de Entrada (Disgusto)</h4>
                        <p>Este es un ejemplo del tipo de audio que procesa el modelo antes de la extracci√≥n de caracter√≠sticas.</p>
                        <audio controls src="presentation/src/audio/03-01-07-02-02-02-11.wav">
                            Tu navegador no soporta el elemento de audio.
                        </audio>
                    </div>
                </div>
            </div>
        </div>

        <!-- NUEVAS DIAPOSITIVAS T√âCNICAS DETALLADAS -->
        <div class="slide hidden" data-slide="24">
            <h2 class="technical-title">An√°lisis T√©cnico de Kernels en Convoluci√≥n 1D</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Kernel Size Seleccionado: 5</h4>
                    <p>En nuestro modelo utilizamos un <strong>kernel de tama√±o 5</strong> para ambas capas convolucionales. Esta decisi√≥n se basa en capturar patrones locales significativos en la secuencia de caracter√≠sticas.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Operaci√≥n de Convoluci√≥n 1D</span>
                        <p>$(f * g)[n] = \sum_{m=-\infty}^{\infty} f[m] \cdot g[n-m]$</p>
                        <p>Para un kernel de tama√±o $K=5$:</p>
                        <p>$y[i] = \sum_{j=0}^{4} x[i+j] \cdot w[j] + b$</p>
                        <ul>
                            <li>$x[i]$: Entrada en la posici√≥n $i$</li>
                            <li>$w[j]$: Peso del kernel en la posici√≥n $j$</li>
                            <li>$b$: Bias del filtro</li>
                            <li>$y[i]$: Salida activada en la posici√≥n $i$</li>
                        </ul>
                    </div>

                    <h4>¬øPor qu√© Kernel Size = 5?</h4>
                    <ul>
                        <li><strong>Equilibrio:</strong> Captura patrones locales sin ser demasiado espec√≠fico</li>
                        <li><strong>Contexto Suficiente:</strong> Analiza 5 caracter√≠sticas consecutivas simult√°neamente</li>
                        <li><strong>Eficiencia Computacional:</strong> Menor que kernels grandes, pero m√°s expresivo que kernels peque√±os</li>
                    </ul>
                </div>
                
                <div class="step-visualization">
                    <div class="kernel-types-table">
                        <h4>Tipos de Kernels Disponibles</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Tama√±o</th>
                                    <th>Caracter√≠sticas</th>
                                    <th>Uso T√≠pico</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>1</strong></td>
                                    <td>Punto √∫nico, sin contexto</td>
                                    <td>Transformaci√≥n puntual</td>
                                </tr>
                                <tr>
                                    <td><strong>3</strong></td>
                                    <td>Patrones muy locales</td>
                                    <td>Detecci√≥n de bordes simples</td>
                                </tr>
                                <tr style="background-color: var(--color-bg-subtle);">
                                    <td><strong>5</strong></td>
                                    <td><strong>Balance contexto/especificidad</strong></td>
                                    <td><strong>Audio, series temporales</strong></td>
                                </tr>
                                <tr>
                                    <td><strong>7-11</strong></td>
                                    <td>Patrones medianos</td>
                                    <td>Estructuras m√°s complejas</td>
                                </tr>
                                <tr>
                                    <td><strong>15+</strong></td>
                                    <td>Patrones globales</td>
                                    <td>Tendencias de largo plazo</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="25">
            <h2 class="technical-title">C√°lculos Detallados de Convoluci√≥n en Nuestra Arquitectura</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Primera Capa Convolucional: Conv1D(256, 5)</h4>
                    <p>Entrada: <code>(batch_size, 180, 1)</code> ‚Üí Salida: <code>(batch_size, 180, 256)</code></p>
                    
                    <div class="formula-block">
                        <span class="formula-title">C√°lculo de Dimensiones de Salida</span>
                        <p>$L_{out} = \frac{L_{in} + 2 \times padding - kernel\_size}{stride} + 1$</p>
                        <p>Con <code>padding='same'</code>:</p>
                        <p>$L_{out} = L_{in} = 180$</p>
                        <p>N√∫mero de par√°metros por filtro: $(5 \times 1) + 1 = 6$ (5 pesos + 1 bias)</p>
                        <p>Total de par√°metros: $256 \times 6 = 1,536$</p>
                    </div>

                    <h4>Activaciones por Neurona</h4>
                    <p>Para cada uno de los 256 filtros, en cada posici√≥n $i$ de la secuencia:</p>
                    <div class="formula-block">
                        <span class="formula-title">Funci√≥n de Activaci√≥n ReLU</span>
                        <p>$a_i^{(l)} = max(0, z_i^{(l)})$</p>
                        <p>donde $z_i^{(l)}$ es la suma ponderada pre-activaci√≥n:</p>
                        <p>$z_i^{(l)} = \sum_{j=0}^{4} w_j^{(l)} \cdot x_{i+j} + b^{(l)}$</p>
                        <ul>
                            <li>Se activa si $z_i^{(l)} > 0$, de lo contrario salida = 0</li>
                            <li>Total de activaciones por muestra: $180 \times 256 = 46,080$</li>
                        </ul>
                    </div>

                    <h4>Segunda Capa Convolucional: Conv1D(128, 5)</h4>
                    <p>Entrada: <code>(batch_size, 36, 256)</code> ‚Üí Salida: <code>(batch_size, 36, 128)</code></p>
                    <div class="formula-block">
                        <span class="formula-title">Par√°metros de la Segunda Capa</span>
                        <p>Cada filtro conecta a los 256 canales de entrada:</p>
                        <p>Par√°metros por filtro: $(5 \times 256) + 1 = 1,281$</p>
                        <p>Total de par√°metros: $128 \times 1,281 = 163,968$</p>
                        <p>Activaciones por muestra: $36 \times 128 = 4,608$</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="26">
            <h2 class="technical-title">MaxPooling 1D: An√°lisis Matem√°tico Detallado</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Operaci√≥n MaxPooling con pool_size=5</h4>
                    <p>El MaxPooling reduce la dimensionalidad tomando el valor m√°ximo de cada ventana deslizante.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Funci√≥n MaxPooling Matem√°tica</span>
                        <p>$y[i] = \max_{j \in W_i} x[j]$</p>
                        <p>donde $W_i = \{i \times s, i \times s + 1, ..., i \times s + p - 1\}$</p>
                        <ul>
                            <li>$s$: stride (por defecto = pool_size = 5)</li>
                            <li>$p$: pool_size = 5</li>
                            <li>$x[j]$: valor de entrada en la posici√≥n $j$</li>
                        </ul>
                    </div>

                    <h4>C√°lculo de Dimensiones</h4>
                    <div class="formula-block">
                        <span class="formula-title">Reducci√≥n Dimensional</span>
                        <p>$L_{out} = \lfloor \frac{L_{in} - pool\_size}{stride} \rfloor + 1$</p>
                        <p><strong>Primer MaxPooling:</strong></p>
                        <p>$L_{out} = \lfloor \frac{180 - 5}{5} \rfloor + 1 = 36$</p>
                        <p><strong>Segundo MaxPooling:</strong></p>
                        <p>$L_{out} = \lfloor \frac{36 - 5}{5} \rfloor + 1 = 7$</p>
                    </div>
                </div>
                
                <div class="step-visualization">
                    <div class="maxpool-details">
                        <h4>Efectos del MaxPooling</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Aspecto</th>
                                    <th>Antes</th>
                                    <th>Despu√©s</th>
                                    <th>Efecto</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Dim. Temporal</strong></td>
                                    <td>180 ‚Üí 36</td>
                                    <td>36 ‚Üí 7</td>
                                    <td>Compresi√≥n 5:1</td>
                                </tr>
                                <tr>
                                    <td><strong>Par√°metros</strong></td>
                                    <td>0</td>
                                    <td>0</td>
                                    <td>Sin par√°metros</td>
                                </tr>
                                <tr>
                                    <td><strong>Invarianza</strong></td>
                                    <td>Baja</td>
                                    <td>Alta</td>
                                    <td>Robustez a ruido</td>
                                </tr>
                                <tr>
                                    <td><strong>Informaci√≥n</strong></td>
                                    <td>Completa</td>
                                    <td>Comprimida</td>
                                    <td>Preserva caracter√≠sticas dominantes</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <div class="maxpool-benefit" style="margin-top: 20px; background: var(--color-bg-subtle); padding: 20px; border-radius: 15px;">
                            <h4 style="color: var(--color-primary);">Beneficios del MaxPooling</h4>
                            <ul>
                                <li><strong>Invarianza Traslacional:</strong> El modelo se vuelve menos sensible a peque√±os desplazamientos</li>
                                <li><strong>Reducci√≥n de Overfitting:</strong> Menos par√°metros implica menor riesgo de sobreajuste</li>
                                <li><strong>Eficiencia Computacional:</strong> Reduce el costo computacional de capas posteriores</li>
                                <li><strong>Extracci√≥n de Caracter√≠sticas Dominantes:</strong> Preserva las activaciones m√°s fuertes</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="27">
            <h2 class="technical-title">Optimizador ADAM: Fundamento Matem√°tico Completo</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øQu√© es ADAM y por qu√© es Superior?</h4>
                    <p><strong>ADAM (Adaptive Moment Estimation)</strong> combina las ventajas de AdaGrad (adaptaci√≥n por caracter√≠stica) y RMSprop (promedio m√≥vil) con correcci√≥n de bias para momentos iniciales.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Algoritmo ADAM Paso a Paso</span>
                        <p><strong>Paso 1: Inicializaci√≥n</strong></p>
                        <p>$m_0 = 0, \quad v_0 = 0, \quad t = 0$</p>
                        
                        <p><strong>Paso 2: En cada iteraci√≥n $t$:</strong></p>
                        <p>$t = t + 1$</p>
                        <p>$g_t = \nabla_{\theta} J(\theta_{t-1})$ <em>(Gradiente de la funci√≥n de p√©rdida)</em></p>
                        
                        <p><strong>Paso 3: Actualizaci√≥n de momentos</strong></p>
                        <p>$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$ <em>(Momento de primer orden)</em></p>
                        <p>$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$ <em>(Momento de segundo orden)</em></p>
                        
                        <p><strong>Paso 4: Correcci√≥n de bias</strong></p>
                        <p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
                        
                        <p><strong>Paso 5: Actualizaci√≥n de par√°metros</strong></p>
                        <p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t$</p>
                    </div>

                    <div class="adam-params">
                        <h4>Hiperpar√°metros en Nuestro Modelo</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Par√°metro</th>
                                    <th>Valor</th>
                                    <th>Prop√≥sito</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>$\alpha$ (learning rate)</td>
                                    <td>0.001</td>
                                    <td>Controla el tama√±o del paso</td>
                                </tr>
                                <tr>
                                    <td>$\beta_1$</td>
                                    <td>0.9</td>
                                    <td>Factor de decaimiento para momento de 1er orden</td>
                                </tr>
                                <tr>
                                    <td>$\beta_2$</td>
                                    <td>0.999</td>
                                    <td>Factor de decaimiento para momento de 2do orden</td>
                                </tr>
                                <tr>
                                    <td>$\epsilon$</td>
                                    <td>1e-7</td>
                                    <td>Previene divisi√≥n por cero</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="28">
            <h2 class="technical-title">ADAM vs. Otros Optimizadores: An√°lisis Comparativo</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øPor qu√© ADAM para Nuestro Problema?</h4>
                    
                    <div class="optimizer-comparison">
                        <h4>Ventajas Espec√≠ficas de ADAM</h4>
                        <ul>
                            <li><strong>Adaptaci√≥n Autom√°tica:</strong> Ajusta el learning rate por par√°metro individualmente</li>
                            <li><strong>Memoria de Gradientes:</strong> Utiliza informaci√≥n de gradientes pasados ($m_t$)</li>
                            <li><strong>Escalado por Varianza:</strong> Normaliza por la varianza hist√≥rica de gradientes ($v_t$)</li>
                            <li><strong>Correcci√≥n de Bias:</strong> Compensa la inicializaci√≥n en cero de los momentos</li>
                            <li><strong>Robustez:</strong> Funciona bien en una amplia gama de problemas sin ajuste fino</li>
                        </ul>
                    </div>

                    <div class="formula-block">
                        <span class="formula-title">Intuici√≥n Matem√°tica</span>
                        <p>El factor de actualizaci√≥n final en ADAM:</p>
                        <p>$\frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t$</p>
                        <ul>
                            <li><strong>$\hat{m}_t$:</strong> Direcci√≥n del gradiente suavizada</li>
                            <li><strong>$\sqrt{\hat{v}_t}$:</strong> Normalizaci√≥n adaptativa por par√°metro</li>
                            <li><strong>Resultado:</strong> Pasos grandes para gradientes consistentes, pasos peque√±os para gradientes variables</li>
                        </ul>
                    </div>
                </div>
                
                <div class="step-visualization">
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Optimizador</th>
                                <th>F√≥rmula de Actualizaci√≥n</th>
                                <th>Ventajas</th>
                                <th>Desventajas</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>SGD</strong></td>
                                <td>$\theta_t = \theta_{t-1} - \alpha \cdot g_t$</td>
                                <td>Simple, convergencia te√≥rica</td>
                                <td>Lento, sensible a lr</td>
                            </tr>
                            <tr>
                                <td><strong>Momentum</strong></td>
                                <td>$\theta_t = \theta_{t-1} - \alpha \cdot m_t$</td>
                                <td>Acelera convergencia</td>
                                <td>Un hiperpar√°metro m√°s</td>
                            </tr>
                            <tr>
                                <td><strong>AdaGrad</strong></td>
                                <td>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{G_t}} \cdot g_t$</td>
                                <td>Adaptativo por caracter√≠stica</td>
                                <td>Learning rate decae muy r√°pido</td>
                            </tr>
                            <tr>
                                <td><strong>RMSprop</strong></td>
                                <td>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{v_t}} \cdot g_t$</td>
                                <td>Mejora AdaGrad</td>
                                <td>Sin correcci√≥n de bias</td>
                            </tr>
                            <tr style="background-color: var(--color-bg-subtle);">
                                <td><strong>ADAM</strong></td>
                                <td>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t$</td>
                                <td><strong>Combina momentum + adaptativo + correcci√≥n bias</strong></td>
                                <td>M√°s complejo computacionalmente</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="29">
            <h2 class="technical-title">An√°lisis Completo de Activaciones por Capa</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Desglose Neuronal Detallado de Nuestra Arquitectura</h4>
                    
                    <div class="neuron-analysis">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Capa</th>
                                    <th>Dimensiones</th>
                                    <th>Neuronas Totales</th>
                                    <th>Neuronas Activas</th>
                                    <th>Funci√≥n Activaci√≥n</th>
                                    <th>Par√°metros</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Input</strong></td>
                                    <td>(None, 180, 1)</td>
                                    <td>180</td>
                                    <td>180 (100%)</td>
                                    <td>Linear</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Conv1D_1</strong></td>
                                    <td>(None, 180, 256)</td>
                                    <td>46,080</td>
                                    <td>~30,000-35,000 (65-75%)</td>
                                    <td>ReLU</td>
                                    <td>1,536</td>
                                </tr>
                                <tr>
                                    <td><strong>MaxPool1D_1</strong></td>
                                    <td>(None, 36, 256)</td>
                                    <td>9,216</td>
                                    <td>9,216 (100%)</td>
                                    <td>Max</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Dropout_1</strong></td>
                                    <td>(None, 36, 256)</td>
                                    <td>9,216</td>
                                    <td>~7,373 (80%)</td>
                                    <td>Dropout</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Conv1D_2</strong></td>
                                    <td>(None, 36, 128)</td>
                                    <td>4,608</td>
                                    <td>~2,760-3,456 (60-75%)</td>
                                    <td>ReLU</td>
                                    <td>163,968</td>
                                </tr>
                                <tr>
                                    <td><strong>MaxPool1D_2</strong></td>
                                    <td>(None, 7, 128)</td>
                                    <td>896</td>
                                    <td>896 (100%)</td>
                                    <td>Max</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Dropout_2</strong></td>
                                    <td>(None, 7, 128)</td>
                                    <td>896</td>
                                    <td>~717 (80%)</td>
                                    <td>Dropout</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Flatten</strong></td>
                                    <td>(None, 896)</td>
                                    <td>896</td>
                                    <td>896 (100%)</td>
                                    <td>Linear</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td><strong>Dense (Output)</strong></td>
                                    <td>(None, 7)</td>
                                    <td>7</td>
                                    <td>7 (100%)</td>
                                    <td>Softmax</td>
                                    <td>6,279</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="formula-block">
                        <span class="formula-title">An√°lisis de Tasa de Activaci√≥n ReLU</span>
                        <p>Para ReLU: $f(x) = max(0, x)$</p>
                        <p><strong>Porcentaje de Activaci√≥n Esperado:</strong></p>
                        <p>En condiciones normales (distribuci√≥n aproximadamente normal): ~50-75%</p>
                        <p><strong>En nuestro modelo:</strong></p>
                        <ul>
                            <li>Conv1D_1: ~65-75% debido a entrada estandarizada</li>
                            <li>Conv1D_2: ~60-75% tras MaxPooling (valores m√°s dispersos)</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="30">
            <h2 class="technical-title">Funci√≥n Softmax: Matem√°tica de la Clasificaci√≥n Final</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øC√≥mo Convierte Puntuaciones en Probabilidades?</h4>
                    <p>La funci√≥n Softmax transforma un vector de puntuaciones reales en una distribuci√≥n de probabilidad v√°lida.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Funci√≥n Softmax Matem√°tica</span>
                        <p>Para un vector $\mathbf{z} = [z_1, z_2, ..., z_K]$ donde $K=7$ (nuestras emociones):</p>
                        <p>$\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$</p>
                        <p>donde $i = 1, 2, ..., 7$ representa cada emoci√≥n</p>
                        
                        <p><strong>Propiedades importantes:</strong></p>
                        <ul>
                            <li>$0 \leq \sigma(\mathbf{z})_i \leq 1$ para todo $i$</li>
                            <li>$\sum_{i=1}^{7} \sigma(\mathbf{z})_i = 1$</li>
                            <li>Amplifica diferencias entre valores grandes y peque√±os</li>
                        </ul>
                    </div>

                    <h4>Ejemplo Pr√°ctico de Nuestro Modelo</h4>
                    <div class="formula-block">
                        <span class="formula-title">Caso Real de Clasificaci√≥n</span>
                        <p><strong>Vector pre-softmax (logits):</strong></p>
                        <p>$\mathbf{z} = [2.1, -0.5, 3.2, 0.8, -1.2, 1.5, 0.3]$</p>
                        
                        <p><strong>Aplicando Softmax:</strong></p>
                        <p>$e^{z_1} = e^{2.1} = 8.17$, $e^{z_2} = e^{-0.5} = 0.61$, $e^{z_3} = e^{3.2} = 24.53$, ...</p>
                        <p>$\sum e^{z_j} = 8.17 + 0.61 + 24.53 + 2.23 + 0.30 + 4.48 + 1.35 = 41.67$</p>
                        
                        <p><strong>Probabilidades finales:</strong></p>
                        <p>Happy: $\frac{24.53}{41.67} = 0.589$ (58.9%)</p>
                        <p>Neutral: $\frac{8.17}{41.67} = 0.196$ (19.6%)</p>
                        <p>Angry: $\frac{4.48}{41.67} = 0.107$ (10.7%)</p>
                        <p>... (otras emociones con probabilidades menores)</p>
                    </div>
                </div>
                
                <div class="step-visualization">
                    <div class="softmax-properties">
                        <h4>Caracter√≠sticas de Softmax</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Propiedad</th>
                                    <th>Descripci√≥n</th>
                                    <th>Importancia</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Normalizaci√≥n</strong></td>
                                    <td>Suma total = 1</td>
                                    <td>Distribuci√≥n de probabilidad v√°lida</td>
                                </tr>
                                <tr>
                                    <td><strong>Amplificaci√≥n</strong></td>
                                    <td>Exponenciaci√≥n de logits</td>
                                    <td>Enfatiza la clase m√°s probable</td>
                                </tr>
                                <tr>
                                    <td><strong>Diferenciabilidad</strong></td>
                                    <td>Funci√≥n suave y continua</td>
                                    <td>Permite backpropagation</td>
                                </tr>
                                <tr>
                                    <td><strong>Estabilidad</strong></td>
                                    <td>Invariante a traslaciones</td>
                                    <td>Previene overflow num√©rico</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <div class="gradient-info">
                            <h4>Gradiente de Softmax</h4>
                            <p>Durante backpropagation, el gradiente de Softmax es:</p>
                            <div class="formula-block">
                                <p>$\frac{\partial \sigma_i}{\partial z_j} = \sigma_i(\delta_{ij} - \sigma_j)$</p>
                                <p>donde $\delta_{ij}$ es la delta de Kronecker</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="31">
            <h2 class="technical-title">Backpropagation en CNN 1D: Flujo de Gradientes</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>¬øC√≥mo Aprende Nuestro Modelo?</h4>
                    <p>El backpropagation calcula los gradientes de la funci√≥n de p√©rdida respecto a cada par√°metro, propag√°ndose desde la salida hacia la entrada.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Funci√≥n de P√©rdida: Categorical Crossentropy</span>
                        <p>$\mathcal{L} = -\sum_{i=1}^{7} y_i \log(\hat{y}_i)$</p>
                        <ul>
                            <li>$y_i$: Etiqueta verdadera (one-hot encoded)</li>
                            <li>$\hat{y}_i$: Probabilidad predicha para la clase $i$</li>
                            <li>Solo contribuye la clase verdadera ($y_i = 1$ para una clase, $0$ para las dem√°s)</li>
                        </ul>
                    </div>

                    <div class="backprop-flow">
                        <h4>Flujo de Gradientes en Nuestra Arquitectura</h4>
                        
                        <div class="gradient-step">
                            <h5>1. Capa Dense (Softmax)</h5>
                            <div class="formula-block">
                                <p>$\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i - y_i$</p>
                                <p>Gradiente respecto a pesos: $\frac{\partial \mathcal{L}}{\partial W_{ij}} = (\hat{y}_i - y_i) \cdot a_j^{(flatten)}$</p>
                            </div>
                        </div>

                        <div class="gradient-step">
                            <h5>2. Flatten (Sin Par√°metros)</h5>
                            <p>Solo reshaping: $\frac{\partial \mathcal{L}}{\partial a^{(conv2)}} = reshape(\frac{\partial \mathcal{L}}{\partial a^{(flatten)}})$</p>
                        </div>

                        <div class="gradient-step">
                            <h5>3. Conv1D Segunda Capa</h5>
                            <div class="formula-block">
                                <p>Para cada filtro $f$ y posici√≥n $i$:</p>
                                <p>$\frac{\partial \mathcal{L}}{\partial w_f} = \sum_{i} \frac{\partial \mathcal{L}}{\partial z_i^f} \cdot a_{i}^{(pool1)}$</p>
                                <p>donde $z_i^f$ es la salida pre-activaci√≥n del filtro $f$ en posici√≥n $i$</p>
                            </div>
                        </div>

                        <div class="gradient-step">
                            <h5>4. MaxPooling (Operaci√≥n de Selecci√≥n)</h5>
                            <div class="formula-block">
                                <p>$\frac{\partial \mathcal{L}}{\partial a_j^{(conv1)}} = \begin{cases} 
                                \frac{\partial \mathcal{L}}{\partial a_i^{(pool)}} & \text{si } a_j^{(conv1)} = \max(\text{ventana}) \\
                                0 & \text{en caso contrario}
                                \end{cases}$</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="32">
            <h2 class="technical-title">An√°lisis de Dropout: Regularizaci√≥n Estoc√°stica</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Dropout Rate = 0.2: ¬øQu√© Significa Exactamente?</h4>
                    <p>Durante el entrenamiento, cada neurona tiene una probabilidad $p = 0.2$ de ser "desactivada" (puesta a cero).</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Matem√°tica del Dropout</span>
                        <p><strong>Durante Entrenamiento:</strong></p>
                        <p>$y_i = \begin{cases} 
                        0 & \text{con probabilidad } p = 0.2 \\
                        \frac{x_i}{1-p} & \text{con probabilidad } 1-p = 0.8
                        \end{cases}$</p>
                        
                        <p><strong>Durante Inferencia:</strong></p>
                        <p>$y_i = x_i$ (sin dropout, pero ya escalado durante entrenamiento)</p>
                        
                        <ul>
                            <li>$x_i$: Activaci√≥n de entrada de la neurona $i$</li>
                            <li>$y_i$: Activaci√≥n de salida tras dropout</li>
                            <li>Factor $\frac{1}{1-p} = 1.25$: Compensa las neuronas desactivadas</li>
                        </ul>
                    </div>

                    <h4>Impacto en Nuestras Capas</h4>
                    <div class="dropout-impact">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Capa</th>
                                    <th>Neuronas Totales</th>
                                    <th>Activas (Entrenamiento)</th>
                                    <th>Desactivadas</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Despu√©s Conv1D_1</td>
                                    <td>9,216</td>
                                    <td>7,373 (80%)</td>
                                    <td>1,843 (20%)</td>
                                </tr>
                                <tr>
                                    <td>Despu√©s Conv1D_2</td>
                                    <td>896</td>
                                    <td>717 (80%)</td>
                                    <td>179 (20%)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
                
                <div class="step-visualization">
                    <div class="dropout-benefits">
                        <h4>¬øPor qu√© Dropout Funciona?</h4>
                        
                        <div class="benefit-item">
                            <h5>1. Prevenci√≥n de Co-adaptaci√≥n</h5>
                            <p>Las neuronas no pueden depender unas de otras, deben ser √∫tiles independientemente.</p>
                        </div>
                        
                        <div class="benefit-item">
                            <h5>2. Ensemble Impl√≠cito</h5>
                            <p>Cada pasada hacia adelante usa una sub-red diferente, promediando m√∫ltiples modelos.</p>
                        </div>
                        
                        <div class="benefit-item">
                            <h5>3. Reducci√≥n de Varianza</h5>
                            <p>Fuerza al modelo a ser m√°s robusto y generalizar mejor.</p>
                        </div>

                        <div class="formula-block">
                            <span class="formula-title">N√∫mero de Sub-redes Posibles</span>
                            <p>Con $n$ neuronas y dropout rate $p$:</p>
                            <p>$\text{Sub-redes} = 2^n$</p>
                            <p>Para nuestra primera capa: $2^{9216} \approx 10^{2774}$ configuraciones posibles</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="33">
            <h2>19. Arquitectura del Modelo CNN 1D</h2>
            <p>Esta es la arquitectura de Keras que implementamos. Keras es una librer√≠a de alto nivel que facilita la construcci√≥n de redes neuronales. La red est√° dise√±ada para procesar la secuencia de 180 caracter√≠sticas extra√≠das de cada audio y aprender a clasificar la emoci√≥n.</p>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 180, 1)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 180</p>
                        <p class="explanation">El punto de partida. Cada audio se representa como una secuencia de 180 valores num√©ricos. El `None` indica un tama√±o de lote flexible.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (256 filtros) + ReLU</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 180, 256)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 46,080</p>
                        <p class="explanation">256 filtros (detectores de patrones) se deslizan sobre la secuencia de entrada. Cada filtro genera un "mapa de caracter√≠sticas" de 180 puntos, resultando en 256 mapas. La activaci√≥n ReLU introduce no-linealidad.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D (`pool_size=5`)</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 36, 256)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 9,216</p>
                        <p class="explanation">Reduce la longitud de cada mapa de caracter√≠sticas de 180 a 36 (180/5). Se toma el valor m√°ximo de cada ventana de 5 elementos, compactando la informaci√≥n y haciendo el modelo m√°s robusto a peque√±as traslaciones.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dropout">
                    <h4>Dropout (0.2)</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 36, 256)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 9,216 (20% desactivadas)</p>
                        <p class="explanation">Durante el entrenamiento, desactiva aleatoriamente el 20% de las conexiones de entrada a esta capa. Esto previene el sobreajuste al forzar a la red a aprender patrones m√°s diversos y redundantes.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv1D (128 filtros) + ReLU</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 36, 128)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 4,608</p>
                        <p class="explanation">Similar a la primera Conv1D, pero ahora con 128 filtros que operan sobre los 256 mapas de caracter√≠sticas anteriores. Esto permite detectar patrones de patrones, es decir, caracter√≠sticas de mayor nivel de abstracci√≥n.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D (`pool_size=5`)</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 7, 128)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 896</p>
                        <p class="explanation">Reduce la longitud de la secuencia de 36 a 7 (36/5, redondeado). Contin√∫a la compactaci√≥n de informaci√≥n, manteniendo solo los picos de activaci√≥n m√°s significativos.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dropout">
                    <h4>Dropout (0.2)</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 7, 128)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 896 (20% desactivadas)</p>
                        <p class="explanation">Otra capa de regularizaci√≥n para mejorar la capacidad de generalizaci√≥n del modelo, evitando que se vuelva demasiado dependiente de caracter√≠sticas espec√≠ficas.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dense">
                    <h4>Flatten</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 896)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 896</p>
                        <p class="explanation">Transforma la salida multidimensional (7 secuencias de 128 caracter√≠sticas) en un √∫nico vector plano de 896 neuronas. Esto es necesario para conectar con las capas densas tradicionales.</p>
                    </div>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box output">
                    <h4>Capa Densa (Softmax)</h4>
                    <div class="layer-info">
                        <p><strong>Forma de Salida:</strong> <span class="output-shape">`(None, 7)`</span></p>
                        <p><strong>Cantidad de Neuronas:</strong> 7</p>
                        <p class="explanation">La capa de salida final. Contiene 7 neuronas, una por cada emoci√≥n a clasificar. La funci√≥n Softmax convierte las puntuaciones en probabilidades, indicando la confianza del modelo en cada emoci√≥n.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="34">
            <h2>20. Resumen de Neuronas y Par√°metros</h2>
            <div class="cnn-details">
                <p>
                    Esta tabla detalla la estructura de nuestro modelo. La <strong>Forma de Salida</strong> muestra c√≥mo cambian las dimensiones de los datos despu√©s de cada capa. Los <strong>Par√°metros</strong> son los pesos o "conocimientos" que el modelo aprende durante el entrenamiento. En total, nuestro modelo debe aprender y ajustar **171,783 par√°metros** para poder realizar la clasificaci√≥n.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa (Tipo)</th>
                            <th>Forma de Salida</th>
                            <th>Cantidad de Neuronas</th>
                            <th>Par√°metros</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>conv1d</td>
                            <td>(None, 180, 256)</td>
                            <td>46,080</td>
                            <td>1,536</td>
                        </tr>
                        <tr>
                            <td>max_pooling1d</td>
                            <td>(None, 36, 256)</td>
                            <td>9,216</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout</td>
                            <td>(None, 36, 256)</td>
                            <td>9,216 (20% desactivadas)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>conv1d_1</td>
                            <td>(None, 36, 128)</td>
                            <td>4,608</td>
                            <td>163,968</td>
                        </tr>
                         <tr>
                            <td>max_pooling1d_1</td>
                            <td>(None, 7, 128)</td>
                            <td>896</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout_1</td>
                            <td>(None, 7, 128)</td>
                            <td>896 (20% desactivadas)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>flatten</td>
                            <td>(None, 896)</td>
                            <td>896</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dense</td>
                            <td>(None, 7)</td>
                            <td>7</td>
                            <td>6,279</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="3"><strong>Total de Par√°metros Entrenables</strong></td>
                            <td><strong>171,783</strong></td>
                        </tr>
                    </tbody>
                </table>
                 <footer class="cnn-table-footnote">
                    <strong>Par√°metros:</strong> Son los "conocimientos" internos de la red que se ajustan durante el entrenamiento.
                </footer>
            </div>
        </div>

        <div class="slide hidden" data-slide="35">
            <h2>21. Recursos</h2>
            <div class="dataset-card">
                <h3>Lenguaje y Frameworks</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal</li>
                    <li><strong>TensorFlow/Keras:</strong> Desarrollo de CNN 1D</li>
                    <li><strong>Librosa:</strong> Extracci√≥n de caracter√≠sticas de audio</li>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y m√©tricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Hardware y Plataformas</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU.</li>
                    <li><strong>GitHub:</strong> Control de versiones.</li>
                </ul>
            </div>
        </div>

        <div class="slide hidden" data-slide="36">
            <h2>22. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo CNN 1D para clasificar 7 emociones.</li>
                        <li>Pipeline de extracci√≥n de 180 caracter√≠sticas.</li>
                        <li>Comparaci√≥n visual 3D de PCA y LDA.</li>
                        <li>M√©tricas de rendimiento del modelo.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>An√°lisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicaci√≥n de usuario final.</li>
                        <li>El modelo no opera en tiempo real.</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="37">
            <h2 class="technical-title">M√©tricas de Evaluaci√≥n y Resultados Esperados</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>M√©tricas de Clasificaci√≥n Multiclase</h4>
                    <p>Para evaluar nuestro modelo de 7 clases, utilizamos m√∫ltiples m√©tricas complementarias.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Accuracy (Precisi√≥n Global)</span>
                        <p>$\text{Accuracy} = \frac{\text{Predicciones Correctas}}{\text{Total de Predicciones}} = \frac{TP + TN}{TP + TN + FP + FN}$</p>
                        
                        <span class="formula-title">Precision por Clase</span>
                        <p>$\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}$</p>
                        
                        <span class="formula-title">Recall por Clase</span>
                        <p>$\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}$</p>
                        
                        <span class="formula-title">F1-Score</span>
                        <p>$F1_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}$</p>
                    </div>

                    <h4>Resultados Esperados</h4>
                    <ul>
                        <li><strong>Accuracy objetivo:</strong> 75-85% en datos de prueba</li>
                        <li><strong>Emociones m√°s f√°ciles:</strong> Happy, Sad (F1 > 0.80)</li>
                        <li><strong>Emociones m√°s dif√≠ciles:</strong> Disgust, Fear (F1 ~ 0.65-0.75)</li>
                        <li><strong>Convergencia:</strong> 20-50 √©pocas con early stopping</li>
                    </ul>
                </div>
                
                <div class="step-visualization">
                    <div class="confusion-matrix-theory">
                        <h4>Matriz de Confusi√≥n Esperada</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Real\Pred</th>
                                    <th>Happy</th>
                                    <th>Sad</th>
                                    <th>Angry</th>
                                    <th>Fear</th>
                                    <th>Surprise</th>
                                    <th>Disgust</th>
                                    <th>Neutral</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Happy</strong></td>
                                    <td style="background-color: var(--color-success); color: white;"><strong>85%</strong></td>
                                    <td>3%</td>
                                    <td>2%</td>
                                    <td>1%</td>
                                    <td>5%</td>
                                    <td>1%</td>
                                    <td>3%</td>
                                </tr>
                                <tr>
                                    <td><strong>Sad</strong></td>
                                    <td>2%</td>
                                    <td style="background-color: var(--color-success); color: white;"><strong>80%</strong></td>
                                    <td>4%</td>
                                    <td>6%</td>
                                    <td>1%</td>
                                    <td>2%</td>
                                    <td>5%</td>
                                </tr>
                                <tr>
                                    <td><strong>Angry</strong></td>
                                    <td>4%</td>
                                    <td>5%</td>
                                    <td style="background-color: var(--color-success); color: white;"><strong>75%</strong></td>
                                    <td>8%</td>
                                    <td>2%</td>
                                    <td>4%</td>
                                    <td>2%</td>
                                </tr>
                                <tr>
                                    <td colspan="8" style="text-align: center; font-style: italic;">... (patrones similares para otras emociones)</td>
                                </tr>
                            </tbody>
                        </table>
                        <p style="font-size: 0.9rem; margin-top: 10px;"><strong>Interpretaci√≥n:</strong> Los valores diagonales (en verde) representan clasificaciones correctas. Los valores fuera de la diagonal indican confusiones entre emociones.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="38">
            <h2 class="technical-title">Optimizaci√≥n de Hiperpar√°metros</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Estrategia de B√∫squeda de Hiperpar√°metros</h4>
                    <p>Los hiperpar√°metros cr√≠ticos a optimizar en nuestro modelo incluyen arquitectura, regularizaci√≥n y entrenamiento.</p>
                    
                    <div class="hyperparameter-grid">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Categor√≠a</th>
                                    <th>Hiperpar√°metro</th>
                                    <th>Rango de B√∫squeda</th>
                                    <th>Valor Seleccionado</th>
                                    <th>Justificaci√≥n</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td rowspan="3"><strong>Arquitectura</strong></td>
                                    <td>Filtros Conv1D_1</td>
                                    <td>[128, 256, 512]</td>
                                    <td><strong>256</strong></td>
                                    <td>Balance capacidad/eficiencia</td>
                                </tr>
                                <tr>
                                    <td>Filtros Conv1D_2</td>
                                    <td>[64, 128, 256]</td>
                                    <td><strong>128</strong></td>
                                    <td>Reducci√≥n progresiva</td>
                                </tr>
                                <tr>
                                    <td>Kernel Size</td>
                                    <td>[3, 5, 7]</td>
                                    <td><strong>5</strong></td>
                                    <td>Contexto local √≥ptimo</td>
                                </tr>
                                <tr>
                                    <td rowspan="2"><strong>Regularizaci√≥n</strong></td>
                                    <td>Dropout Rate</td>
                                    <td>[0.1, 0.2, 0.3, 0.5]</td>
                                    <td><strong>0.2</strong></td>
                                    <td>Previene overfitting sin perder capacidad</td>
                                </tr>
                                <tr>
                                    <td>Pool Size</td>
                                    <td>[3, 5, 7]</td>
                                    <td><strong>5</strong></td>
                                    <td>Reduce dimensionalidad adecuadamente</td>
                                </tr>
                                <tr>
                                    <td rowspan="3"><strong>Entrenamiento</strong></td>
                                    <td>Learning Rate</td>
                                    <td>[1e-4, 1e-3, 1e-2]</td>
                                    <td><strong>1e-3</strong></td>
                                    <td>Convergencia estable con ADAM</td>
                                </tr>
                                <tr>
                                    <td>Batch Size</td>
                                    <td>[32, 64, 128]</td>
                                    <td><strong>64</strong></td>
                                    <td>Balance memoria/estabilidad gradientes</td>
                                </tr>
                                <tr>
                                    <td>Early Stopping Patience</td>
                                    <td>[5, 10, 15]</td>
                                    <td><strong>10</strong></td>
                                    <td>Permite convergencia sin overfitting</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="formula-block">
                        <span class="formula-title">Validaci√≥n Cruzada para Hiperpar√°metros</span>
                        <p>Utilizamos validaci√≥n cruzada estratificada con $k=5$ folds:</p>
                        <p>$\text{Score}_{CV} = \frac{1}{k}\sum_{i=1}^{k} \text{Accuracy}_i$</p>
                        <p>donde cada fold mantiene la proporci√≥n de clases del dataset original.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="39">
            <h2 class="technical-title">Consideraciones de Implementaci√≥n y Escalabilidad</h2>
            <div class="technical-step-layout full-width">
                <div class="step-explanation">
                    <h4>Complejidad Computacional</h4>
                    <p>An√°lisis del costo computacional de nuestro modelo para entender sus limitaciones y posibilidades de escalamiento.</p>
                    
                    <div class="formula-block">
                        <span class="formula-title">Complejidad por Operaci√≥n</span>
                        <p><strong>Convoluci√≥n 1D:</strong></p>
                        <p>$O(L_{in} \times K \times C_{in} \times C_{out})$</p>
                        <p>Para Conv1D_1: $O(180 \times 5 \times 1 \times 256) = O(230,400)$</p>
                        <p>Para Conv1D_2: $O(36 \times 5 \times 256 \times 128) = O(23,040,000)$</p>
                        
                        <p><strong>MaxPooling 1D:</strong></p>
                        <p>$O(L_{in} \times C)$</p>
                        <p>MaxPool1D_1: $O(180 \times 256) = O(46,080)$</p>
                        
                        <p><strong>Dense Layer:</strong></p>
                        <p>$O(N_{in} \times N_{out})$</p>
                        <p>Capa final: $O(896 \times 7) = O(6,272)$</p>
                    </div>

                    <h4>Tiempo de Inferencia</h4>
                    <ul>
                        <li><strong>CPU (Intel i7):</strong> ~2-5ms por muestra</li>
                        <li><strong>GPU (Tesla V100):</strong> ~0.1-0.5ms por muestra</li>
                        <li><strong>Batch Processing:</strong> Eficiencia aumenta con batches de 32-128 muestras</li>
                    </ul>
                </div>
                
                <div class="step-visualization">
                    <div class="scalability-analysis">
                        <h4>Escalabilidad del Sistema</h4>
                        
                        <div class="scalability-item">
                            <h5>Memoria RAM Requerida</h5>
                            <table class="comparison-table">
                                <thead>
                                    <tr>
                                        <th>Componente</th>
                                        <th>Tama√±o (MB)</th>
                                        <th>Descripci√≥n</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Modelo (par√°metros)</td>
                                        <td>~0.7</td>
                                        <td>171,783 par√°metros √ó 4 bytes</td>
                                    </tr>
                                    <tr>
                                        <td>Activaciones (batch=64)</td>
                                        <td>~12</td>
                                        <td>Almacenamiento intermedio</td>
                                    </tr>
                                    <tr>
                                        <td>Gradientes (entrenamiento)</td>
                                        <td>~0.7</td>
                                        <td>Mismo tama√±o que par√°metros</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Total m√≠nimo</strong></td>
                                        <td><strong>~13.4</strong></td>
                                        <td>Para inferencia en batch</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="deployment-considerations">
                            <h5>Consideraciones para Despliegue</h5>
                            <ul>
                                <li><strong>Edge Computing:</strong> Modelo suficientemente peque√±o para dispositivos m√≥viles</li>
                                <li><strong>Cuantizaci√≥n:</strong> Reducir precisi√≥n de Float32 a Int8 (75% menos memoria)</li>
                                <li><strong>Pruning:</strong> Eliminar conexiones poco importantes (50% menos par√°metros)</li>
                                <li><strong>Distillation:</strong> Crear modelo m√°s peque√±o que imite el comportamiento</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide hidden" data-slide="40">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de caracter√≠sticas y clasificar emociones complejas en la voz.
            </p>
            <div class="highlight-box">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
                <p>Hemos cubierto desde los fundamentos matem√°ticos hasta la implementaci√≥n t√©cnica completa del modelo CNN 1D para reconocimiento de emociones en audio.</p>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="presentation/src/js/script.js"></script>
</body>
</html>
