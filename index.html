<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="presentation/src/css/styles.css">
    <!-- Soporte para LaTeX (MathJax) -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 34</div>

    <div class="presentation-container">
        <!-- Diapositivas 1-14 se mantienen intactas -->
        <div class="slide" data-slide="1">
            <h1>Análisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Acústicos para la Clasificación del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque técnico para la modelación de características prosódicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro Pérez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Gutiérrez</strong></p>
            </div>
        </div>
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificación Técnica</h2>
            <div class="highlight-box">
                <h3>¿Por qué Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma acústica compleja, rica en información latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Características Prosódicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribución de energía en las frecuencias (formantes) varía sistemáticamente con la emoción.</p></div>
                <div class="stat-item"><h3>Señal No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con técnicas de IA.</p></div>
            </div>
            <p><strong>¿Por qué Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracción de Patrones:</strong> Capacidad para identificar automáticamente características complejas en espectrogramas, indetectables para el análisis tradicional.</li>
                <li><strong>Análisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificación consistente y reproducible de las características vocales.</li>
                <li><strong>Modelado de Alta Dimensión:</strong> Habilidad para procesar miles de características extraídas de una sola señal de audio.</li>
            </ul>
        </div>
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripción del Problema</h2>
            <div class="highlight-box">
                <h3>Problema Técnico Principal</h3>
                <p>El desafío de clasificar estados emocionales a partir de la señal del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¿Qué reto técnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones acústicos para 7 emociones distintas: <strong>alegría, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabación.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la señal hasta la clasificación.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificación de emociones humanas a partir del análisis de características acústicas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la señal hasta la predicción del modelo.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Específicos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar múltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar características acústicas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Diseñar, entrenar y optimizar modelos especializados de aprendizaje automático</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando métricas estándar de clasificación</div>
            <div class="methodology-step"><strong>5.</strong> Implementar técnicas de reducción de dimensionalidad y visualización</div>
        </div>
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodología Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo cíclico, que nos permite refinar y mejorar continuamente nuestro modelo basándonos en los resultados obtenidos.</p>
            <div class="iterative-cycle-diagram">
                <div class="cycle-center-text">
                    Retroalimentación y Mejora Continua
                </div>
                <div class="cycle-step" style="--i:0;">
                    <h4>1. Adquisición de Datos</h4>
                </div>
                <div class="cycle-step" style="--i:1;">
                    <h4>2. Análisis y Preprocesamiento</h4>
                </div>
                <div class="cycle-step" style="--i:2;">
                    <h4>3. Extracción de Características</h4>
                </div>
                <div class="cycle-step" style="--i:3;">
                    <h4>4. Entrenamiento del Modelo</h4>
                </div>
                <div class="cycle-step" style="--i:4;">
                    <h4>5. Evaluación y Resultados</h4>
                </div>
            </div>
            <footer class="footnote" style="position: relative; border-top: none; text-align: center; margin-top: 20px;">
                Los resultados de la fase de <strong>Evaluación</strong> informan ajustes en las fases anteriores, creando un ciclo de refinamiento.
            </footer>
        </div>
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisición de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Características:</strong> Español mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptación específica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Características:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Características:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituyó un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>
        <div class="slide hidden" data-slide="8">
            <h2>7. Análisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigación:</h3>
            <ul>
                <li>¿Existen diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¿Cómo varían las características prosódicas entre emociones?</li>
                <li>¿Qué nivel de variabilidad existe dentro de cada categoría?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Distribución de Pitch</div>
            <div class="methodology-step"><strong>2.</strong> Gráficos de Violín de Coeficientes MFCC</div>
            <div class="methodology-step"><strong>3.</strong> Gráficos de Dispersión 3D de PCA y LDA</div>
            <footer class="footnote">
                <strong>Nota:</strong> Las siguientes visualizaciones fueron generadas con un script de Python para analizar las características del dataset RAVDESS.
            </footer>
        </div>
        <div class="slide hidden" data-slide="9">
            <h2>8. Visualización: Distribución del Pitch</h2>
            <div class="single-chart-container">
                <h3>Análisis de Pitch: Distribución de Frecuencia Fundamental (F0) por Emoción</h3>
                <img src="graficas/distribucion_pitch_por_emocion.png" alt="Histograma Pitch por Emoción">
                <p>
                    Esta gráfica nos muestra cómo se distribuye el <strong>tono de voz (Pitch)</strong> para cada emoción. El <strong>eje X</strong> representa la frecuencia en Hertz, donde valores más altos significan un tono más agudo. El <strong>eje Y</strong> indica la densidad de probabilidad, es decir, qué tan comunes son ciertos tonos para una emoción.
                </p>
                <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Podemos notar que emociones de alta energía como <strong>'happy' (feliz)</strong> y <strong>'surprised' (sorprendido)</strong> tienen sus curvas desplazadas hacia la derecha, indicando una tendencia a usar tonos más agudos. Por el contrario, <strong>'sad' (triste)</strong> se concentra en la zona izquierda, mostrando una clara preferencia por tonos más graves y monótonos.
                </footer>
            </div>
        </div>
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualización y Análisis de MFCCs</h2>
            <div class="mfcc-explanation">
                <div class="single-chart-container">
                    <h3>Análisis Espectral: Distribución de los Primeros 13 MFCCs por Emoción</h3>
                    <img src="graficas/distribucion_mfcc_por_emocion.png" alt="Gráficos de Violín de MFCCs por Emoción" style="max-width: 100%; border: 1px solid var(--color-accent);">
                </div>
                <p style="text-align: center; margin-top: 20px;">
                    Esta visualización nos permite comparar la "forma" del sonido para cada emoción a través de los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong>. Cada "violín" muestra el rango y la concentración de valores para un coeficiente (eje X) y una emoción (color).
                </p>
                 <footer class="chart-observation">
                    <strong>Observaciones Clave:</strong> Si observamos <strong>MFCC_1</strong>, notamos que la distribución para <strong>'angry' (enojado)</strong> es muy diferente a la de <strong>'sad' (triste)</strong>. Estas diferencias en la forma y posición de los violines son los patrones que el modelo de IA aprende para poder distinguir una emoción de otra. Coeficientes con distribuciones muy distintas entre colores son altamente informativos para la clasificación.
                </footer>
                <table class="mfcc-table">
                    <thead>
                        <tr>
                            <th>Coeficiente(s)</th>
                            <th>Interpretación General</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MFCC 0</strong></td>
                            <td>Energía total o sonoridad de la señal.</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 1-4</strong></td>
                            <td>Capturan la forma general y la pendiente del espectro (contornos principales).</td>
                        </tr>
                        <tr>
                            <td><strong>MFCC 5-13+</strong></td>
                            <td>Describen los detalles más finos y las "texturas" del espectro.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <div class="slide hidden" data-slide="11">
            <h2 class="technical-title">El Viaje del Audio: De la Onda al Vector</h2>
            <p>Antes de que la IA pueda analizar una emoción, debemos traducir la onda de sonido a un lenguaje que entienda: los números. Este proceso se llama <strong>Extracción de Características</strong>. A continuación, veremos el paso a paso de cómo convertimos un archivo de audio en un único vector de 180 características.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">🔊</div>
                    <div class="flow-text">Audio Original (.wav)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">#️⃣</div>
                    <div class="flow-text">Digitalización (Muestreo)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">🖼️</div>
                    <div class="flow-text">Ventaneo (Frames)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">📊</div>
                    <div class="flow-text">FFT y MFCCs (Por Ventana)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                     <div class="flow-icon">🧬</div>
                    <div class="flow-text">Vector Final (Promedio)</div>
                </div>
            </div>
            <footer class="footnote">Este es el corazón del preprocesamiento: transformar datos no estructurados (audio) en datos estructurados (un vector).</footer>
        </div>
        <div class="slide hidden" data-slide="12">
            <h2 class="technical-title">Paso 1: Digitalización y Ventaneo</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>1.1 Digitalización</h4>
                    <p>Una onda de sonido es una señal analógica continua. Para que una computadora la procese, debemos <strong>muestrearla</strong>. Esto significa tomar "fotos" o mediciones de su amplitud a intervalos de tiempo regulares.</p>
                    <ul>
                        <li><strong>Frecuencia de Muestreo (sr):</strong> 22,050 Hz. Tomamos 22,050 mediciones por segundo.</li>
                        <li><strong>Resultado (`x[n]`):</strong> Obtenemos un largo arreglo de números, donde cada número es la amplitud del sonido en un instante.</li>
                    </ul>
                    <h4>1.2 Ventaneo (Framing)</h4>
                    <p>El habla no es estática. Para analizarla, la dividimos en pequeños segmentos superpuestos llamados <strong>ventanas</strong> o <strong>frames</strong>, donde asumimos que el sonido es estable.</p>
                     <ul>
                        <li><strong>Tamaño de Ventana:</strong> ~25 milisegundos.</li>
                        <li><strong>Resultado:</strong> En lugar de un arreglo largo, ahora tenemos una colección de muchos arreglos pequeños (las ventanas).</li>
                    </ul>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/audio wave sampling and framing.png" alt="Diagrama de Digitalización y Ventaneo" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La onda de sonido se divide en múltiples ventanas (frames) para su análisis.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="13">
            <h2 class="technical-title">Paso 2: La Transformada de Fourier (FFT)</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>¿Qué es y para qué sirve?</h4>
                    <p>Para cada una de esas ventanas, necesitamos saber qué frecuencias la componen. La <strong>Transformada Rápida de Fourier (FFT)</strong> es la herramienta matemática que lo hace posible.</p>
                    <p>La FFT descompone la señal del dominio del tiempo (amplitud vs. tiempo) al <strong>dominio de la frecuencia</strong> (energía vs. frecuencia). Es como pasar de ver la onda completa a ver un ecualizador que nos muestra qué tan fuertes son los graves, los medios y los agudos en ese instante.</p>
                    <div class="formula-block">
                         <span class="formula-title">Modelo Matemático: Transformada Discreta de Fourier</span>
                         <p>$$X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i \frac{2\pi}{N} kn}$$</p>
                         <ul>
                            <li>$X_k$: El espectro resultante (un número complejo que contiene amplitud y fase para la frecuencia $k$).</li>
                            <li>$x_n$: El valor de la muestra $n$ en la ventana de audio.</li>
                            <li>$N$: El número total de muestras en la ventana.</li>
                            <li>$k$: El índice de la frecuencia que se está calculando (desde 0 hasta $N-1$).</li>
                         </ul>
                    </div>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/time domain to frequency domain FFT.png" alt="Diagrama de FFT" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">La FFT convierte una ventana de audio en su espectro de frecuencias.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="14">
            <h2 class="technical-title">Paso 3: MFCCs - El "ADN" de la Voz</h2>
             <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Más allá del Espectro</h4>
                    <p>El espectro de la FFT es útil, pero no es eficiente. Los <strong>Coeficientes Cepstrales en la Frecuencia Mel (MFCCs)</strong> son una forma mucho más inteligente de resumir la información del espectro, imitando cómo funciona el oído humano.</p>
                    <ol>
                        <li><strong>Escala Mel:</strong> Primero, se aplica un banco de filtros al espectro para agrupar las frecuencias de una manera logarítmica, similar a nuestra percepción auditiva.</li>
                        <li><strong>Logaritmo:</strong> Se toma el logaritmo de las energías, de nuevo, para imitar cómo percibimos la sonoridad.</li>
                        <li><strong>DCT:</strong> Finalmente, se aplica la Transformada de Coseno Discreta (DCT), una operación que comprime toda esa información espectral en unos pocos coeficientes.</li>
                    </ol>
                     <p>El resultado son los MFCCs: una descripción numérica muy compacta y robusta del <strong>timbre</strong> de la voz en esa ventana.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/MFCC block diagram.png" alt="Proceso de MFCC" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Flujo simplificado para obtener los MFCCs a partir del espectro.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="15">
            <h2 class="technical-title">Paso 4: El Vector Final de Características</h2>
            <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Del Análisis por Ventana al Resumen Global</h4>
                    <p>El proceso anterior nos da una matriz $M$ de características, donde cada fila $t$ corresponde a una ventana de tiempo y cada columna $j$ a una de las 180 características.</p>
                    <p>Para obtener un único vector $V$ que represente todo el audio, calculamos la media de cada característica a lo largo de todas las ventanas de tiempo $T$.</p>
                    <div class="formula-block">
                        <span class="formula-title">Cálculo del Vector Promedio</span>
                        <p>$$V_j = \frac{1}{T} \sum_{t=1}^{T} M_{t,j}$$</p>
                        <ul>
                           <li>$V_j$: Es el valor final de la característica $j$ en nuestro vector.</li>
                           <li>$T$: Es el número total de ventanas (frames) en el audio.</li>
                           <li>$M_{t,j}$: Es el valor de la característica $j$ en la ventana de tiempo $t$.</li>
                        </ul>
                   </div>
                   <p>Este proceso condensa la información temporal en una sola "ficha técnica" que describe las propiedades acústicas promedio de todo el clip.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/feature matrix averaging to vector.png" alt="Proceso de Promedio" style="width:80%; margin: 20px auto; display: block; border-radius: 10px;">
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="16">
            <h2>11. Preprocesamiento para Reducción Dimensional</h2>
            <h3>¿Qué fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-text-dark);">Estandarización de Características (Scaling)</h3>
                <p style="color: var(--color-text-dark);">
                    Técnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las características con rangos de valores más grandes (como el pitch) dominarían a las de rangos más pequeños (como los MFCCs), sesgando el análisis.
                </p>
            </div>
            <p>La estandarización asegura que todas las características contribuyan de manera equitativa al análisis, resultando en un modelo más justo y preciso.</p>
        </div>
        <div class="slide hidden" data-slide="17">
            <h2>12. ¿Cómo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. Cálculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las características (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">μ = (Σx) / n</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>2. Cálculo de la Desviación Estándar</h4>
                    <p>Luego, calcula la desviación estándar, que mide cuánta variación o dispersión existe respecto a la media.</p>
                     <p class="formula">σ = √[Σ(x-μ)² / n]</p>
                </div>
                <div class="scaler-arrow">→</div>
                <div class="scaler-step">
                    <h4>3. Transformación (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviación estándar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - μ) / σ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> Técnica de preprocesamiento que estandariza las características al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>
        <div class="slide hidden" data-slide="18">
            <h2>13. Implementación del Modelo CNN 1D</h2>
            <h3>1. Preparación de Datos (Código Real del Proyecto)</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Dividir datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# Estandarizar las características
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Añadir una dimensión para la CNN 1D
X_train_cnn = np.expand_dims(X_train_scaled, axis=2)
X_test_cnn = np.expand_dims(X_test_scaled, axis=2)</code></pre>
            <p class="code-description">Dividimos los datos, asegurando que cada emoción esté representada por igual en ambos conjuntos (`stratify=y`). Luego, escalamos los datos y añadimos una dimensión extra, que es el formato que espera la capa `Conv1D` de Keras.</p>
            
            <h3>2. Definición y Entrenamiento del Modelo</h3>
            <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense

model = Sequential([
    Conv1D(256, 5, padding='same', activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Conv1D(128, 5, padding='same', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(0.2),
    Flatten(),
    Dense(y_encoded.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train_cnn, y_train, epochs=100, batch_size=64, validation_split=0.2
)</code></pre>
            <p class="code-description">Definimos la arquitectura de la CNN 1D, la compilamos con el optimizador 'adam' y la función de pérdida para clasificación multiclase. Finalmente, la entrenamos con los datos de entrenamiento, usando un 20% de estos para validación interna en cada época.</p>
        </div>
        <div class="slide hidden" data-slide="19">
            <h2>14. Reducción de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>Análisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersión de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>Análisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separación entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyección para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegría"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por sí mismo.
            </footer>
        </div>
        <div class="slide hidden" data-slide="20">
            <h2>15. Visualización 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gráfica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos. Cada punto es un audio y su color corresponde a una emoción. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersión natural de los datos.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qué tan dispersos están los datos. <strong>Dispersión:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>
        <div class="slide hidden" data-slide="21">
            <h2>16. Visualización 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aquí, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separación entre las emociones. El resultado es una separación mucho más clara y cúmulos más compactos, lo que confirma visualmente que nuestras características son muy efectivas para la clasificación.
            </p>
        </div>
        <div class="slide hidden" data-slide="22">
            <h2>17. Comparación Final y Reflexión</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>Análisis de Componentes Principales (PCA)</th>
                        <th>Análisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersión general de los datos</td>
                        <td>Muestra qué tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opinión Reflexiva</h3>
            <p>Al comparar ambas técnicas, <strong>LDA demuestra ser más efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es útil para entender la estructura general de la varianza en los datos, LDA, al ser un método supervisado, logra crear proyecciones donde las emociones forman cúmulos más definidos y distinguibles. Esto sugiere que las características extraídas, cuando se proyectan con un objetivo de clasificación, son altamente discriminativas.</p>
        </div>
        <div class="slide hidden" data-slide="23">
            <h2>18. Fundamento Teórico: Red Neuronal Convolucional 1D</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>¿Cómo "Piensa" una CNN 1D?</h4>
                    <div class="code-block-container">
                        <div class="code-block-header">
                            <span class="code-title">Pseudocódigo de Operación</span>
                            <div class="window-controls">
                                <span class="control-btn minimize"></span>
                                <span class="control-btn maximize"></span>
                                <span class="control-btn close"></span>
                            </div>
                        </div>
                        <pre>
Entrada = Secuencia de Características (180 números)

<span class="comment">// 1. Capas Convolucionales 1D (Detectores de Patrones)</span>
Para cada filtro en la capa:
    Desliza el filtro sobre la secuencia
    Calcula la suma ponderada (convolución)
    Aplica función de activación (ReLU) -> Resalta patrones

<span class="comment">// 2. Capas de Pooling 1D (Compresores de Información)</span>
Reduce la longitud de la secuencia (ej. toma el valor máximo)
-> Mantiene la información más relevante y descarta el resto

<span class="comment">// 3. Capas Densas (Clasificador Final)</span>
Aplana la salida a un solo vector
Conecta todas las neuronas
Aplica Softmax para calcular probabilidades

<span class="comment">// 4. Salida</span>
Predicción = Emoción con la probabilidad más alta
                        </pre>
                    </div>
                    <div class="activation-function-info">
                        <h4>Funciones de Activación Clave</h4>
                        <p><strong>ReLU (Rectified Linear Unit):</strong> <span class="formula">ReLU(x) = max(0, x)</span>. Se usa en capas ocultas para introducir no-linealidad, permitiendo al modelo aprender relaciones complejas.</p>
                        <p><strong>Softmax:</strong> Se usa en la capa de salida para convertir las puntuaciones en una distribución de probabilidad sobre las 7 emociones.</p>
                    </div>
                </div>
        
                <div class="cnn-side-panel">
                    <div class="advantages-disadvantages-1d">
                        <h4>Ventajas del Enfoque 1D</h4>
                        <ul>
                            <li>Analiza directamente la secuencia temporal de características.</li>
                            <li>Eficiente computacionalmente, menos parámetros que una CNN 2D.</li>
                            <li>Arquitectura ideal para cualquier tipo de señal o serie de tiempo.</li>
                        </ul>
                        <h4>Desventajas</h4>
                        <ul>
                            <li>Puede perder información contextual que un espectrograma 2D sí capturaría.</li>
                            <li>Depende fuertemente de la calidad de las características extraídas manualmente.</li>
                        </ul>
                    </div>
                    <div class="cnn-audio-sample">
                        <h4>Muestra de Audio de Entrada (Disgusto)</h4>
                        <p>Este es un ejemplo del tipo de audio que procesa el modelo antes de la extracción de características.</p>
                        <audio controls src="presentation/src/audio/03-01-07-02-02-02-11.wav">
                            Tu navegador no soporta el elemento de audio.
                        </audio>
                    </div>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="24">
            <h2>19. Arquitectura del Modelo CNN 1D</h2>
            <p>Esta es la arquitectura de Keras que implementamos. Keras es una librería de alto nivel que facilita la construcción de redes neuronales. La red está diseñada para procesar la secuencia de 180 características extraídas de cada audio y aprender a clasificar la emoción.</p>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <p>Secuencia de Características<br><span class="layer-shape">(180 características, 1)</span></p>
                    <p class="layer-desc">Recibe el vector de 180 características como una secuencia temporal.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box conv">
                    <h4>Conv1D (256 filtros) + ReLU</h4>
                    <p class="layer-desc">La primera capa convolucional busca patrones temporales simples en la secuencia de entrada usando 256 filtros diferentes.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Reduce la longitud de la secuencia a una quinta parte, manteniendo solo las activaciones de patrones más fuertes para la siguiente capa.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box conv">
                    <h4>Conv1D (128 filtros) + ReLU</h4>
                    <p class="layer-desc">Una segunda capa convolucional busca patrones más complejos y de mayor duración a partir de las características detectadas por la capa anterior.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box pool">
                    <h4>MaxPooling1D</h4>
                    <p class="layer-desc">Vuelve a reducir la secuencia para condensar aún más la información.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box dense">
                    <h4>Flatten & Capa Densa</h4>
                    <p class="layer-desc">La capa Flatten convierte la secuencia final en un solo vector largo, que se conecta a una capa densa para la clasificación.</p>
                </div>
                <div class="arrow">↓</div>
                <div class="layer-box output">
                    <h4>Capa de Salida (Softmax)</h4>
                    <p class="layer-desc">La capa final tiene 7 neuronas, una por cada emoción, y usa Softmax para asignar una probabilidad a cada una.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="25">
            <h2 class="technical-title">Dentro de la "Caja Negra": La Red Neuronal</h2>
            <p>Ahora que tenemos nuestro vector de 180 características, es el turno de la Red Neuronal Convolucional 1D. Su trabajo es tomar este vector y, a través de una serie de capas y operaciones matemáticas, clasificarlo en una de las siete emociones. Veremos cómo lo hace.</p>
            <div class="process-flow-diagram">
                <div class="flow-step">
                    <div class="flow-icon">🧬</div>
                    <div class="flow-text">Vector de 180 Características</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">🔍</div>
                    <div class="flow-text">Capas Convolucionales</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                    <div class="flow-icon">🧠</div>
                    <div class="flow-text">Capas Densas (Clasificación)</div>
                </div>
                <div class="flow-arrow">→</div>
                <div class="flow-step">
                     <div class="flow-icon">🏷️</div>
                    <div class="flow-text">Predicción de Emoción</div>
                </div>
            </div>
             <footer class="footnote">El modelo aprende a asociar patrones en el vector de entrada con una etiqueta de emoción específica.</footer>
        </div>
        <div class="slide hidden" data-slide="26">
            <h2 class="technical-title">Red Neuronal - Paso 1: Convolución</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Los Detectores de Patrones</h4>
                    <p>La operación de <strong>convolución</strong> es el corazón de la CNN. Un "filtro" (un pequeño vector de pesos que el modelo aprende) se desliza a lo largo de nuestro vector de 180 características.</p>
                    <p>En cada posición, calcula el <strong>producto punto</strong> entre el filtro y la sección del vector. El resultado es un número alto si la sección del vector se "parece" al patrón que el filtro está buscando.</p>
                    <p>Nuestro modelo usa 256 filtros en la primera capa, cada uno buscando un patrón simple y diferente. El resultado son 256 "mapas de características" que nos dicen dónde se encontraron esos patrones.</p>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/1Dconvolutionanimationgif.gif" alt="Animación de Convolución 1D" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Un filtro (amarillo) se desliza sobre la entrada (azul) para producir un mapa de características (verde).</p>
                </div>
            </div>
            <footer class="footnote"><strong>Producto Punto:</strong> Una operación matemática que multiplica los elementos correspondientes de dos vectores y suma los resultados. Mide qué tan 'alineados' o similares son los dos vectores.</footer>
        </div>
        <div class="slide hidden" data-slide="27">
            <h2 class="technical-title">Red Neuronal - Paso 2: ReLU y Pooling</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>Filtrar y Resumir</h4>
                    <p>Después de la convolución, se aplican dos operaciones simples pero cruciales:</p>
                    <ol>
                        <li><strong>Activación ReLU:</strong> A cada número en los mapas de características, le aplicamos la función `ReLU(x) = max(0, x)`. Esto elimina todas las activaciones negativas, permitiendo que el modelo se enfoque solo en la presencia significativa de un patrón.</li>
                        <li><strong>MaxPooling:</strong> Reducimos el tamaño de cada mapa de características. Por ejemplo, de cada 5 valores nos quedamos solo con el más alto. Esto hace al modelo más eficiente y robusto, ya que se concentra en si un patrón apareció en una región, no en su posición exacta.</li>
                    </ol>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/ReLU and Max Pooling diagram.png" alt="Diagrama de ReLU y Pooling" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">ReLU elimina los valores negativos. MaxPooling resume la información, quedándose con los picos de activación.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="28">
            <h2 class="technical-title">Red Neuronal - Paso 3: Capas Profundas</h2>
             <div class="technical-step-layout full-width">
                 <div class="step-explanation">
                    <h4>Creando Abstracción</h4>
                    <p>El verdadero poder de las redes profundas viene de <strong>apilar</strong> estas capas. La salida de un bloque `Convolución -> ReLU -> Pooling` se convierte en la entrada del siguiente.</p>
                    <p>Los filtros de la segunda capa ya no buscan patrones en el vector original, sino que buscan <strong>patrones de patrones</strong>. Por ejemplo, un filtro puede aprender a activarse si detecta el "patrón A" seguido del "patrón B" de la capa anterior. Esto crea una jerarquía de características, donde el modelo aprende conceptos cada vez más complejos y abstractos.</p>
                </div>
                <div class="step-visualization">
                     <img src="presentation/src/assets/images/CNN feature hierarchy.png" alt="Arquitectura Profunda" style="width:90%; margin: 20px auto; display: block; border-radius: 10px;">
                     <p class="viz-caption">Las capas se apilan para que el modelo aprenda de lo simple a lo complejo.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="29">
            <h2 class="technical-title">Red Neuronal - Paso 4: Clasificación Final</h2>
            <div class="technical-step-layout">
                <div class="step-explanation">
                    <h4>La Toma de Decisión</h4>
                    <ol>
                        <li><strong>Flatten (Aplanar):</strong> Después de la última capa de pooling, todos los mapas de características se "aplanan" y se unen en un solo vector larguísimo.</li>
                        <li><strong>Capas Densas:</strong> Este vector entra a una o más capas "densas", donde cada neurona está conectada a todas las del vector aplanado. Estas capas aprenden a "votar" por una emoción basándose en la combinación de todos los patrones detectados.</li>
                        <li><strong>Capa de Salida (Softmax):</strong> La última capa tiene 7 neuronas (una por emoción). La función <strong>Softmax</strong> toma las 7 puntuaciones finales y las convierte en un vector de probabilidades que suma 1.</li>
                    </ol>
                </div>
                <div class="step-visualization">
                    <img src="presentation/src/assets/images/neural network flatten dense layer softmax.png" alt="Diagrama de Capas Densas y Softmax" style="width:100%; border-radius: 10px;">
                    <p class="viz-caption">Las características abstractas se conectan a las capas de decisión, que emiten una probabilidad para cada clase.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="30">
            <h2 class="technical-title">El Aprendizaje: ¿Cómo se Entrena el Modelo?</h2>
            <p>El entrenamiento es un ciclo de optimización para ajustar los 171,783 pesos de los filtros y las capas densas. Se repite miles de veces hasta que el error es mínimo.</p>
            <div class="training-steps-grid">
                <div class="training-step-card">
                    <div class="step-number">1</div>
                    <h4>Predicción (Forward Pass)</h4>
                    <p>El modelo recibe datos de entrada y genera una predicción de la emoción, propagando la información a través de sus capas.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">2</div>
                    <h4>Cálculo de la Pérdida (Error)</h4>
                    <p>Comparamos la predicción del modelo con la etiqueta de emoción real. La función de pérdida (entropía cruzada categórica) cuantifica qué tan "equivocado" estuvo el modelo.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">3</div>
                    <h4>Backpropagation (Retropropagación)</h4>
                    <p>El error calculado se propaga hacia atrás a través de la red. Esto nos permite determinar cómo cada peso individual contribuyó al error total.</p>
                </div>
                <div class="training-step-card">
                    <div class="step-number">4</div>
                    <h4>Actualización de Pesos (Optimización)</h4>
                    <p>Utilizando un optimizador (como Adam), los pesos del modelo se ajustan ligeramente en la dirección que minimiza la función de pérdida, preparando al modelo para una mejor predicción en la siguiente iteración.</p>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="31">
            <h2>20. Resumen de Neuronas y Parámetros</h2>
            <div class="cnn-details">
                <p>
                    Esta tabla detalla la estructura de nuestro modelo. La <strong>Forma de Salida</strong> muestra cómo cambian las dimensiones de los datos después de cada capa. Los <strong>Parámetros</strong> son los pesos o "conocimientos" que el modelo aprende durante el entrenamiento. En total, nuestro modelo debe aprender y ajustar **171,783 parámetros** para poder realizar la clasificación.
                </p>
                <table class="cnn-table">
                    <thead>
                        <tr>
                            <th>Capa (Tipo)</th>
                            <th>Forma de Salida</th>
                            <th>Parámetros</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>conv1d</td>
                            <td>(None, 180, 256)</td>
                            <td>1,536</td>
                        </tr>
                        <tr>
                            <td>max_pooling1d</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout</td>
                            <td>(None, 36, 256)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>conv1d_1</td>
                            <td>(None, 36, 128)</td>
                            <td>163,968</td>
                        </tr>
                         <tr>
                            <td>max_pooling1d_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dropout_1</td>
                            <td>(None, 7, 128)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>flatten</td>
                            <td>(None, 896)</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>dense</td>
                            <td>(None, 7)</td>
                            <td>6,279</td>
                        </tr>
                        <tr class="cnn-total">
                            <td colspan="2"><strong>Total de Parámetros Entrenables</strong></td>
                            <td><strong>171,783</strong></td>
                        </tr>
                    </tbody>
                </table>
                 <footer class="cnn-table-footnote">
                    <strong>Parámetros:</strong> Son los "conocimientos" internos de la red que se ajustan durante el entrenamiento.
                </footer>
            </div>
        </div>

        <!-- Diapositivas Finales -->
        <div class="slide hidden" data-slide="32">
            <h2>21. Recursos</h2>
            <div class="dataset-card">
                <h3>Lenguaje y Frameworks</h3>
                <ul>
                    <li><strong>Python 3.8+:</strong> Lenguaje principal</li>
                    <li><strong>TensorFlow/Keras:</strong> Desarrollo de CNN 1D</li>
                    <li><strong>Librosa:</strong> Extracción de características de audio</li>
                    <li><strong>Scikit-learn:</strong> Preprocesamiento y métricas</li>
                </ul>
            </div>
            <div class="dataset-card">
                <h3>Hardware y Plataformas</h3>
                <ul>
                    <li><strong>Google Colab:</strong> Entrenamiento de modelos con GPU.</li>
                    <li><strong>GitHub:</strong> Control de versiones.</li>
                </ul>
            </div>
        </div>
        <div class="slide hidden" data-slide="33">
            <h2>22. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo CNN 1D para clasificar 7 emociones.</li>
                        <li>Pipeline de extracción de 180 características.</li>
                        <li>Comparación visual 3D de PCA y LDA.</li>
                        <li>Métricas de rendimiento del modelo.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>Análisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicación de usuario final.</li>
                        <li>El modelo no opera en tiempo real.</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="34">
            <h1 class="farewell-title">¡Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la efectividad de las CNN 1D para modelar secuencias<br>
                de características y clasificar emociones complejas en la voz.
            </p>
            <div class="highlight-box">
                <h3>🤔 ¿Preguntas o Comentarios?</h3>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <a href="https://www.instagram.com/alexpl.0" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro Pérez</span>
                    </a>
                    <a href="https://www.instagram.com/davidsandoval____" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <a href="https://www.instagram.com/_jair.gg" target="_blank" class="contact-item" rel="noopener noreferrer">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">← Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente →</button>
    </div>

    <script src="presentation/src/js/script.js"></script>
</body>
</html>
