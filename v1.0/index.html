<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="../presentation/src/assets/logo.png" type="image/png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An√°lisis de Emociones en la Voz con IA</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>
    
    <div class="slide-counter" id="slideCounter">1 / 29</div>

    <div class="presentation-container">
        <!-- Diapositivas 1-15 (Sin cambios) -->
        <div class="slide" data-slide="1">
            <h1>An√°lisis de Emociones en la Voz con Inteligencia Artificial</h1>
            <div class="highlight-box">
                <h3>Explorando Patrones Ac√∫sticos para la Clasificaci√≥n del Habla Afectiva</h3>
                <p style="text-align: center; font-size: 1.2rem; margin-top: 20px;">
                    Un enfoque t√©cnico para la modelaci√≥n de caracter√≠sticas pros√≥dicas y espectrales.
                </p>
            </div>
            <div class="team-info">
                <h3>Equipo de Desarrollo</h3>
                <p><strong>Alejandro P√©rez</strong></p>
                <p><strong>Yusmany Rejopachi</strong></p>
                <p><strong>Jair Guti√©rrez</strong></p>
            </div>
        </div>
        <div class="slide hidden" data-slide="2">
            <h2>1. Justificaci√≥n T√©cnica</h2>
            <div class="highlight-box">
                <h3>¬øPor qu√© Audio para Reconocimiento Emocional?</h3>
                <p>La voz humana contiene una firma ac√∫stica compleja, rica en informaci√≥n latente sobre el estado afectivo del hablante.</p>
            </div>
            <div class="stats-grid">
                <div class="stat-item"><h3>Caracter√≠sticas Pros√≥dicas</h3><p>El pitch, la intensidad y el ritmo del habla son indicadores clave del estado emocional.</p></div>
                <div class="stat-item"><h3>Patrones Espectrales</h3><p>La distribuci√≥n de energ√≠a en las frecuencias (formantes) var√≠a sistem√°ticamente con la emoci√≥n.</p></div>
                <div class="stat-item"><h3>Se√±al No Estructurada</h3><p>El habla es una fuente de datos compleja, ideal para ser modelada con t√©cnicas de IA.</p></div>
            </div>
            <p><strong>¬øPor qu√© Inteligencia Artificial?</strong></p>
            <ul>
                <li><strong>Extracci√≥n de Patrones:</strong> Capacidad para identificar autom√°ticamente caracter√≠sticas complejas en espectrogramas, indetectables para el an√°lisis tradicional.</li>
                <li><strong>An√°lisis Objetivo:</strong> Los modelos de IA ofrecen una cuantificaci√≥n consistente y reproducible de las caracter√≠sticas vocales.</li>
                <li><strong>Modelado de Alta Dimensi√≥n:</strong> Habilidad para procesar miles de caracter√≠sticas extra√≠das de una sola se√±al de audio.</li>
            </ul>
        </div>
        <div class="slide hidden" data-slide="3">
            <h2>2. Descripci√≥n del Problema</h2>
            <div class="highlight-box">
                <h3>Problema T√©cnico Principal</h3>
                <p>El desaf√≠o de clasificar estados emocionales a partir de la se√±al del habla, que es inherentemente variable, ruidosa y de alta dimensionalidad.</p>
            </div>
            <h3>¬øQu√© reto t√©cnico abordamos?</h3>
            <ul>
                <li>Desarrollar un modelo capaz de analizar las sutiles variaciones en grabaciones de voz.</li>
                <li>Identificar y diferenciar patrones ac√∫sticos para 7 emociones distintas: <strong>alegr√≠a, tristeza, enojo, miedo, sorpresa, disgusto y neutralidad.</strong></li>
                <li>Manejar la variabilidad entre diferentes hablantes, idiomas y calidades de grabaci√≥n.</li>
                <li>Construir un pipeline de datos robusto, desde el preprocesamiento de la se√±al hasta la clasificaci√≥n.</li>
            </ul>
            <footer class="footnote">
                <strong>Pipeline:</strong> Secuencia de pasos de procesamiento de datos, donde la salida de un paso es la entrada del siguiente.
            </footer>
        </div>
        <div class="slide hidden" data-slide="4">
            <h2>3. Objetivo General</h2>
            <div class="highlight-box" style="display: flex; align-items: center; justify-content: center; text-align: center; min-height: 50vh;">
                <p style="font-size: 1.5rem; line-height: 1.7;">Desarrollar y evaluar un modelo de inteligencia artificial para la clasificaci√≥n de emociones humanas a partir del an√°lisis de caracter√≠sticas ac√∫sticas y espectrales del habla, estableciendo un pipeline completo desde el preprocesamiento de la se√±al hasta la predicci√≥n del modelo.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="5">
            <h2>4. Objetivos Espec√≠ficos</h2>
            <div class="methodology-step"><strong>1.</strong> Integrar y preprocesar m√∫ltiples conjuntos de datos de audio emocional</div>
            <div class="methodology-step"><strong>2.</strong> Extraer y analizar caracter√≠sticas ac√∫sticas relevantes del habla emocional</div>
            <div class="methodology-step"><strong>3.</strong> Dise√±ar, entrenar y optimizar modelos especializados de aprendizaje autom√°tico</div>
            <div class="methodology-step"><strong>4.</strong> Evaluar el rendimiento utilizando m√©tricas est√°ndar de clasificaci√≥n</div>
            <div class="methodology-step"><strong>5.</strong> Implementar t√©cnicas de reducci√≥n de dimensionalidad y visualizaci√≥n</div>
        </div>
        <div class="slide hidden" data-slide="6">
            <h2>5. Metodolog√≠a Iterativa</h2>
            <p>Adoptamos un enfoque de desarrollo c√≠clico, permitiendo la retroalimentaci√≥n y mejora continua en cada fase del proyecto.</p>
            <div class="methodology-diagram-iterative">
                <div class="step-container">
                    <div class="step">Adquisici√≥n de Datos</div>
                    <div class="arrow-down">‚Üì</div>
                    <div class="step">An√°lisis Exploratorio</div>
                    <div class="arrow-down">‚Üì</div>
                    <div class="step">Preprocesamiento</div>
                </div>
                <div class="step-container">
                    <div class="arrow-loop-right">‚§¥</div>
                    <div class="step">Evaluaci√≥n</div>
                    <div class="arrow-up">‚Üë</div>
                     <div class="step">Entrenamiento</div>
                    <div class="arrow-up">‚Üë</div>
                    <div class="step">Extracci√≥n de Caracter√≠sticas</div>
                    <div class="arrow-loop-left">‚§µ</div>
                </div>
            </div>
            <p style="text-align: center; margin-top: 20px;">Este modelo permite regresar a pasos anteriores si los resultados de la evaluaci√≥n no son satisfactorios, por ejemplo, volviendo al preprocesamiento o a la extracci√≥n de caracter√≠sticas para refinar el modelo.</p>
        </div>
        <div class="slide hidden" data-slide="7">
            <h2>6. Adquisici√≥n de Conjuntos de Datos</h2>
            <div class="dataset-card"><h3>Conjunto 1: Base de Datos de Habla Emocional Mexicana (MESD)</h3><ul><li><strong>Cantidad:</strong> 864 grabaciones de audio</li><li><strong>Caracter√≠sticas:</strong> Espa√±ol mexicano, 6 emociones + neutralidad</li><li><strong>Utilidad:</strong> Adaptaci√≥n espec√≠fica al habla mexicana</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 2: Audio de Habla Emocional RAVDESS</h3><ul><li><strong>Cantidad:</strong> 1,440 grabaciones vocales</li><li><strong>Caracter√≠sticas:</strong> Calidad profesional, 24 actores</li><li><strong>Utilidad:</strong> Benchmarks robustos de rendimiento</li></ul></div>
            <div class="dataset-card"><h3>Conjunto 3: Toronto Emotional Speech Set (TESS)</h3><ul><li><strong>Cantidad:</strong> 2,800 muestras de audio</li><li><strong>Caracter√≠sticas:</strong> Alta calidad, actrices entrenadas</li><li><strong>Utilidad:</strong> Datos consistentes y controlados para el modelo base</li></ul></div>
             <footer class="footnote">
                <strong>Nota:</strong> Se sustituy√≥ un dataset originalmente planeado por TESS debido a que el primero fue retirado de Kaggle, asegurando la disponibilidad y reproducibilidad del proyecto.
            </footer>
        </div>
        <div class="slide hidden" data-slide="8">
            <h2>7. An√°lisis Exploratorio de Datos (EDA)</h2>
            <h3>Preguntas principales de investigaci√≥n:</h3>
            <ul>
                <li>¬øExisten diferencias espectrales consistentes entre estados emocionales?</li>
                <li>¬øC√≥mo var√≠an las caracter√≠sticas pros√≥dicas entre emociones?</li>
                <li>¬øQu√© nivel de variabilidad existe dentro de cada categor√≠a?</li>
            </ul>
            <h3>Visualizaciones Seleccionadas:</h3>
            <div class="methodology-step"><strong>1.</strong> Histogramas de Caracter√≠sticas Pros√≥dicas</div>
            <div class="methodology-step"><strong>2.</strong> Mapas de Calor de Correlaci√≥n Espectral</div>
            <div class="methodology-step"><strong>3.</strong> Gr√°ficos de Dispersi√≥n de Componentes Principales</div>
        </div>
        <div class="slide hidden" data-slide="9">
            <h2>8. Visualizaci√≥n: Distribuci√≥n del Pitch</h2>
            <div class="single-chart-container">
                <h3>Pitch Fundamental por Emoci√≥n</h3>
                <img src="presentation/src/assets/images/histograma_pitch_por_emocion.png" alt="Histograma Pitch por Emoci√≥n">
                <p>
                    Esta gr√°fica muestra la distribuci√≥n de la frecuencia fundamental (Pitch o F0) para cada emoci√≥n. El eje X representa el Pitch en Hertz (Hz), que corresponde a qu√© tan grave o agudo es un sonido. El eje Y muestra la densidad de probabilidad. Se observa que emociones como "alegr√≠a" y "sorpresa" tienden a tener un pitch m√°s alto, mientras que la "tristeza" se asocia con tonos m√°s graves.
                </p>
            </div>
        </div>
        <div class="slide hidden" data-slide="10">
            <h2>9. Visualizaci√≥n: Correlaci√≥n de MFCC</h2>
            <div class="single-chart-container">
                <h3>Correlaci√≥n entre MFCC y Emociones</h3>
                <img src="graficas/mfcc/heatmap_correlacion_mfcc_emocion.png" alt="Mapa de calor MFCC-Emoci√≥n">
                <p>
                    Este mapa de calor muestra la correlaci√≥n entre los 13 coeficientes MFCC y las categor√≠as emocionales. Los colores c√°lidos (rojo) indican una correlaci√≥n positiva, mientras que los fr√≠os (azul) indican una negativa. Esta visualizaci√≥n nos ayuda a entender qu√© coeficientes son m√°s importantes para distinguir entre emociones, guiando la selecci√≥n de caracter√≠sticas para el modelo.
                </p>
            </div>
            <footer class="footnote">
                <strong>MFCCs (Coeficientes Cepstrales en la Frecuencia Mel):</strong> Caracter√≠stica que representa el espectro de una se√±al de audio de una manera compacta, imitando la percepci√≥n del o√≠do humano. https://turing.iimas.unam.mx/~ivanvladimir/posts/mfcc/
            </footer>
        </div>
        <div class="slide hidden" data-slide="11">
            <h2>10. Preprocesamiento de Datos</h2>
            <div class="preprocessing-content">
                <p>Se aplicaron varios pasos para limpiar y estandarizar los datos de audio, asegurando la calidad para el entrenamiento del modelo.</p>
                <ul>
                    <li><strong>Normalizaci√≥n de Audio:</strong> Unificar todos los archivos a 22.05 kHz y 16 bits.</li>
                    <li><strong>Eliminaci√≥n de Datos Corruptos:</strong> Descartar archivos inv√°lidos o de duraci√≥n menor a 1 segundo.</li>
                    <li><strong>Filtrado de Ruido:</strong> Aplicar filtros para eliminar ruido de fondo y frecuencias no relevantes.</li>
                    <li><strong>Segmentaci√≥n Temporal:</strong> Dividir audios largos en ventanas de 3 segundos con solapamiento.</li>
                    <li><strong>Balanceo de Clases:</strong> Usar SMOTE para sobremuestrear clases minoritarias y equilibrar el dataset.</li>
                </ul>
            </div>
            <footer class="footnote">
                <strong>Solapamiento:</strong> T√©cnica donde las ventanas de audio se superponen para no perder informaci√≥n en los bordes. <strong>SMOTE:</strong> T√©cnica para crear muestras sint√©ticas de las clases minoritarias y as√≠ balancear el dataset.
            </footer>
        </div>
        <div class="slide hidden" data-slide="12">
            <h2>11. Preprocesamiento para Reducci√≥n Dimensional</h2>
            <h3>¬øQu√© fue necesario para poder usar PCA y LDA correctamente?</h3>
            <div class="highlight-box" style="background: linear-gradient(135deg, var(--color-bg-accent), var(--color-bg-card));">
                <h3 style="color: var(--color-bg-dark);">Estandarizaci√≥n de Caracter√≠sticas (Scaling)</h3>
                <p style="color: var(--color-bg-dark);">
                    T√©cnicas como PCA y LDA son muy sensibles a la escala de las variables de entrada. Sin un escalado previo, las caracter√≠sticas con rangos de valores m√°s grandes (como el pitch) dominar√≠an a las de rangos m√°s peque√±os (como los MFCCs), sesgando el an√°lisis.
                </p>
            </div>
            <p>La estandarizaci√≥n asegura que todas las caracter√≠sticas contribuyan de manera equitativa al an√°lisis, resultando en un modelo m√°s justo y preciso.</p>
        </div>
        <div class="slide hidden" data-slide="13">
            <h2>12. ¬øC√≥mo Funciona StandardScaler?</h2>
            <div class="scaler-explanation">
                <div class="scaler-step">
                    <h4>1. C√°lculo de la Media</h4>
                    <p>Primero, calcula la media (promedio) de cada una de las caracter√≠sticas (columnas) en el conjunto de datos de entrenamiento.</p>
                    <p class="formula">Œº = (Œ£x) / n</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>2. C√°lculo de la Desviaci√≥n Est√°ndar</h4>
                    <p>Luego, calcula la desviaci√≥n est√°ndar, que mide cu√°nta variaci√≥n o dispersi√≥n existe respecto a la media.</p>
                     <p class="formula">œÉ = ‚àö[Œ£(x-Œº)¬≤ / n]</p>
                </div>
                <div class="scaler-arrow">‚Üí</div>
                <div class="scaler-step">
                    <h4>3. Transformaci√≥n (Z-score)</h4>
                    <p>Finalmente, para cada valor, resta la media y lo divide por la desviaci√≥n est√°ndar. Esto centra los datos en 0 y les da una varianza de 1.</p>
                    <p class="formula">z = (x - Œº) / œÉ</p>
                </div>
            </div>
             <footer class="footnote">
                <strong>StandardScaler:</strong> T√©cnica de preprocesamiento que estandariza las caracter√≠sticas al remover la media y escalar a una varianza unitaria.
            </footer>
        </div>
        <div class="slide hidden" data-slide="14">
            <h2>13. Implementaci√≥n B√°sica del Algoritmo</h2>
            <h3>1. Divisi√≥n de Datos (Train/Test Split)</h3>
            <pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)</code></pre>
            <p class="code-description">Se dividen los datos en conjuntos de entrenamiento (70%) y prueba (30%). `stratify=y` asegura que la proporci√≥n de cada emoci√≥n sea la misma en ambos conjuntos.</p>
            <h3>2. Entrenamiento del Modelo (.fit)</h3>
            <pre><code>from sklearn.linear_model import LogisticRegression

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)</code></pre>
            <p class="code-description">Se escalan los datos de entrenamiento y luego se entrena el modelo de Regresi√≥n Log√≠stica con ellos.</p>
            <h3>3. Predicci√≥n y Evaluaci√≥n (.predict + .score)</h3>
            <pre><code>X_test_scaled = scaler.transform(X_test)
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)</code></pre>
            <p class="code-description">Se usan el `scaler` y el `model` ya entrenados para hacer predicciones sobre los datos de prueba y se calcula la precisi√≥n.</p>
        </div>
        <div class="slide hidden" data-slide="15">
            <h2>14. Reducci√≥n de Dimensionalidad: PCA vs. LDA</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h4>An√°lisis de Componentes Principales (PCA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> No Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la varianza de los datos.</li>
                        <li><strong>Funcionamiento:</strong> Ignora las etiquetas y solo se enfoca en la dispersi√≥n de los datos.</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h4>An√°lisis Discriminante Lineal (LDA)</h4>
                    <ul>
                        <li><strong>Tipo:</strong> Supervisado.</li>
                        <li><strong>Objetivo:</strong> Encontrar los ejes que maximizan la separaci√≥n entre las clases.</li>
                        <li><strong>Funcionamiento:</strong> Usa las etiquetas para encontrar la mejor proyecci√≥n para clasificar.</li>
                    </ul>
                </div>
            </div>
            <footer class="footnote">
               <strong>Supervisado:</strong> El algoritmo aprende de datos que han sido etiquetados (p. ej., un audio etiquetado como "alegr√≠a"). <strong>No Supervisado:</strong> El algoritmo aprende de datos sin etiquetar, buscando patrones por s√≠ mismo.
            </footer>
        </div>

        <!-- ***** INICIO DE DIAPOSITIVAS ACTUALIZADAS ***** -->

        <!-- Diapositiva 16: PCA 3D (Actualizada) -->
        <div class="slide hidden" data-slide="16">
            <h2>15. Visualizaci√≥n 3D: PCA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/pca_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Esta gr√°fica muestra los datos proyectados en los 3 Componentes Principales (PC1, PC2, PC3), que juntos capturan la mayor parte de la varianza de los datos (aprox. 68%). Cada punto es un audio y su color corresponde a una emoci√≥n. PCA, al ser no supervisado, no intenta separar los colores, sino mostrar la dispersi√≥n natural de los datos. Observamos que algunas emociones como 'enojo' y 'alegr√≠a' forman c√∫mulos algo definidos, pero hay un considerable solapamiento general.
            </p>
            <footer class="footnote">
                <strong>Varianza:</strong> Medida de qu√© tan dispersos est√°n los datos. <strong>Dispersi√≥n:</strong> Grado en que los datos se distribuyen o se alejan de un valor central.
            </footer>
        </div>

        <!-- Diapositiva 17: LDA 3D (Actualizada) -->
        <div class="slide hidden" data-slide="17">
            <h2>16. Visualizaci√≥n 3D: LDA</h2>
            <div class="iframe-plot-container">
                <iframe src="graficas/plots_3d/lda_3d_plot.html"></iframe>
            </div>
            <p class="plot-description">
                Aqu√≠, los ejes (LD1, LD2, LD3) son calculados por LDA para maximizar la separaci√≥n entre las emociones (los colores). A diferencia de PCA, el objetivo es puramente discriminativo. El resultado es una separaci√≥n mucho m√°s clara y c√∫mulos m√°s compactos. Esto demuestra visualmente que las caracter√≠sticas ac√∫sticas que extrajimos contienen informaci√≥n muy relevante para distinguir entre las diferentes clases de emociones.
            </p>
        </div>
        <div class="slide hidden" data-slide="18">
            <h2>17. Comparaci√≥n Final y Reflexi√≥n</h2>
            <h3>Tabla Comparativa</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Criterio</th>
                        <th>An√°lisis de Componentes Principales (PCA)</th>
                        <th>An√°lisis Discriminante Lineal (LDA)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tipo</td>
                        <td>No Supervisado</td>
                        <td>Supervisado</td>
                    </tr>
                    <tr>
                        <td>Objetivo</td>
                        <td>Maximizar la varianza de los datos</td>
                        <td>Maximizar la separabilidad entre clases</td>
                    </tr>
                    <tr>
                        <td>Uso de Etiquetas</td>
                        <td>No utiliza las etiquetas de las emociones</td>
                        <td>Utiliza las etiquetas para encontrar los ejes</td>
                    </tr>
                    <tr>
                        <td>Resultado Visual</td>
                        <td>Muestra la dispersi√≥n general de los datos</td>
                        <td>Muestra qu√© tan bien se pueden separar las clases</td>
                    </tr>
                </tbody>
            </table>
            <h3>Opini√≥n Reflexiva</h3>
            <p>Al comparar ambas t√©cnicas, <strong>LDA demuestra ser m√°s efectivo para visualizar la separabilidad de las clases</strong> en nuestro problema. Mientras que PCA es √∫til para entender la estructura general de la varianza en los datos, LDA, al ser un m√©todo supervisado, logra crear proyecciones donde las emociones forman c√∫mulos m√°s definidos y distinguibles. Esto sugiere que las caracter√≠sticas extra√≠das, cuando se proyectan con un objetivo de clasificaci√≥n, son altamente discriminativas.</p>
        </div>
        <div class="slide hidden" data-slide="19">
            <h2>18. Fundamento Te√≥rico: Redes Neuronales Convolucionales (CNN)</h2>
            <div class="cnn-theory-layout">
                <div class="cnn-text">
                    <h4>Pseudoc√≥digo: ¬øC√≥mo funciona una CNN?</h4>
                    <pre>
Entrada = Imagen o espectrograma

// 1. Capas Convolucionales
Para cada filtro en la capa:
    Desliza el filtro sobre la imagen
    Calcula la suma ponderada (convoluci√≥n)
    Aplica funci√≥n de activaci√≥n (ReLU)

// 2. Capas de Pooling
Reduce el tama√±o de la imagen (ej. toma el valor m√°ximo en cada regi√≥n)

// 3. Capas Densas (Fully Connected)
Aplana la salida anterior
Conecta con neuronas densas
Aplica funci√≥n de activaci√≥n

// 4. Capa de Salida
Calcula probabilidades para cada clase (Softmax)
                    </pre>

                    <div class="cnn-details">
                        <h4>Funci√≥n de Activaci√≥n</h4>
                        <div class="formula">ReLU(x) = max(0, x)</div>
                        <p>Utilizamos <strong>ReLU</strong> en las capas ocultas y <strong>Softmax</strong> en la salida para clasificaci√≥n multiclase.</p>
                    </div>

                    <h4>Ventajas</h4>
                    <ul>
                        <li>Extracci√≥n autom√°tica de caracter√≠sticas.</li>
                        <li>Robustez ante peque√±as variaciones.</li>
                        <li>Alto rendimiento en tareas de clasificaci√≥n de im√°genes.</li>
                    </ul>
                    <h4>Desventajas</h4>
                    <ul>
                        <li>Requieren grandes cantidades de datos.</li>
                        <li>Computacionalmente intensivas.</li>
                        <li>Su l√≥gica interna puede ser dif√≠cil de interpretar ("caja negra").</li>
                    </ul>
                </div>
                <div class="cnn-audio">
                    <h4>Audio de Ejemplo (Disgusto)</h4>
                    <p>Este es el audio del cual se generaron los siguientes espectrogramas.</p>
                    <audio controls>
                        <source src="graficas/espectrogramas/03-01-07-02-02-02-11.wav" type="audio/wav">
                        Tu navegador no soporta el elemento de audio.
                    </audio>
                </div>
            </div>
        </div>
        <div class="slide hidden" data-slide="20">
            <h2>An√°lisis Espectral (1/4): Forma de Onda</h2>
            <div class="single-chart-container">
                <img src="graficas/espectrogramas/espectrograma_waveform.png" alt="Forma de Onda">
                <p>Esta gr√°fica muestra la <strong>amplitud</strong> de la se√±al (eje Y) a lo largo del <strong>tiempo</strong> (eje X). Representa la variaci√≥n de la presi√≥n del aire, que percibimos como volumen. Nos permite observar la din√°mica general del habla, como pausas y cambios de intensidad.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="21">
            <h2>An√°lisis Espectral (2/4): Espectrograma Lineal</h2>
            <div class="single-chart-container">
                <img src="graficas/espectrogramas/espectrograma_linear.png" alt="Espectrograma Lineal">
                <p>Aqu√≠, el eje X es el <strong>tiempo</strong>, el eje Y es la <strong>frecuencia en Hertz (Hz)</strong>, y el <strong>color</strong> representa la energ√≠a o intensidad de cada frecuencia. Los colores c√°lidos (amarillo) indican alta energ√≠a. Esta vista nos permite ver qu√© frecuencias componen el sonido en cada instante.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="22">
            <h2>An√°lisis Espectral (3/4): Espectrograma Mel</h2>
            <div class="single-chart-container">
                <img src="graficas/espectrogramas/espectrograma_mel.png" alt="Espectrograma Mel">
                <p>Es una variaci√≥n del espectrograma lineal donde la escala de frecuencia (eje Y) se ajusta a la <strong>escala Mel</strong>, que imita c√≥mo el o√≠do humano percibe el sonido. Damos m√°s resoluci√≥n a las bajas frecuencias, que son cruciales para entender el habla. Esta es la representaci√≥n que alimentar√° a nuestra CNN.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="23">
            <h2>An√°lisis Espectral (4/4): Cromograma</h2>
            <div class="single-chart-container">
                <img src="graficas/espectrogramas/espectrograma_chroma.png" alt="Cromograma">
                <p>Esta gr√°fica muestra la energ√≠a distribuida entre las <strong>12 clases de notas musicales</strong> (Do, Do#, Re...). Es muy √∫til para capturar la <strong>entonaci√≥n y melod√≠a</strong> del habla, que son aspectos importantes de la expresi√≥n emocional, independientemente del timbre de la voz.</p>
            </div>
        </div>
        <div class="slide hidden" data-slide="24">
            <h2>19. Arquitectura de la Red Neuronal</h2>
            <div class="neural-architecture">
                <div class="layer-box input">
                    <h4>Capa de Entrada</h4>
                    <p>Espectrograma Mel<br><span class="layer-shape">128 √ó 128 √ó 1</span></p>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box conv">
                    <h4>Conv2D (32 filtros) + ReLU</h4>
                    <ul>
                        <li><strong>Conv2D:</strong> Aplica 32 filtros para extraer patrones espaciales del espectrograma.</li>
                        <li><strong>ReLU:</strong> Activa solo los valores positivos, permitiendo no linealidad.</li>
                    </ul>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box pool">
                    <h4>MaxPooling2D</h4>
                    <ul>
                        <li><strong>MaxPooling2D:</strong> Reduce la dimensi√≥n espacial tomando el valor m√°ximo en regiones peque√±as, haciendo la red m√°s eficiente y robusta.</li>
                    </ul>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box flatten">
                    <h4>Flatten</h4>
                    <ul>
                        <li><strong>Flatten:</strong> Convierte la salida 2D en un vector 1D para conectar con la capa densa.</li>
                    </ul>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box dense">
                    <h4>Capa Densa (128) + ReLU</h4>
                    <ul>
                        <li><strong>Densa:</strong> Combina toda la informaci√≥n aprendida para la clasificaci√≥n.</li>
                        <li><strong>ReLU:</strong> Activa solo los valores positivos.</li>
                    </ul>
                </div>
                <div class="arrow">‚Üì</div>
                <div class="layer-box output">
                    <h4>Capa de Salida (Softmax)</h4>
                    <p>Asigna una probabilidad a cada una de las 7 clases de emoci√≥n.<br><span class="layer-shape">7</span></p>
                </div>
            </div>
            <footer class="footnote">
                <strong>Conv2D:</strong> Capa convolucional bidimensional, detecta patrones espaciales.<br>
                <strong>MaxPooling2D:</strong> Reduce la resoluci√≥n espacial, mantiene las caracter√≠sticas m√°s importantes.<br>
                <strong>Flatten:</strong> Convierte la matriz en un vector.<br>
                <strong>Densa:</strong> Capa totalmente conectada.<br>
                <strong>Softmax:</strong> Convierte los valores en probabilidades para cada clase.<br>
                <strong>ReLU:</strong> Funci√≥n de activaci√≥n que deja pasar solo valores positivos.
            </footer>
        </div>

        <!-- Nueva diapositiva: Resumen de Neuronas -->
        <div class="slide hidden" data-slide="25">
            <h2>20. Resumen de Neuronas por Capa</h2>
            <div class="cnn-details">
                <p>
                    La siguiente tabla muestra cu√°ntas neuronas (o unidades de activaci√≥n) hay en cada paso de la red neuronal convolucional, desde la entrada (espectrograma) hasta la salida (clases de emoci√≥n). Cada capa transforma la informaci√≥n y la representa de manera diferente:
                </p>
                <ul class="cnn-table-explanation">
                    <li><strong>Conv2D (32 filtros):</strong> Aplica 32 filtros sobre la imagen, generando 32 mapas de activaci√≥n.</li>
                    <li><strong>MaxPooling2D:</strong> Reduce la dimensi√≥n espacial, manteniendo solo los valores m√°ximos de cada regi√≥n.</li>
                    <li><strong>Flatten:</strong> Convierte la salida 2D/3D en un vector 1D.</li>
                    <li><strong>Densa (128):</strong> Capa totalmente conectada con 128 neuronas.</li>
                    <li><strong>Salida:</strong> 7 neuronas, una por cada emoci√≥n.</li>
                </ul>
                <table class="cnn-table">
                    <tr>
                        <th>Capa</th>
                        <th>Forma de salida</th>
                        <th>Neuronas</th>
                        <th>Activaci√≥n</th>
                    </tr>
                    <tr>
                        <td>Entrada</td>
                        <td>128 √ó 128 √ó 1</td>
                        <td>16,384</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Conv2D (32 filtros)</td>
                        <td>128 √ó 128 √ó 32</td>
                        <td>524,288</td>
                        <td>ReLU</td>
                    </tr>
                    <tr>
                        <td>MaxPooling2D</td>
                        <td>64 √ó 64 √ó 32</td>
                        <td>131,072</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Flatten</td>
                        <td>1 √ó 131,072</td>
                        <td>131,072</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Densa (128)</td>
                        <td>128</td>
                        <td>128</td>
                        <td>ReLU</td>
                    </tr>
                    <tr>
                        <td>Salida</td>
                        <td>7</td>
                        <td>7</td>
                        <td>Softmax</td>
                    </tr>
                    <tr class="cnn-total">
                        <td colspan="2"><strong>Total de neuronas</strong></td>
                        <td colspan="2"><strong>802,971</strong></td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="slide hidden" data-slide="26">
            <h2>21. Alcance del Proyecto</h2>
            <div class="advantages-disadvantages">
                <div class="advantages">
                    <h3>Incluido en el Proyecto</h3>
                    <ul>
                        <li>Modelo de IA para clasificar 7 emociones desde audio.</li>
                        <li>Pipeline completo: carga, preprocesamiento y evaluaci√≥n.</li>
                        <li>Procesamiento de archivos WAV de 1 a 10 segundos.</li>
                        <li>Comparaci√≥n de t√©cnicas de reducci√≥n de dimensionalidad (PCA vs. LDA).</li>
                        <li>M√©tricas de rendimiento del modelo (Accuracy, F1-Score).</li>
                    </ul>
                </div>
                <div class="disadvantages">
                    <h3>Limitaciones y Exclusiones</h3>
                    <ul>
                        <li>An√°lisis exclusivo de la modalidad de audio.</li>
                        <li>No se desarrolla una aplicaci√≥n de usuario final (GUI o web).</li>
                        <li>El modelo no opera en tiempo real.</li>
                        <li>No se realiza una validaci√≥n cl√≠nica ni se infieren diagn√≥sticos.</li>
                    </ul>
                </div>
            </div>
        </div>
        
        <!-- Diapositiva 27: Gracias (Actualizada) -->
        <div class="slide hidden farewell-slide" data-slide="27">
            <h1 class="farewell-title">¬°Gracias!</h1>
            <p class="farewell-message">
                Este proyecto demuestra la viabilidad de usar IA para extraer y clasificar<br>
                patrones emocionales complejos a partir de la se√±al de voz.
            </p>
            <div class="highlight-box">
                <h3>ü§î ¬øPreguntas o Comentarios?</h3>
            </div>
            <div class="contact-section">
                <h3>Contacto del Equipo</h3>
                <div class="contact-grid">
                    <href="https://www.instagram.com/alexpl.0?igsh=MTAwNTM2cnRxYmQ0eQ==" target="_blank" class="contact-item">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Alejandro P√©rez</span>
                    </a>
                    <href="https://www.instagram.com/davidsandoval____?igsh=MWllczVjNHpqMjRpcg==" target="_blank" class="contact-item">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Yusmany Rejopachi</span>
                    </a>
                    <href="https://www.instagram.com/_jair.gg?igsh=MWVxeDdlcTR1OW1lZg==" target="_blank" class="contact-item">
                        <svg class="instagram-logo" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg>
                        <span>Jair Gutierrez</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">Siguiente ‚Üí</button>
    </div>

    <script src="script.js"></script>
</body>
</html>
